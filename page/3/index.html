<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>代码人生</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="赵坤的个人网站">
<meta property="og:type" content="website">
<meta property="og:title" content="代码人生">
<meta property="og:url" content="http://blog.kunzhao.org/page/3/index.html">
<meta property="og:site_name" content="代码人生">
<meta property="og:description" content="赵坤的个人网站">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="代码人生">
<meta name="twitter:description" content="赵坤的个人网站">
  
    <link rel="alternate" href="/atom.xml" title="代码人生" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/blog/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://blog.kunzhao.org"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">首页</a>
        
          <a class="main-nav-link" href="/blog/archives">归档</a>
        
          <a class="main-nav-link" href="/blog/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">代码人生</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-crawling-deep-web-entity-pages" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/11/25/crawling-deep-web-entity-pages/" class="article-date">
  <time datetime="2017-11-25T10:41:29.000Z" itemprop="datePublished">2017-11-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/论文/">论文</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/blog/2017/11/25/crawling-deep-web-entity-pages/">crawling-deep-web-entity-pages</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="Crawling-Deep-Web-Entity-Pages"><a href="#Crawling-Deep-Web-Entity-Pages" class="headerlink" title="Crawling Deep Web Entity Pages"></a>Crawling Deep Web Entity Pages</h2><ul>
<li><strong>2013</strong> 年论文，发表在<a href="https://dl.acm.org/citation.cfm?id=2433442" target="_blank" rel="external">WSDM ‘13 Proceedings of the sixth ACM international conference on Web search and data mining</a></li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong><font color="red">Introduction</font></strong></h3><ul>
<li>Document-oriented textual content: Wikipedia, PubMed, Twitter</li>
<li>Curate structured entities: almost all online shopping sites</li>
</ul>
<p>Existing crawling techniques optimized for document oriented content <strong>are not best suited for</strong> entity-oriented sites.</p>
<p>In this paper we focus on entity-oriented deep-web sites. These sites <strong>curate structured entities</strong> and expose them through <strong>search</strong> interfaces.</p>
<h4 id="Deep-Web"><a href="#Deep-Web" class="headerlink" title="Deep Web"></a>Deep Web</h4><p><strong>定义</strong>: The deep web, invisible web, or hidden web are parts of the World Wide Web whose contents are <strong>not indexed by standard search engines</strong> for any reason. The content is <strong>hidden behind HTML forms</strong>.</p>
<p><strong>大小</strong>:</p>
<ul>
<li><strong>Only a 4% of information</strong> is visible to the search engines like <code>Google</code> or <code>Yahoo</code> which is said to be a “Visible Web” or “Surface Web“.</li>
<li>The Deep Web is estimated to be <strong>500x</strong> the size of the Surface Web</li>
</ul>
<p><img src="What-are-some-Positive-Uses-for-the-Deep-Web-1.jpg" alt=""></p>
<h4 id="Entity-oriented-Deep-Web"><a href="#Entity-oriented-Deep-Web" class="headerlink" title="Entity-oriented Deep Web"></a>Entity-oriented Deep Web</h4><p><strong>Amazon.com</strong>:</p>
<p><img src="2017_11_15_22_34_27.png" alt=""></p>
<p><strong>lagou.com</strong>:</p>
<p><img src="2017_11_15_22_36_01.png" alt=""></p>
<p><strong>movie.mtime.com</strong>:</p>
<p><img src="2017_11_15_22_37_46.png" alt=""></p>
<h3 id="现有搜索引擎爬取暗网解决方案"><a href="#现有搜索引擎爬取暗网解决方案" class="headerlink" title="现有搜索引擎爬取暗网解决方案"></a>现有搜索引擎爬取暗网解决方案</h3><ul>
<li><strong><code>Baidu</code></strong>: 第三方平台通过提交<strong>结构化数据</strong>给百度，从而获得百度的搜索结果。</li>
</ul>
<p><img src="2017_11_25_19_28_30.png" alt=""></p>
<ul>
<li><strong><code>Google</code></strong>: 自动模拟 <code>form</code> 表单提交过程，来尽量进行一个更全的数据索引。</li>
</ul>
<p><img src="2017_11_25_19_46_34.png" alt=""><br><img src="2017_11_25_19_43_48.png" alt=""></p>
<h4 id="The-goal-of-our-system"><a href="#The-goal-of-our-system" class="headerlink" title="The goal of our system"></a>The goal of our system</h4><p>Crawl product entities from a large number of online retailers for <strong>advertisement landing page</strong> purposes. The purpose of the system is <strong>not to obtain</strong> all <code>iphone</code> listings, but only a representative few of these listings for ads landing pages.</p>
<p>While the exact use of such entities content in advertisement is <strong>beyond</strong> the scope of this paper, the system requirement is simple to state: We are provided as <strong>input</strong> a list of retailers’ websites, and the objective is to <strong>crawl</strong> high-quality product entity <strong>pages</strong> efficiently and effectively.</p>
<p><img src="2017_11_15_22_41_42.png" alt=""></p>
<p>Traditional deep-web crawl literature, which tends to deal with individual site and <strong>focus</strong> on obtaining <strong>exhausive</strong> content coverage.</p>
<h3 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a><strong><font color="red">System Overview</font></strong></h3><p><img src="2017_11_15_23_02_42.png" alt=""></p>
<h3 id="URL-template-generation"><a href="#URL-template-generation" class="headerlink" title="URL template generation"></a><strong><font color="red">URL template generation</font></strong></h3><p>几个深网网站主页面作为输入:</p>
<table>
<thead>
<tr>
<th>Deep-Web sites</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>https://www.ebay.com</code></td>
</tr>
<tr>
<td><code>https://www.taobao.com</code></td>
</tr>
</tbody>
</table>
<p>经过 <strong>URL 模板生成器</strong> 模块以后输出:</p>
<table>
<thead>
<tr>
<th>URL templates</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>https://www.ebay.com/sch/i.html?_nkw={query}</code></td>
</tr>
<tr>
<td><code>https://s.taobao.com/search?q={query}</code></td>
</tr>
</tbody>
</table>
<hr>
<p>Parse search forms technique <a href="https://homes.cs.washington.edu/~alon/files/vldb08deepweb.pdf" target="_blank" rel="external">Google’s Deep-Web Crawl</a>:</p>
<p><img src="2017_11_18_11_58_08.png" alt=""></p>
<ul>
<li>便于搜索引擎索引</li>
<li>忽略带有 <code>password</code> 类型的 <code>input</code> ，需要输入个人信息</li>
<li>忽略带有 <code>textarea</code> 类型的 <code>input</code> ，一般作为 <code>feedback</code>/<code>comments</code></li>
<li>处理 <code>JavaScript</code> 事件，超出了论文范围</li>
</ul>
<p><img src="2017_11_25_11_47_19.png" alt=""></p>
<p><img src="2017_11_18_12_05_47.png" alt=""></p>
<p><strong>输入的查询组合</strong>:</p>
<p><img src="2017_11_27_10_53_07.png" alt=""></p>
<p>生成 7 个查询模板：</p>
<ul>
<li><code>{地点}</code></li>
<li><code>{时间}</code></li>
<li><code>{房客}</code></li>
<li><code>{地点, 时间}</code></li>
<li><code>{时间, 房客}</code></li>
<li><code>{地点, 房客}</code></li>
<li><code>{地点, 时间, 房客}</code></li>
</ul>
<p>每个模板，提交若干个查询，观察返回页面内容变化情况，如果大部分返回内容都相同或相似，则说明这个查询模板不是<strong>富含信息查询模板</strong>。</p>
<p>假设按照上面方式对所有模板一一试探，判断对其是否富含信息查询模板，则因为查询模板数量太多，<strong>系统效率还是会低</strong>。为了进一步减少提交的查询数目，<code>Google</code> 又提出了 <u><code>ISIT (Incremental Search for Informative Query Templates)</code></u> 算法:</p>
<p>首先从<strong>一维模板</strong>开始，对一维模板逐个考察，看其是否富含信息查询模板，如果是的话，则将这个<strong>一维模板扩展到二维</strong>，再次考察对应的二维模板，如此类推，逐步增加维数，直到再也无法找到富含信息查询模板为止。通过这种方式，能够大幅度提升系统效率。</p>
<hr>
<p>However, our analysis shows that generating URL templates by <strong>enumerating values combination in multiple input fields</strong> can lead to an inefficiently large number of templates and may not scale to the number of websites that we are interested in crawling.</p>
<p>我们还发现 for entity-oriented sites:</p>
<ul>
<li>the main search form is almost <strong>always on home pages</strong> instead of somewhere deep in the site</li>
<li>search forms predominantly <strong>use one main text fields</strong> to accept keyword queries</li>
</ul>
<p><img src="2017_11_16_09_24_10.png" alt=""></p>
<p>This <strong>obviates</strong> the need to use sophisticated techniques to <strong>locate search forms</strong> deep in websites.</p>
<h3 id="Query-generation-and-URL-generation"><a href="#Query-generation-and-URL-generation" class="headerlink" title="Query generation and URL generation"></a><font color="red">Query generation and URL generation</font></h3><p>We utilize two data sources for query generation: <strong>query logs</strong> and <strong>knowledge-bases</strong>. Our main observation here is that classical techniques in information retrieval and entity extraction are <strong>already effective</strong> in generating entity queries.</p>
<h4 id="1-Entity-extraction-from-query-logs"><a href="#1-Entity-extraction-from-query-logs" class="headerlink" title="1. Entity extraction from query logs"></a><font color="#673AB7">1. Entity extraction from query logs</font></h4><p>Takes the <code>Freebase</code> and query logs as input, outputs queries consistent with the semantics of each deep-web site.</p>
<p><img src="2017_11_16_09_35_22.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;查询的关键词, 点击的 URL, URL 被点击的次数&gt;</div></pre></td></tr></table></figure>
<p><img src="2017_11_25_10_59_26.png" alt=""></p>
<p>问题所在: queries in the query logs tend to <strong>contain extraneous tokens</strong> in addition to the central entity of interest:</p>
<p><code>Google</code> 实际接受的关键字查询 <code>hp touchpad reviews</code> 用来直接在 <code>ebay</code> 上搜索:</p>
<p><img src="2017_11_16_09_39_27.png" alt=""></p>
<p><code>ebay</code> 实际上需要的词汇 <code>hp touchpad</code>:</p>
<p><img src="2017_11_16_09_40_23.png" alt=""></p>
<p>这个时候我们需要…<strong>a comprehensive entity dictionary</strong></p>
<p><strong>Freebase</strong>: <code>Freebase</code> 是个类似 <code>wikipedia</code> 的创作共享类网站，所有内容都由用户添加，采用创意共用许可证，可以自由引用。两者之间最大的不同在于，<code>Freebase</code> 中的条目都采用<strong>结构化数据</strong>的形式，而 <code>wikipedia</code> 不是。 </p>
<p><img src="820px-Freebase_Logo_optimised.svg.png" alt=""></p>
<ul>
<li>It was developed by the American software company <strong><code>Metaweb</code></strong> and ran publicly since <strong>March 2007</strong>.</li>
<li><code>Metaweb</code> was acquired by Google in a private sale announced <strong>16 July 2010</strong>. Google’s Knowledge Graph was powered in part by Freebase.</li>
<li>对外提供 <strong>RDF、API</strong> 形式的数据</li>
<li>On 16 December 2015, Google officially announced the <strong>Knowledge Graph API</strong>, which is meant to be a replacement to the Freebase API. <code>Freebase.com</code> was officially shut down on 2 May 2016.</li>
</ul>
<p><strong>RDF 把天下所有的信息以同一种方式描述:</strong></p>
<p>每一条描述都是一个<strong>主谓宾三元组</strong>构成的短句:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123; 苹果, 是, 公司 &#125;, </div><div class="line">&#123; 库克, 是, 人 &#125;, </div><div class="line">&#123; 苹果, CEO 是, 库克 &#125;</div></pre></td></tr></table></figure>
<p>你要直接说 <strong>「苹果公司的 CEO 是个叫库克的人」</strong> ，计算机就凌乱了啊，因为<strong>自然语言包含太多的不确定性</strong>。有了许多这样的三元组以后，我们就可以得到一个<strong>知识网</strong>。</p>
<p><img src="example-graph.jpg" alt=""></p>
<p>“知识图谱” （Knowledge Graph） —— <strong>可以将搜索结果进行知识系统化，任何一个关键词都能获得完整的知识体系</strong>。比如搜索“Amazon”（亚马逊河），一般的搜索结果会给出和 <code>Amazon</code> 最相关的信息。比如 <code>Amazon</code> 网站，因为网上关于它的信息最多，但 <code>Amazon</code> 并不仅仅是一个网站，它还是全球流量最大的 <code>Amazon</code> 河流。如果在追溯历史，它可能还是希腊女战士一族的代称。而这些结果未来都会在 <code>Google</code> 搜索的“知识图谱”中展现出来。</p>
<p><strong>Knowledge Graph data</strong> about <strong>Thomas Jefferson</strong> displayed on Google Web Search:</p>
<p><img src="2017_11_25_11_13_54.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https://kgsearch.googleapis.com/v1/entities:search?query=taylor+swift&amp;key=API_KEY&amp;limit=1&amp;indent=True</div></pre></td></tr></table></figure>
<p>服务器返回结果:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="string">"@context"</span>: &#123;</div><div class="line">    <span class="string">"@vocab"</span>: <span class="string">"http://schema.org/"</span>,</div><div class="line">    <span class="string">"goog"</span>: <span class="string">"http://schema.googleapis.com/"</span>,</div><div class="line">    <span class="string">"resultScore"</span>: <span class="string">"goog:resultScore"</span>,</div><div class="line">    <span class="string">"detailedDescription"</span>: <span class="string">"goog:detailedDescription"</span>,</div><div class="line">    <span class="string">"EntitySearchResult"</span>: <span class="string">"goog:EntitySearchResult"</span>,</div><div class="line">    <span class="string">"kg"</span>: <span class="string">"http://g.co/kg"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"@type"</span>: <span class="string">"ItemList"</span>,</div><div class="line">  <span class="string">"itemListElement"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"@type"</span>: <span class="string">"EntitySearchResult"</span>,</div><div class="line">      <span class="string">"result"</span>: &#123;</div><div class="line">        <span class="string">"@id"</span>: <span class="string">"kg:/m/0dl567"</span>,</div><div class="line">        <span class="string">"name"</span>: <span class="string">"Taylor Swift"</span>,</div><div class="line">        <span class="string">"@type"</span>: [</div><div class="line">          <span class="string">"Thing"</span>,</div><div class="line">          <span class="string">"Person"</span></div><div class="line">        ],</div><div class="line">        <span class="string">"description"</span>: <span class="string">"Singer-songwriter"</span>,</div><div class="line">        <span class="string">"image"</span>: &#123;</div><div class="line">          <span class="string">"contentUrl"</span>: <span class="string">"https://t1.gstatic.com/images?q=tbn:ANd9GcQmVDAhjhWnN2OWys2ZMO3PGAhupp5tN2LwF_BJmiHgi19hf8Ku"</span>,</div><div class="line">          <span class="string">"url"</span>: <span class="string">"https://en.wikipedia.org/wiki/Taylor_Swift"</span>,</div><div class="line">          <span class="string">"license"</span>: <span class="string">"http://creativecommons.org/licenses/by-sa/2.0"</span></div><div class="line">        &#125;,</div><div class="line">        <span class="string">"detailedDescription"</span>: &#123;</div><div class="line">          <span class="string">"articleBody"</span>: <span class="string">"Taylor Alison Swift is an American singer-songwriter and actress. Raised in Wyomissing, Pennsylvania, she moved to Nashville, Tennessee, at the age of 14 to pursue a career in country music. "</span>,</div><div class="line">          <span class="string">"url"</span>: <span class="string">"http://en.wikipedia.org/wiki/Taylor_Swift"</span>,</div><div class="line">          <span class="string">"license"</span>: <span class="string">"https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License"</span></div><div class="line">        &#125;,</div><div class="line">        <span class="string">"url"</span>: <span class="string">"http://taylorswift.com/"</span></div><div class="line">      &#125;,</div><div class="line">      <span class="string">"resultScore"</span>: <span class="number">896.576599</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>we first obtained a dump of the Freebase data — a manually curated repository with about 22M entities. We then find the <strong>maximum-length subsequence</strong> in each search engine query that <strong>matches</strong> Freebase entities as an entity mention. The remaining tokens are treated as <strong>entity-irrelevant prefix/suffix</strong>. We aggregate distinct prefix/suffix across the query logs to obtain common patterns ordered by their <strong>frequency of occurrences</strong>. The most frequent patterns are likely to be irrelevant to entities and need to be cleaned.</p>
<ul>
<li>匹配 <code>entity</code></li>
<li>按照出现的频率排序 <code>entity</code> 不想关的前缀/后缀</li>
<li>移除出现较多的前缀/后缀</li>
</ul>
<p><img src="2017_11_16_10_33_18.png" alt=""></p>
<p><strong>Example entities</strong> extracted for each deep-web site:</p>
<p><img src="2017_11_16_11_05_51.png" alt=""></p>
<h4 id="2-Entity-extraction-from-using-knowledge-bases"><a href="#2-Entity-extraction-from-using-knowledge-bases" class="headerlink" title="2. Entity extraction from using knowledge-bases"></a><font color="#673AB7">2. Entity extraction from using knowledge-bases</font></h4><p>While query logs provide a good set of initial seed entities, its <strong>coverage</strong> for each site depends on the <strong>site’s popularity</strong> as well as the <strong>item’s popularity</strong>. Even for highly popular sites, there is a <strong>long tail</strong> of less popular items which may not be captured by query logs.</p>
<p><strong>长尾效应</strong>: 极少数个体（横轴）对应极高的值（纵轴），而拥有极低值的个体，数量却占总体的绝大多数:</p>
<p><img src="Long-Tail-Large-1.png" alt=""></p>
<p>Examples of these two types of keywords would be:</p>
<ul>
<li>Non-long tail: <strong>“jewelry”</strong> – 201,000 searches per month in Google </li>
<li>Long tail:  <strong>“men’s silver jewelry”</strong> – 260 searches per month in Google </li>
</ul>
<p>“Men’s silver jewelry” is an example of a long tail keyword as it gets searched for less than the more generic “jewelry” keyword. As these long tail keywords are searched for less often, they can be less attractive for large and successful websites with big marketing budgets to try and rank for.</p>
<p>On the other hand, we observe that there exists manually curated entity repositories (e.g., Freebase), that maintain entities <strong>in certain domains with very high coverage (city names, books, car models, movies, etc) 在相应领域覆盖面都比较全</strong>.Thus, for each site, we need to bootstrap from these seed entities to <strong>expand</strong> to Freebase entity “types” that are relevant to each site’s semantics.</p>
<p>我们可以…继续使用 <code>Freebase</code> 来扩展我们的种子 <code>entities</code>。</p>
<hr>
<ul>
<li><strong>词频 (term frequency, TF)</strong>: </li>
</ul>
<p><img src="bg2013031503.png" alt=""><br><img src="bg2013031504.png" alt=""></p>
<ul>
<li><strong>逆向文件频率 (inverse document frequency, IDF)</strong> 是一个词语普遍重要性的度量</li>
</ul>
<p><img src="bg2013031506.png" alt=""></p>
<p>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<ul>
<li><strong>TF-IDF</strong>:</li>
</ul>
<p><img src="bg2013031507.png" alt=""></p>
<p><strong>可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。</strong></p>
<p><a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="external">TF-IDF 的应用</a>:</p>
<ul>
<li>提取文档关键词</li>
<li>信息检索时，对于每个文档，都可以分别计算<strong>一组搜索词</strong>（”中国”、”蜜蜂”、”养殖”）的 TF-IDF，将它们<strong>相加</strong>，就可以得到整个文档的TF-IDF。这个<strong>值最高</strong>的文档就是与搜索词最相关的文档</li>
<li>寻找与原文章相似的其他文章</li>
<li>对文章进行自动摘要</li>
</ul>
<p>We borrow classical techniques from information retrieval: if we view the multi-set of Freebase entity mentions for each site as a <strong>document</strong>, and <strong>the list of entities</strong> in each Freebase <strong>type</strong> as a <strong>query</strong>, then the classical term-frequency, inverse document frequency (<strong>TF-IDF</strong>) ranking can be applied.</p>
<p>For each Freebase type, we use TF-IDF to produce <strong>a ranked list of deep-web sites by their similarity scores</strong>. We then “threshold” the sorted list using a relative score. That is, we include all sites with scores <strong>above</strong> a fixed percentage, τ, <strong>of the highest similarity score in the same Freebase type as matches</strong>. Empirically results in Section 8 show that setting <strong>τ = 0.5</strong> achieves good results and is used in our system. This approach is significantly <strong>more effective</strong> than other alternatives like <strong>Cosine or Jaccard Similarity</strong>.</p>
<p>对于 <code>ebay.com</code>，我们获取了一个初始的 <code>document</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">iphone 4</div><div class="line">lenovo</div><div class="line">...</div></pre></td></tr></table></figure>
<p>对于 <code>bestbuy.com</code>,我们获取了一个初始的 <code>document</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hp touchpad,</div><div class="line">sony vaio</div></pre></td></tr></table></figure>
<p>然后我们有一个 <code>Freebase Type</code>, 比如 <code>Electronics</code>， 有关键词:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Samsung Galaxy s8,</div><div class="line">lenovo</div><div class="line">...</div></pre></td></tr></table></figure>
<p><img src="2017_11_25_15_26_42.png" alt=""></p>
<h3 id="Empty-page-filter"><a href="#Empty-page-filter" class="headerlink" title="Empty page filter"></a><font color="red">Empty page filter</font></h3><p>我们发现… Empty pages from the same site <strong>tend to be highly similar</strong>.</p>
<p>随机输入采集空页面样本:</p>
<p><img src="2017_11_16_11_20_38.png" alt=""><br><img src="2017_11_16_11_21_01.png" alt=""></p>
<p>Let $S_{p1}$ and $S_{p2}$ be the sets of tokens representing the <strong>signature</strong> of the crawled page $p1$ and $p2$. The <strong><code>Jaccard Similarity</code></strong> between $S_{p1}$ and $S_{p2}$, denoted $Sim_{jac} (S_{p1}, S_{p2})$, is defined as:</p>
<p><img src="2017_11_18_11_29_34.png" alt=""></p>
<hr>
<p><strong>Jaccard 相似度</strong>:</p>
<p>计算两个集合之间的相似度</p>
<p><img src="20131015134839640.png" alt=""></p>
<p>当平均值大于 $\theta$ 的时候，我们将它判断为空页面。 As we will show in experiments, this approach is very effective in detecting empty pages across different websites (with an overall <strong>precision of 0.89</strong> and a <strong>recall of 0.9</strong>).</p>
<hr>
<p>对于 <code>signature</code> 是如何计算的:</p>
<p>While the exact details of <code>Sig</code> are <strong>less important</strong>, we enumerate the important properties we want from such a function <a href="https://homes.cs.washington.edu/~alon/files/vldb08deepweb.pdf" target="_blank" rel="external">参考: Google’s Deep-Web Crawl</a>:</p>
<ul>
<li>the signature should be <strong>agnostic to HTML formatting</strong>, since presentation inputs often simply change the layout of the web page.</li>
<li>Third, the signature must be <strong>tolerant to minor differences</strong> in page content. A common source of differences are advertisements, especially on commercial sites。 These advertisements are typically displayed on page margins. They contribute to the text on the page but do not reflect the content of the retrieved records and hence have to be filtered away.</li>
<li>Lastly, the signature should <strong>not include the input values</strong> themselves.</li>
</ul>
<h3 id="二级页面抓取"><a href="#二级页面抓取" class="headerlink" title="二级页面抓取"></a><font color="red">二级页面抓取</font></h3><p>We refer to the first set of pages obtained through URL templates as “first-level pages” (because they are <strong>one click</strong> away from homepage), and those pages that are linked from first-level pages as “second-level pages”.</p>
<h4 id="The-motivation-for-second-level-crawl"><a href="#The-motivation-for-second-level-crawl" class="headerlink" title="The motivation for second level crawl"></a>The motivation for second level crawl</h4><p><img src="2017_11_16_21_13_21.png" alt=""></p>
<p>Faceted search/browsing paradigm:</p>
<p><img src="2017_11_18_12_11_45.png" alt=""></p>
<ul>
<li>improving content coverage</li>
</ul>
<h4 id="URL-extraction-and-filtering"><a href="#URL-extraction-and-filtering" class="headerlink" title="URL extraction and filtering"></a>URL extraction and filtering</h4><ul>
<li>Whil some second-level URLs are desirable, not all second level URLs should be crawled for effiency as well as <strong>quality</strong> reasons.</li>
<li>We observe that filtering URLs by <strong>keyword-query arguments</strong> significantly reduces the number of URLs — typically by a factor of 3-5.</li>
</ul>
<h4 id="URL-deduplication"><a href="#URL-deduplication" class="headerlink" title="URL deduplication"></a>URL deduplication</h4><p>We observe that after URL filtering, there are groups of URLs that are different in their text string but really lead to <strong>similar or nearly identical</strong> deep-web content.</p>
<p><img src="2017_11_16_21_55_55.png" alt=""></p>
<p>content-based URL deduplication</p>
<ul>
<li><code>www.cnn.com/story?id=num</code></li>
<li><code>www.cnn.com/story_num</code></li>
</ul>
<hr>
<p>In this paper we propose an approach that analyzes <strong>URL argument patterns</strong> and deduplicates URLs even <strong>before</strong> any pages are crawled.</p>
<p>First, we categorize query segments into <strong>three groups</strong>:</p>
<ol>
<li><u><strong>selection segments</strong></u>: are query segments that correspond to selection predicates and can <strong>affect</strong> the set of result entities.</li>
<li><u><strong>presentation segments</strong></u>: are query segments that do not change the result set, but only affect how the set of entities are <strong>presented</strong>.</li>
<li><u><strong>content irrelevant segments</strong></u>: are query segments that have no immediate impact on the result entities.</li>
</ol>
<p>We then define two URLs as <strong>semantic duplicates</strong> if the corresponding selection queries <strong>have the same set of selection segments</strong>.</p>
<p>然后我们又注意到:</p>
<ol>
<li>The fact that these query segments in <strong>almost all pages</strong> indicates that they are not specific to the input keyword query, and are thus likely to be either <strong>presentational</strong> (sorting, page number, etc.), or <strong>content irrelevant</strong> (internal tracking, etc.).</li>
<li>On the other hand, selection segments, like manufacturer name (“mfgid=-652” for “Garmin”) in the previous example, are <strong>much more sensitive to the input queries</strong>.</li>
</ol>
<p>To capture this intuition we define a notion of <strong>prevalence</strong> at the <u>query segment (argument/value pair) level</u> and also at the <u>argument level</u>:</p>
<p><img src="2017_11_25_17_43_32.png" alt=""></p>
<p>The average prevalence score at argument level is a more robust indicator of the prevalence of an argument.</p>
<p>我们将 any argument with prevalence higher than <code>\theta</code> 视为 <strong>semantically-irrelevant</strong>. On the other hand, if an query segment’s arguments has prevalence lower than <code>\theta</code> it is assumed to be a <strong>selection argment</strong>.</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><font color="red">Experiments</font></h3><p>简单讨论一下实验结果</p>
<h4 id="Query-extraction-from-query-logs"><a href="#Query-extraction-from-query-logs" class="headerlink" title="Query extraction from query logs"></a><font color="#673AB7">Query extraction from query logs</font></h4><p>In this experiment, we used <strong>6 month’s worth Google’s query logs</strong>, and entities in Freebase as seed entities. In order to evaluate whether patterns produced by our approach is truly entity-irrelevant or not, we asked a <strong>domain expert</strong> to manually label top 200 patterns, as correct predictions (irrelevant to entities) or incorrect predictions (relevant to entities).</p>
<p>The top 10 most frequent prefix and suffix patterns we produced:</p>
<p><img src="2017_11_17_11_00_27.png" alt=""></p>
<p>We summarize the precision for top 10, 20, 50, 100 and 200 patterns:</p>
<p><img src="2017_11_17_10_58_11.png" alt=""></p>
<h4 id="Entity-expansion-using-Freebase"><a href="#Entity-expansion-using-Freebase" class="headerlink" title="Entity expansion using Freebase"></a><font color="#673AB7">Entity expansion using Freebase</font></h4><p><img src="2017_11_17_11_13_58.png" alt=""></p>
<ul>
<li>10 个领域</li>
<li>流量最大的前 100 零售商</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;网站, Freebase 类型&gt;</div></pre></td></tr></table></figure>
<p><img src="2017_11_17_11_52_34.png" alt=""></p>
<p><img src="2017_11_17_11_48_14.png" alt=""></p>
<h4 id="Empty-page-filtering"><a href="#Empty-page-filtering" class="headerlink" title="Empty page filtering"></a><font color="#673AB7">Empty page filtering</font></h4><p><strong>Precision and recall (准确率 &amp; 召回率)</strong>:</p>
<p><strong>准确率</strong>和<strong>召回率</strong>是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的<strong>查准率</strong>；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的<strong>查全率</strong>。 <a href="http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/" target="_blank" rel="external">参考</a></p>
<p>一般来说，<code>Precision</code> 就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。</p>
<p>正确率、召回率是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先：</p>
<ul>
<li><code>正确率 = 提取出的正确信息条数 /  提取出的信息条数</code></li>
<li><code>召回率 = 提取出的正确信息条数 /  样本中的信息条数</code></li>
</ul>
<p>两者取值在 <code>0</code> 和 <code>1</code> 之间，数值越接近 <code>1</code>，查准率或查全率就越高。</p>
<p><img src="20111124212141974.jpg" alt=""></p>
<p>When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is <strong>“how useful the search results are”</strong>, and recall is <strong>“how complete the results are”</strong>.</p>
<p>In pattern recognition, information retrieval and binary classification, Precision and Recall 是<strong>两个很重要的评估指标</strong>。</p>
<hr>
<p><img src="2017_11_17_17_18_12.png" alt=""></p>
<p>随机选择了 10 个高流量的网站:</p>
<p><img src="2017_11_17_17_34_31.png" alt=""></p>
<h4 id="URL-deduplication-1"><a href="#URL-deduplication-1" class="headerlink" title="URL deduplication"></a><font color="#673AB7">URL deduplication</font></h4><p>继续使用了上面 10 个网站，阈值在 0.01 ~ 0.5 之间的结果：</p>
<p><img src="2017_11_17_21_08_37.png" alt=""><br><img src="2017_11_17_21_12_14.png" alt=""></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><font color="red">Conclusion</font></h3><ul>
<li>In the template generation, our parsing approach <strong>only handles HTML “GET”</strong> forms but not “POST” forms or javascript forms.</li>
<li>In query generation, although Freebase-based entity expansion is useful, certain sites with <strong>低流量</strong> or diverse traffic do not get matched with Freebase types effectively using query logs alone.</li>
<li>Utilizing additional signals (e.g., entities bootstrapped from crawled pages) for <strong>entity expansion</strong> is an interesting area.</li>
<li>Efficiently enumerate entity query for search forms with <strong>多个文本框</strong> is another interesting challenge.</li>
</ul>
<p>（完）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.kunzhao.org/2017/11/25/crawling-deep-web-entity-pages/" data-id="cjcdlsfxj003sdiemf0ufvqra" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-deep-web-paper" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/11/09/deep-web-paper/" class="article-date">
  <time datetime="2017-11-09T08:58:26.000Z" itemprop="datePublished">2017-11-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/开发者手册/">开发者手册</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/blog/2017/11/09/deep-web-paper/">deep-web-paper</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="Deep-Web-Paper"><a href="#Deep-Web-Paper" class="headerlink" title="Deep Web Paper"></a>Deep Web Paper</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul>
<li>购物网站： <code>entity-oriented</code></li>
</ul>
<p>解决的问题:</p>
<ul>
<li>query generation</li>
<li>empty page filtering</li>
<li>URL deduplication</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>(1) 购物网站</strong>:</p>
<ul>
<li>商品名称</li>
<li>品牌</li>
<li>价格</li>
<li>…</li>
</ul>
<p><strong>(2) 电影网站</strong></p>
<p><strong>(3) 求职网站</strong></p>
<p>输入:</p>
<ul>
<li>a list of retailers’ websites</li>
</ul>
<p>输出:</p>
<ul>
<li>高质量的产品</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://www.ijcseonline.org/pub_paper/IJCSE-00308.pdf" target="_blank" rel="external">Deep Web Data Scraper: Search Engine </a></li>
</ul>
<p>In this paper, we discussed <strong>the problem faced by users</strong> in scraping the information from the deep web and also discussed the solution of these problems by using our new approach.</p>
<p>In future, this work can be extended by finding the more appropriate method, <strong>how efficiently we can store our data in repository and fast get accessed</strong>. So that overall efficiency of searching can be improved. </p>
<ul>
<li><a href="https://dl.acm.org/citation.cfm?id=2433442" target="_blank" rel="external">Crawling Deep Web Entity Pages</a></li>
</ul>
<p>Although crawling such entity-oriented content is clearly useful for a variety of purposes, existing crawling techniques optimized for document oriented content are not best suited for entity-oriented sites. In this work, we describe a prototype system we have built that specializes in crawling <strong>entity-oriented deep-web sites</strong>. We propose techniques tailored to tackle important subproblems including query generation, empty page filtering and URL deduplication in the specific context of entity oriented deep-web sites.</p>
<p>While these techniques are shown to be useful, our experience points to a few areas that warrant future studies. For example, in the template generation, our parsing approach <strong>only handles HTML “GET” forms</strong> but not “POST” forms or javascript forms, which reduces site coverage. In query generation, although Freebase-based entity expansion is useful, certain sites with low traffic or diverse traffic do not get matched with Freebase types effectively using query logs alone. Utilizing additional signals (e.g., entities bootstrapped from crawled pages) for entity expansion is an interesting area. Efficiently enumerate entity query for search forms with multiple input fields is another interesting challenge.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.kunzhao.org/2017/11/09/deep-web-paper/" data-id="cjcdlsfxh003odiemdge3j784" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-software-engineering" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/11/09/software-engineering/" class="article-date">
  <time datetime="2017-11-09T07:18:16.000Z" itemprop="datePublished">2017-11-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/开发者手册/">开发者手册</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/blog/2017/11/09/software-engineering/">software-engineering</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.kunzhao.org/2017/11/09/software-engineering/" data-id="cjcdlsfyt005vdiemlgc2aeyo" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-culture-of-the-computer-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/11/03/culture-of-the-computer-world/" class="article-date">
  <time datetime="2017-11-03T12:37:41.000Z" itemprop="datePublished">2017-11-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/开发者手册/">开发者手册</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/blog/2017/11/03/culture-of-the-computer-world/">The Culture of the Computer World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="The-Culture-of-the-Computer-World"><a href="#The-Culture-of-the-Computer-World" class="headerlink" title="The Culture of the Computer World"></a>The Culture of the Computer World</h2><h3 id="为什么书呆子不受欢迎"><a href="#为什么书呆子不受欢迎" class="headerlink" title="为什么书呆子不受欢迎"></a>为什么书呆子不受欢迎</h3><p>答案就是他们真的不想让自己受欢迎。 文艺复兴时期的代表人物阿尔伯蒂有一句名言： “任何一种艺术， 不管是否重要，如果你想要在该领域出类拔萃， 就必须全身心投入。 ”虽然“书呆子”饱尝不受欢迎之苦， 但是为了解除痛苦而让他们放弃“聪明”， 我想大多数人是不会愿意的。 对他们来说， 平庸的智力是不可忍受的。</p>
<h3 id="《极客与团队》"><a href="#《极客与团队》" class="headerlink" title="《极客与团队》"></a>《极客与团队》</h3><h4 id="1-天才程序员的传说"><a href="#1-天才程序员的传说" class="headerlink" title="1. 天才程序员的传说"></a>1. 天才程序员的传说</h4><ul>
<li>能让 Google Code 上的 Subversion 隐藏某个分之么?</li>
<li>能不能实现这样的功能: 先把新建项目隐藏起来，等到准备妥当的时候再公开发布?</li>
<li>我想推倒重来，能不能删掉整个历史记录呢?</li>
</ul>
<p>这里的要害就是<strong>缺乏安全感</strong>，人们不喜欢自己做到一半的事情被别人指手画脚。</p>
<hr>
<p><strong>隐瞒是有害的</strong>:如果你一直单打独斗的话，你其实是增加了自己失败的风险，而且浪费了自己成长的可能性。</p>
<p>真正做出产品之前不愿分享好创意实际上是一场很大的赌博。一开始就踏错步的概率总是很高的，越早征求意见和反馈，就越能把风险降低。记住这句久经考验的原则—-<strong>“确保失败尽早发生，尽快发生，经常发生”</strong>。 相比担心自己的创意被偷走或是被别人笑话，你更应该担心自己是不是在错误的方向上浪费了大量时间。<strong>一个人躲在自己的小黑屋抖聪明是没用的</strong>。</p>
<p><strong>公车因子</strong>: 一个项目里，需要有多少人被公车撞到才能令其完全瘫痪。</p>
<hr>
<p>社交技巧三支柱: 谦虚、尊重、信任。</p>
<ul>
<li>谦虚: 不要去担心你的个人形象是不是高大，而应努力去营造一种团队和集体荣誉感。</li>
<li>尊重: 只有真正关心对方的人才会提出建设性意见，希望对方在自身或是工作上有所进步。所以学者尊重对方，礼貌地给出建设性批评吧。</li>
<li>信任对方:</li>
</ul>
<p>“老兄，你完全把这个方法的控制流程给弄错啦。你应该和大家一样用标准的 xyzzy 代码模式才对。”</p>
<p>(谦虚地把问题归到自己头上) “嗨~我有点看不懂这段代码的控制流程。要是用 xyzzy 代码模式的话会不户更清楚一点?维护起来也方便?”</p>
<p><strong>接受意见改变自己没什么大不了的</strong>。不要随意挑起争斗。不要忘了，要别人听你话，首先是要学会当一个听众。承认自己犯错或是无知从长远来讲其实能提升你的形象。事实上它蕴含了 HRT 的全部方面；它对完表示了”谦虚”，这是有责任心、负责的态度，这也是表示“信任”别人意见的态度，同时作为回报，别人也会因为你的诚实和坚强而“尊重”你。</p>
<h4 id="3-团队文化"><a href="#3-团队文化" class="headerlink" title="3. 团队文化"></a>3. 团队文化</h4><p>团队或者公司的创始人决定了团队大部分的特点，但它还是会随着时间不断变化发展。</p>
<p>大多数工程师都会犯的第一个错误是假设建设团队文化是负责人的事。</p>
<p>如果你想要优秀的工程师为自己的团队工作，首要的就是雇佣出色的工程师！</p>
<p>沟通的指导原则之一就是在同步沟通的时候（比如开会），人越少越好。而在异步沟通的时候（比如 E-mail)，涉及的听众越多越好。</p>
<hr>
<p><strong>设计文档</strong>:</p>
<p>开发新项目的时候，往往很难抑制住那种想要立刻开始写代码的冲动，但是这么做的结果往往是糟糕的（除非你只是打算很快地做出一个粗糙的原型出来）。</p>
<hr>
<p><strong>在线聊天</strong>:</p>
<p>害怕自己会出傻问题的不安全感会让人更倾向于一对一的讨论，以避免当众出丑的风险。可惜这么做只会给团队增加负担，因为这样一来知识就无法分享，同事之间可能会重复不断地问同一个问题。</p>
<h4 id="4-大海航行靠船长"><a href="#4-大海航行靠船长" class="headerlink" title="4. 大海航行靠船长"></a>4. 大海航行靠船长</h4><p>经理若能表明对手下员工的信任，员工就会感受到正面压力，并努力让自己对得起这份信任。</p>
<p>作为主管，不要忘记你创造的是不同的价值。</p>
<p>仆人式领导要为团队填补前进道路上的裂缝，并在必要的时候给予建议，同时还要勇于冲到第一线。仆人式领导唯一要做的管理工作就是对团队的技术和人事健康状况负责。</p>
<hr>
<p>找到对的那个人的成本（不管是付给面试官的钱，还是花在广告上的钱，还是寻找推荐的费用）比起招到一个不应该招的员工的代价来绝对是微不足道的。</p>
<p>通常你怎么待人，他们就会怎么处事。所以如果你把他们当成孩子或者犯人，那么别吃惊，他们的行为真的会像孩子和犯人一样。</p>
<hr>
<p>人们对有勇气道歉的领袖反而会倍加尊敬，和广泛的认知相反，道歉并不会降低你的威信。</p>
<p>CEO 只要动一动，就能让六七层之外的工程师齿轮飞速旋转！</p>
<hr>
<p>工程师来问你建议通常不是要你去解决他的问题，而是要你<strong>帮助他</strong>解决问题，所以最简单的方法应该是问问题。</p>
<hr>
<p>遭受失败的时候<strong>指责个人不利于团结</strong>，而且会从根本上阻碍承担风险的意愿。</p>
<p>刚开始的时候，指导你的手下，给他们自学的机会是非常困难的事情，但这却是优秀领导必不可少的素质。事实上，你只需要具备三个条件就基本可以胜任了:</p>
<ul>
<li>熟悉团队的流程和系统</li>
<li>向他人解释事物的能力</li>
<li>估计被指导的人到底需要多少帮助的能力</li>
</ul>
<p>只要帮助团队设定好方向和目标，你就可以放手给他们更多的自主性，只要定期检查看他们有没有偏离方向就可以了。</p>
<p>“我不会对你撒谎，但是我会告诉你有些事情我不能说，或者我确实不知道。”</p>
<hr>
<p>警惕三明治赞美法</p>
<hr>
<p>我们认识一个主管，他自制了一张表格，列出了没人想干但是一定要完成的脏活累活，然后平均地分配给大家。</p>
<p>在刚刚担任领导职务的时候，你往往需要克制自己，把工作交给其他人去做，即使他们比你要花更多的时间才能完成。如果你已经有了领导的经验，然后去领导一支新的队伍，那么获得团队尊敬，并且跟上大家步伐最简单的办法就是卷起袖管亲自动手—–最好是那些没人愿意做的脏活累活。</p>
<hr>
<p>你应该”雇佣比自己聪明的人”</p>
<hr>
<p><strong>知道什么时候做恶人</strong>:等待只会延缓问题的爆发。</p>
<hr>
<p>真正优秀的主管一般对事物有没有回旋余地都有很准的直觉: “一两天试用说不定可以提升产品的类库”、”发布一个未来十年都需要提供支持的产品”，一定要好好权衡。</p>
<hr>
<p><strong>内部激励和外部激励</strong>: 工程师拥有自主权的意思就是他可以独立工作，不需要别人盯着才能干活。<strong>工程师的水平就和刀刃一样</strong>，只用到不磨刀的话，用不了几年，这把刀就钝了，运气差点干脆就废了。<strong>要让你的工程师有机会学习新技能</strong>，在现有的基础上继续磨炼提高。</p>
<h4 id="4-对付害群之马"><a href="#4-对付害群之马" class="headerlink" title="4. 对付害群之马"></a>4. 对付害群之马</h4><p>要剔走的是<strong>行为本身</strong>，而不是人，单纯地区分好人和坏人是很幼稚的想法。</p>
<p>大多数人在行为出格的时候，要么是没有意识到自己过分了，要么就是根本不在乎别人的感受。<strong>无知和冷漠其实比蓄意更严重</strong>。绝大多数出格的行为都可以归结为缺乏基本的 HRT。</p>
<hr>
<p>太追求完美会变得瞻前顾后、犹豫不决。</p>
<p><strong>别太感情用事</strong>:</p>
<p>你的任务是写出漂亮的软件，没有义务讨好所有人，也不需要一再去证明自己存在的价值。你越是情绪化，就越容易浪费宝贵的时间去写一些激昂的回帖。</p>
<hr>
<p>没有什么坏人处心积虑地想要毁掉你的文化—-大多数人只是被误导了而已；又或者是想要得到认可，同事又不太擅长与人交际罢了。</p>
<h4 id="5-操纵组织的艺术"><a href="#5-操纵组织的艺术" class="headerlink" title="5. 操纵组织的艺术"></a>5. 操纵组织的艺术</h4><p>在完成自己工作的前提下，要求更多的责任。</p>
<p>只要经理够开明，失败其实快速成长的最佳手段，认识自己的极限，然后逐步提升他们。和追求更多责任一样，用于冒险也是展现自己有能力做大事的一种方法。</p>
<p>你的经理不是千里眼，公司里几乎不存在沟通过多的情况，所以别犹豫，一定要在经理问你进展之前，就向他汇报自己在干什么。</p>
<p>害怕失败是糟糕的经理身上最常见的特征。</p>
<hr>
<p>对办公室政治高手最好敬而远之，没什么事的话就躲远点，但不要口无遮拦地在公司里跟人说他的坏话。</p>
<p>在公司里，另外一个寻求改变的办法就是拉拢群众。当你找到足够的人支持你的创意或者是采用某个特定产品的时候，官僚机构常常发现已经民意难违了。</p>
<p>做承诺的时候要谨慎，而干工作的时候要尽最大努力。</p>
<hr>
<ul>
<li><strong>进取性工作</strong>: 界面改进、速度提升、操作性互动增强</li>
<li><strong>防御性工作</strong>: 代码重构、特性重写、修改数据库模式、数据迁移、改进紧急监控</li>
</ul>
<p>不管技术债务有多少，团队也永远不应该花超过三分之一甚至一半的时间和精力去做防御性的工作，否则就等于政治自杀。</p>
<hr>
<p>在对自己职位感到满意的前提下，稍微投资一点精力在获得晋升上面是保护自己的好办法。仔细研读公司的晋升流程，和你的经理聊一聊，看看哪些事情有助于晋升的，然后有条不紊地完成它们。</p>
<hr>
<p>越短的邮件越有机会得到回复。</p>
<h4 id="6-用户也是人"><a href="#6-用户也是人" class="headerlink" title="6. 用户也是人"></a>6. 用户也是人</h4><p>太多选择会让人焦虑和痛苦。</p>
<h3 id="Murphy’s-law-墨菲定律"><a href="#Murphy’s-law-墨菲定律" class="headerlink" title="Murphy’s law (墨菲定律)"></a>Murphy’s law (墨菲定律)</h3><p>从餐桌上掉下的面包片总是涂了果酱的一面着地，电影院里迟到的人总是坐在前排:</p>
<ul>
<li>任何事都没有表面看起来那么简单</li>
<li>所有的事都会比你预计的时间长</li>
<li>会出错的事总会出错</li>
<li>如果你担心某种情况发生，那么它就更有可能发生</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.kunzhao.org/2017/11/03/culture-of-the-computer-world/" data-id="cjcdlsfx2003idiem4xbru6va" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2017/10/26/hadoop/" class="article-date">
  <time datetime="2017-10-26T00:52:22.000Z" itemprop="datePublished">2017-10-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/blog/categories/开发者手册/">开发者手册</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/blog/2017/10/26/hadoop/">Hadoop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><blockquote>
<p>算法再好，通常也难敌更多的数据．</p>
</blockquote>
<h3 id="为什么不能使用数据库加上更多磁盘来做大规模的批量分析"><a href="#为什么不能使用数据库加上更多磁盘来做大规模的批量分析" class="headerlink" title="为什么不能使用数据库加上更多磁盘来做大规模的批量分析"></a>为什么不能使用数据库加上更多磁盘来做大规模的批量分析</h3><p>这个问题来自于磁盘驱动器的发展趋势: <strong>寻址时间的提高速度远远慢于传输速率的提高速度</strong>．</p>
<h3 id="Hadoop-源码最重要的几个库"><a href="#Hadoop-源码最重要的几个库" class="headerlink" title="Hadoop 源码最重要的几个库"></a><code>Hadoop</code> 源码最重要的几个库</h3><p><img src="2017_10_27_20_26_31.png" alt=""></p>
<p>这几个库的系统默认配置文件:</p>
<ul>
<li><code>./hadoop-common-project/hadoop-common/src/main/resources/core-default.xml</code></li>
<li><code>./hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml</code></li>
<li><code>./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml</code></li>
</ul>
<p>相应的这几个库的重要配置文件:</p>
<ul>
<li><code>./hadoop-common-project/hadoop-common/src/main/conf/core-site.xml</code></li>
<li><code>./hadoop-hdfs-project/hadoop-hdfs/src/main/conf/hdfs-site.xml</code></li>
<li><code>./hadoop-mapreduce-project/conf/mapred-site.xml</code></li>
</ul>
<p>相应的最重要的几个脚本:</p>
<ul>
<li><code>./hadoop-common-project/hadoop-common/src/main/bin/start-all.sh</code></li>
<li><code>./hadoop-hdfs-project/hadoop-hdfs/src/main/bin/start-dfs.sh</code></li>
</ul>
<p>API 版本兼容问题:</p>
<ul>
<li><code>org.apache.hadoop.mapred.*</code> 这个包下面包含<strong>旧</strong>的对外编程接口和 <code>MapReduce</code> 各个服务实现</li>
<li><code>org.apache.hadoop.mapreduce.*</code> 这个包下面包含<strong>新</strong>的对外编程接口以及一些新特性</li>
</ul>
<p>自带的例子:</p>
<ul>
<li><code>./hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java</code></li>
</ul>
<hr>
<p>配置文件：</p>
<p><img src="2017_10_31_17_20_21.png" alt=""></p>
<p><code>Configuration</code> 一上来会加载这两个类:</p>
<p><img src="2017_10_31_17_25_47.png" alt=""></p>
<h3 id="WordCount-运行过程"><a href="#WordCount-运行过程" class="headerlink" title="WordCount 运行过程"></a><code>WordCount</code> 运行过程</h3><p><img src="2017_10_27_22_05_31.png" alt=""></p>
<ul>
<li>将数据切分为若干个<strong>输入分片</strong></li>
<li>每个输入分片交给一个<strong>Map Task</strong>处理</li>
<li><code>Map Task</code> 从对应的输入分片中解析出一个个的 <code>key/value</code>,并调用 <code>map()</code> 函数处理</li>
<li>根据 <code>Reduce Task</code> 个数将结果分成若干个分片 <code>Partition</code> 写到本地磁盘</li>
<li>每个 <code>Reduce Task</code> 从每个 <code>Map Task</code> 中读取属于自己的那个 <code>partition</code>,然后用基于排序的方法将 <code>key</code> 相同的数据聚集在一起</li>
<li>调用 <code>reduce()</code> 函数处理,并将结果输出到文件中</li>
</ul>
<p><code>TokenizerMapper</code>:</p>
<p><img src="2017_10_27_22_11_25.png" alt=""></p>
<p><code>IntSumReducer</code>:</p>
<p><img src="2017_10_27_22_12_23.png" alt=""></p>
<h3 id="map-的工作方式"><a href="#map-的工作方式" class="headerlink" title="map 的工作方式"></a><code>map</code> 的工作方式</h3><p><img src="2017_10_26_09_14_46.png" alt=""></p>
<p>这些行以<strong>键/值对</strong>的方式来表示 <code>map</code> 函数:</p>
<p><img src="2017_10_26_09_15_41.png" alt=""></p>
<p>行号会被忽略，<code>map</code> 函数提取年份和气温，并将其作为输出发送:</p>
<p><img src="2017_10_26_09_17_11.png" alt=""></p>
<p><code>map</code> 函数的输出先由　<code>MapReduce</code> 框架处理，然后再被发送到 <code>reduce</code> 函数．这一处理过程根据键来键/值对进行<strong>排序和分组</strong>．<code>reduce</code> 函数会看到如下输入:</p>
<p><img src="2017_10_26_09_18_45.png" alt=""></p>
<p>每年的年份都有一系列气温读数，所有 <code>reduce</code> 函数必须重复这个列表并从中找出最大的读数:</p>
<p><img src="2017_10_26_09_20_42.png" alt=""></p>
<p>整个逻辑数据流:</p>
<p><img src="2017_10_26_09_21_52.png" alt=""></p>
<h3 id="MapReduce-解决的问题"><a href="#MapReduce-解决的问题" class="headerlink" title="MapReduce 解决的问题"></a><code>MapReduce</code> 解决的问题</h3><p>任务可以被分解为多个子问题,且这些子问题相对独立,彼此之间不会有牵制. 基于该特点， <code>MapReduce</code> 编程模型给出了其分布式编程方法， 共分 5 个步骤：</p>
<ul>
<li>迭代（ iteration）。 遍历输入数据， 并将之解析成 key/value 对。</li>
<li>将输入 key/value 对映射（ map） 成另外一些 key/value 对。</li>
<li>依据 key 对中间数据进行分组（ grouping）。</li>
<li>以组为单位对数据进行归约（ reduce）。</li>
<li>迭代。 将最终产生的 key/value 对保存到输出文件中。</li>
</ul>
<p>MapReduce 将计算过程分解成以上 5 个步骤带来的最大好处是<strong>组件化</strong>与<strong>并行化</strong>。</p>
<p><img src="2017_10_27_23_49_07.png" alt=""></p>
<p>应用范围:</p>
<ul>
<li>分布式 <code>grep</code></li>
<li><code>URL</code> 访问频率统计</li>
<li>倒排索引构建</li>
<li>分布式排序</li>
<li><code>Top K</code> 问题</li>
<li><code>K-means</code> 聚类</li>
<li>贝叶斯分类</li>
</ul>
<blockquote>
<p>注: <code>Fibonacci</code> 数值计算不能用 <code>MapReduce</code> 来解决,因为下一个结果依赖于前面的计算结果.</p>
</blockquote>
<h3 id="HDFS-架构"><a href="#HDFS-架构" class="headerlink" title="HDFS 架构"></a><code>HDFS</code> 架构</h3><p><img src="hdfsarchitecture.gif" alt=""></p>
<p>当用户上传一个大的文件到 <code>HDFS</code> 上时， 该文件会被切分成若干个 <code>block</code>， 分别存储到不同的 <code>DataNode</code>； 同时， 为了保证数据可靠， 会将同一个 <code>block</code> 以流水线方式写到<strong>若干个</strong>（ 默认是 3， 该参数可配置） 不同的 <code>DataNode</code> 上。 这种文件切割后存储的过程是对用户透明的。</p>
<h3 id="MapReduce-架构"><a href="#MapReduce-架构" class="headerlink" title="MapReduce 架构"></a><code>MapReduce</code> 架构</h3><p><img src="2017_10_27_22_39_27.png" alt=""></p>
<p><code>Split</code> 和 <code>Block</code> 的对应关系:</p>
<p><img src="2017_10_27_22_45_53.png" alt=""></p>
<p><code>MapTask</code> 执行过程:</p>
<p><img src="2017_10_27_23_26_24.png" alt=""></p>
<p><code>ReduceTask</code> 执行过程:</p>
<p><img src="2017_10_27_23_30_28.png" alt=""></p>
<p><code>Hadoop MapReduce</code> 作业的<strong>生命周期</strong>:</p>
<p><img src="2017_10_27_23_35_58.png" alt=""></p>
<h3 id="提交作业后"><a href="#提交作业后" class="headerlink" title="提交作业后"></a>提交作业后</h3><table>
<thead>
<tr>
<th>字段</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>mapred.mapper.new-api</code></td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td><code>mapred.reducer.new-api</code></td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td><code>yarn.app.mapreduce.client.job.max-retries</code></td>
<td><code>3</code></td>
<td></td>
</tr>
<tr>
<td><code>yarn.app.mapreduce.client.job.retry-interval</code></td>
<td><code>2000</code></td>
<td></td>
</tr>
<tr>
<td><code>mapreduce.job.reduces</code></td>
<td><code>1</code></td>
<td>the number of reduce tasks for this job</td>
</tr>
</tbody>
</table>
<ul>
<li>创建 <code>Cluster</code><ul>
<li>使用 <code>YarnClientProtocolProvider</code> 创建 <code>ClientProtocol</code></li>
<li>如果配置文件设置了 <code>mapreduce.framework.name</code> 为 <code>yarn</code>,就返回 <code>YARNRunner</code></li>
<li>TODO: 猜测可能使用的是 <code>LocalJobRunner</code></li>
</ul>
</li>
<li>假设是 <code>LocalJobRunner</code><ul>
<li>本地 <code>mapreduce.jobtracker.system.dir</code> 目录 : <code>/tmp/hadoop/mapred/system</code></li>
<li><code>stageArea 目录</code> = <code>/tmp/hadoop/mapred/staging</code> + <code>groupName</code> + <code>randid</code> + <code>/.staging</code></li>
</ul>
</li>
<li>检查 <code>OutputSpecs</code>:<ul>
<li>保障 <code>Output</code> 目录不存在</li>
<li><code>jobId</code> = <code>new JobID(&quot;local&quot; + randid, ++jobid)</code></li>
<li><code>submit 目录</code> = <code>stageArea 目录</code> + <code>/</code> + <code>jobId</code></li>
</ul>
</li>
<li>上传资源文件夹<ul>
<li><code>sharedCache</code> 只在 <code>Yarn</code> 平台上才可以使用</li>
<li>默认 <code>replication</code> 是 <code>10</code></li>
<li>如果 <code>submit 目录</code> 已经存在于本地的 <code>HDFS</code> 中, 抛出 <code>IOException</code></li>
<li>授予 <code>submit 目录</code> 一个 <code>0700 (rwx------)</code> 权限</li>
<li>创建 <code>submit 目录</code></li>
<li><code>Job 分布式缓存文件目录</code> = <code>submit 目录</code> + <code>/</code> + <code>files</code></li>
<li><code>Job 分布式缓存库目录</code> = <code>submit 目录</code> + <code>/</code> + <code>libjars</code></li>
<li><code>Job 分布式缓存 archives 目录</code> = <code>submit 目录</code> + <code>/</code> + <code>archives</code></li>
<li><code>job.jar 路径</code> = <code>submit 目录</code> + <code>/</code> + <code>job.jar</code></li>
<li>将用户配置的 <code>tmpfiles</code> 文件拷贝到 <code>Job 分布式缓存目录</code></li>
<li>将用户配置的 <code>tmpjars</code> 文件拷贝到 <code>Job 分布式缓存库目录</code></li>
<li>将用户配置的 <code>tmparchives</code> 文件拷贝到 <code>Job 分布式缓存 archives 目录</code></li>
<li>讲用户配置的 <code>jobJar</code> 路径拷贝到 <code>job.jar</code> 路劲下面<ul>
<li>以上拷贝的时候, 特意对比了两个是不是位于同一个文件系统上,仅仅比对 <code>uri</code> 字符串,没有 <code>DNS Lookup</code></li>
</ul>
</li>
</ul>
</li>
<li>获取 <code>submit 目录</code> + <code>/</code> + <code>job.xml</code><ul>
<li><code>InputFormat</code> 转为 <code>InputSplit[]</code><ul>
<li>获得输入 <code>Path</code> 列表</li>
<li>过滤 <code>_</code> 或者 <code>.</code> 开头的文件</li>
<li>计算目标大小: <code>totalSize / numSplits</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(1) 得到 <code>map-reduce</code> 任务的 <code>InputFormat</code> 表示</strong>:</p>
<ul>
<li>对于每一个文件,获取分区信息:</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">BlockLocation(offset: <span class="number">0</span>, length: BLOCK_SIZE, hosts: &#123;<span class="string">"host1:9866"</span>, <span class="string">"host2:9866, host3:9866"</span>&#125;)</div></pre></td></tr></table></figure>
<ul>
<li>根据文件大小排序,大的排在前面,先运行</li>
</ul>
<p><strong>(2) 获取 <code>Job</code> 提交的队列的名字,默认 <code>default</code></strong>:</p>
<ul>
<li>配置写入 <code>job.xml</code> 文件中</li>
<li>正式提交 <code>Job</code></li>
</ul>
<p><strong>(3) 查询信息:</strong></p>
<ul>
<li>使用 <code>NetworkedJob</code> 封装 <code>Job</code></li>
<li>监控 <code>Job</code> 的运行和状态信息</li>
</ul>
<h3 id="Hadoop-启动"><a href="#Hadoop-启动" class="headerlink" title="Hadoop 启动"></a><code>Hadoop</code> 启动</h3><p>修改三个配置文件:</p>
<p>(1) <code>$HADOOP/etc/hadoop/mapred-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:54311<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<p>(2) <code>$HADOOP/etc/core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/zk/Documents/hadoop-hdfs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:54310<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<p>(3) <code>$HADOOP/etc/hdfs-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
<p><code>Hadoop</code> 启动/停止脚本时需要通过 <code>SSH</code> 发送命令启动相关守护进程,为了避免每次启动/停止 <code>Hadoop</code> 输入密码进行验证,需要设置免密码登录:拷贝本机的 <code>./ssh/id_rsa.pub</code> 到 <code>authorized_keys</code> 中.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 格式化文件系统</span></div><div class="line">hadoop namenode -format</div><div class="line"><span class="comment"># 启动 Hadoop</span></div><div class="line"><span class="variable">$HADOOP</span>/sbin/start-all.sh</div></pre></td></tr></table></figure>
<p>启动之后可以使用 <code>$HADOOP/bin/hadoop</code> 来输入命令,完成各种操作,支持的命令请参考: <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html" target="_blank" rel="external">Commands Guide</a></p>
<h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><p>在 <code>Hadoop MapReduce</code> 中， 序列化的主要作用有两个： <strong>永久存储和进程间通信</strong>。在 <code>Hadoop MapReduce</code> 中， 使一个 Java 对象可序列化的方法是让其对应的类实现 <code>Writable</code> 接口。 但对于 <code>key</code> 而言， 由于它是<strong>数据排序</strong>的关键字， 因此还需要提供比较两个 <code>key</code> 对象的方法。 为此， <code>key</code> 对应类需实现 <code>WritableComparable</code> 接口。</p>
<p><img src="2017_10_31_16_37_07.png" alt=""></p>
<h3 id="报告进度"><a href="#报告进度" class="headerlink" title="报告进度"></a>报告进度</h3><p><code>Reporter</code> 是 <code>MapReduce</code> 提供给应用程序的工具。 如图 3-4 所示， 应用程序可使用 <code>Reporter</code> 中的方法报告完成进度（ progress）、 设定状态消息（ setStatus） 以及更新计数器<br>（ incrCounter）。</p>
<h3 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a><code>InputFormat</code></h3><p><img src="2017_10_31_17_56_09.png" alt=""></p>
<h3 id="必读"><a href="#必读" class="headerlink" title="必读"></a>必读</h3><ul>
<li><a href="https://www.zhihu.com/question/19795366" target="_blank" rel="external">知乎-零基础学习 Hadoop 该如何下手？</a></li>
<li><a href="https://wiki.apache.org/hadoop#Setting_up_a_Hadoop_Cluster" target="_blank" rel="external">配置 <code>Hadoop</code> 集群教程</a><ul>
<li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank" rel="external">Running Hadoop on Ubuntu Linux (Single-Node Cluster)</a></li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.kunzhao.org/2017/10/26/hadoop/" data-id="cjcdlsfxq0047diemep7nzxx3" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/blog/page/2/">&laquo; Prev</a><a class="page-number" href="/blog/">1</a><a class="page-number" href="/blog/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/blog/page/4/">4</a><a class="page-number" href="/blog/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/23/">23</a><a class="extend next" rel="next" href="/blog/page/4/">Next&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    

  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2018/01/06/where-the-time-actually-gone/">时间都去哪儿了</a>
          </li>
        
          <li>
            <a href="/blog/2018/01/05/dev-reflection/">开发反思</a>
          </li>
        
          <li>
            <a href="/blog/2018/01/05/my-microblog/">我的微博</a>
          </li>
        
          <li>
            <a href="/blog/2018/01/04/gcc-basic/">gcc-basic</a>
          </li>
        
          <li>
            <a href="/blog/2018/01/04/java-internal/">Java Internal</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2018 赵坤&nbsp;|&nbsp;
      Theme by <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      Contact&nbsp;|&nbsp;igozhaokun@163.com
    </div>
  </div>
</footer>
 <script src="/blog/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">首页</a>
  
    <a href="/blog/archives" class="mobile-nav-link">归档</a>
  
    <a href="/blog/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/blog/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>

      

  

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js"></script>
  


 <script src="/blog/js/is.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/script.js"></script>
<script src="/blog/js/elevator.js"></script>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>