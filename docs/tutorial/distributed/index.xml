<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式系统与架构设计 on 赵坤的个人网站</title>
    <link>https://kunzhao.org/docs/tutorial/distributed/</link>
    <description>Recent content in 分布式系统与架构设计 on 赵坤的个人网站</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="https://kunzhao.org/docs/tutorial/distributed/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>服务治理</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/it-govern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/it-govern/</guid>
      <description>服务治理 企业治理：对企业 IT 的问题梳理、改进、优化，IT 治理是为业务服务的，涉及到了组织、管理效能、架构、基础资源、应用、数据等治理。
 SOA 治理，技术栈太重
  推荐的微服务工程组织模式
  DevOps 最核心的工作就是构建标准化、规范化和自动化的研发流水线或工具链，实现计划、设计、开发、测试、发布和运维的紧密协同。
 DevOps 通常包含如下工作：
 测试用例管理 测试环境管理 自动化持续构建 (CI) 持续部署 (CD) 发布管理 负载测试 应用系统监控 反馈管理  微服务架构 代理模式 Spring Cloud 使用 Zuul 组件实现代理网关。
缺点：
 网络上多了一次请求，比直连模式慢 网关存在单点隐患  直连模式 缺点：
 服务方、调用方耦合性较强  边车模式 弱耦合 SDK 微服务框架，将直连模式的 SDK 拆分出来，以独立进程和微服务应用部署在同一个操作系统中，使其免受技术选型和开发语言的限制，业界称之为 ServiceMesh。
直连模式架构  服务提供方的 SDK 做了什么?
 将业务逻辑封装成一个远程服务，然后暴露出去。Java 普遍采用的手段是：
 Instrumentation 字节码替换技术 InvocationHandler 动态代理技术，生成代理类，让代理类来负责远程请求的解析匹配和本地真实服务的调用。   服务调用放的 SDK 做了什么?</description>
    </item>
    
    <item>
      <title>服务度量</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/service-measurement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/service-measurement/</guid>
      <description>服务度量  如果你不能度量它，你就无法改进它。&amp;mdash; 彼得德鲁克
 调用量、延时、异常 点: 单次请求指标采集 线：单服务一分钟指标叠加统计 面：单服务时间纬度汇总统计 对性能进行度量 调用耗时分区统计 部分请求落到了远离中心的 256 ~ 512 ms 这个长尾区间，这就意味着系统中存在异常的延时 &amp;ldquo;毛刺&amp;rdquo;，周期性出现的毛刺，和系统脆弱性有关，在高并发、大负载情况下，这种脆弱性会被放大，给系统造成严重影响。
性能横向对比 服务异常纬度 整体错误分部 </description>
    </item>
    
    <item>
      <title>服务管控</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/service-control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/service-control/</guid>
      <description>服务管控 服务负载 随机策略 各个服务器处理能力不同，处理性能弱的会被打趴。可以加上权重：
collection = {A: 5, B:2, C:2, D:1}(1) 策略1
{A,A,A,A,A,B,B,C,C,D}
random.nextInt(10) 缺点就是这个 collection 集合可能会比较大，内存占用大
(2) 策略2
权重换算成长度，先算出总长度，然后再计算出一个偏移量
totalWeight = sum(collection) offset = random.nextInt(totalWeight) 缺点就是选取的时候，需要遍历集合，复杂度 O(n)
轮询策略 如果各个节点权重一致：
[total_request_count + 1] % node_count 如果权重不同，
collection = {A: 5, B:2, C:2, D:1}那么，最高的权重是 5：
[total_request_count + 1] % maxWeight = currentWeight那么 [currentWeight, maxWeight] 就是可用的权重范围。
一致性 Hash 策略 节点数少，节点变动，大量键发生波动，造成数据倾斜，因此可以引入虚拟节点，每个节点通过引入编号计算多个 Hash 值，模拟多个虚拟节点。
限流 漏桶算法 算法类似于餐厅排号就餐，整个餐厅所能容纳的顾客数量是有限的，有出才能有进。
Semaphore sem = new Semaphore(30); if (sem.</description>
    </item>
    
    <item>
      <title>APM 及调用链跟踪</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/apm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/apm/</guid>
      <description>APM 及调用链跟踪  APM: Application Performance Management
 日志埋点  字节码适配自动插码埋点 中间件自动埋点 基于环境语义构建 TraceId  日志采集  RingBuffer 作为日志缓存代替 BlockingQueue，避免锁冲突 避免频繁 I/O：秒级刷盘 压缩：LZO 算法或 Snappy 压缩 无 I/O：  </description>
    </item>
    
    <item>
      <title>体系的深度治理</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/deep-govern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/deep-govern/</guid>
      <description>体系的深度治理 服务分层与业务中台 DevOps DevOps 工具生态 脱颖而出的工具：
 持续集成工具和工具流引擎：Jenkins 环境隔离&amp;amp;构建：Docker Iaas、Paas、Cloud 基础设施即代码 版本管理工具：Git 协同开发工具：Jira  </description>
    </item>
    
    <item>
      <title>分布式事务解决方案汇总</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/transaction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/transaction/</guid>
      <description>分布式事务解决方案汇总  如何保证转账这个操作在两个系统中同时成功呢？
 2PC 每个参与者要实现三个接口：Prepare、Commit、Rollback 三个接口，这就是 XA 协议。
XA 则规范了 TM 与 RM 之间的通信接口，在 TM 与多个 RM 之间形成一个双向通信桥梁，从而在多个数据库资源下保证 ACID 四个特性。
主要的缺点就是：
 性能问题 事务执行到中间，事务协调者宕机，整个事务处于悬而不决的状态。 一个参与者超时了，那么其它参与者应该提交还是回滚呢？ 2PC 主要用在两个数据库之间，而非两个系统之间。  3PC 3PC 把 2PC 的准备阶段分为了准备阶段和预处理阶段，在第一阶段只是询问各个资源节点是否可以执行事务，而在第二阶段，所有的节点反馈可以执行事务，才开始执行事务操作，最后在第三阶段执行提交或回滚操作。并且在事务管理器和资源管理器中都引入了超时机制，如果在第三阶段，资源节点一直无法收到来自资源管理器的提交或回滚请求，它就会在超时之后，继续提交事务。
所以 3PC 可以通过超时机制，避免管理器挂掉所造成的长时间阻塞问题，但其实这样还是无法解决在最后提交全局事务时，由于网络故障无法通知到一些节点的问题，特别是回滚通知，这样会导致事务等待超时从而默认提交。
消息中间件-最终一致性 消息中间件本身如 Kafka 不提供事务消息功能，那么解决步骤如下：
 系统 A 增加一张消息表，消息写入到消息表中和 DB1 的扣钱操作放到一个数据库的事务里，保证原子性。 系统 A 后台程序源源不断地将消息表中的消息传送给消息中间件，失败了也尝试重传。 系统 B 通过消息中间件的 ACK 机制，明确自己是否消费成功。 系统 B 增加判重表，记录处理成功的消息 ID 和消息中间件对应的 offset，实现业务幂等性，应对重复消费问题；如果业务本身有业务数据，可以判断是否重复，那么就无需这个判重表。  消息中间件如 RocketMQ 本身提供事务消息：
RocketMQ 会定期 (默认 1min) 扫描所有的预发送但是还没有确认的消息，回调给发送方，询问这条消息是要发送出去，还是取消。发送方根据自己的业务数，知道这条消息是应该发送出去 (DB 更新成功)，还是应该取消 (DB 更新失败)。</description>
    </item>
    
    <item>
      <title>副本一致性算法</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/multi-replica-consistency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/multi-replica-consistency/</guid>
      <description>副本一致性算法 Paxos Raft  Raft 之所以会出现，主要是因为 Paxos 晦涩难懂，大家表示很难看懂。
  Paxos 可以多点写入，无需选举 Leader，每个节点都可以接受写请求。虽然为了避免活锁问题，Multi Paxos 可以选举一个 Leader，但是也不是强制执行的，允许同一时间有多个 Leader 同时存在。多点写入，这增加了复杂度。 Raft 只能单点写入 任意时刻只能有一个有效的 Leader，只能 Leader 接受写请求，Leader 同步给超过半数的 Follower  日志结构  index: 日志的顺序编号，如 1、2、3、4 &amp;hellip; term: 写入日志的 Leader 所在的任期、轮数，其他地方称之为 epoch commitIndex: 这条日志被复制到了多数的机器上 lastApplied: 哪些日志已经回放到了状态机   term 只会单调递增，日志的顺序满足: 后一条日志的 term &amp;gt;= 前一条日志的 term
 term 作用 (1) 防止脑裂
网络分区恢复，存在两个 Leader，旧 Leader 向 Follower 发送数据，Follower 知道它的 term = 4，就知道它是过期的 Leader，于是拒绝执行写入，同时反馈给老的 Leader，你已经过期了。
(2) term 如何一直递增?</description>
    </item>
    
    <item>
      <title>存储高可用方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/storage-high-availablity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/storage-high-availablity/</guid>
      <description>存储高可用方案 数据如何复制 主备复制 最简单的复制方案就是主备复制，MySQL、Redis、MongoDB 都提供了这种复制方案。
缺点：故障的时候，需要人工干预，硬件上有浪费。适合于内部的后台管理系统。
主从复制  从意味着是要干活的，也就是从节点提供读的能力，主提供了读写能力。
 缺点：故障的时候，需要人工干预。一般适用于写少读多的业务。
主备倒换/主从倒换 主主复制 因为数据必须满足可以双向复制，因此适合临时性、可丢失、可覆盖的数据场景
数据集群 参考  从零开始学架构 : 照着做，你也能成为架构师  </description>
    </item>
    
    <item>
      <title>业务高可用方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/business-high-availablity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/business-high-availablity/</guid>
      <description>业务高可用方案 高可用是系统级工程 应对机房断电、火灾、城市地震、水灾等极端情况，就需要异地多活架构。
跨城异地和同城异区 银行存款余额、支付宝余额无法做到跨城异地多活。例如，挖掘机挖断光缆后，广州机房和北京机房是不是可以同时转出去1W元？因此只能做同城异区架构(应对机房级别故障)。
 跨城异地和同城异区，是完全两套不同的架构。距离数字上的变化，量变引起了质变，架构复杂度大大提升，网络传输速度降低，中间不可控因素增多。上述这些问题，同城异区也会遇到，但是概率小很多，而且同城异区还可以搭建多套互联通道，成本可控，搭建同城异区，架构上可以将两个机房当作本地机房来设计，无需额外考虑。
 跨城异地多活  优先实现核心业务的异地多活架构 异地多活理论上就不可能很快，物理因素决定的，因此只同步核心业务的数据，保证最终一致性，不保证实时一致性 多种手段同步数据：消息队列、B 中心本机读取失败再去 A 中心读取一次、重新生成数据方式、数据库同步等 只保证大部分用户的异地多活：异地多活无法保证 100% 的业务可用  异地多活设计步骤  业务分级，挑选核心业务：访问量大的业务、核心业务、产生大量收入的业务 数据分类：数据量、数据是否必须唯一 (例如用户 ID)、实时性、可丢失性 (session)、可恢复性 数据同步方案：MySQL 数据同步、消息队列同步、重复生成 异常处理：避免整体业务不可用、修正异常数据、弥补用户损失  通过多个通道同步的方式，来进行异常处理：
 一个走公网，一个走内网 数据可以重复覆盖  通过同步和访问结合方案的设计，来进行异常处理：
 接口走公网，同步走内网 数据有路由规则 优先读取本地数据，然后再通过接口访问  接口级故障应对  核心思想：优先保证核心业务，优先保证绝大部分用户。
 异地多活架构应对系统级别故障，另外一种常见的是接口级别的故障 (访问超时、异常、响应缓慢)。
具体措施：降级 (应对系统自身故障)、熔断 (应对依赖的外部系统故障)、限流 (性能压测确定阈值)、排队 (限流的变种)
参考  《从零开始学架构 : 照着做，你也能成为架构师》  </description>
    </item>
    
    <item>
      <title>高并发设计方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/high-concurrency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/high-concurrency/</guid>
      <description>高并发设计方案 高并发读 高并发读的设计思路主要是：加缓存 (Redis、MySQL 的 Slave、CDN)、并发读 (异步 RPC、Google 提出的冗余请求)、重写轻读、提前计算好多个表的关联查询 (定时计算、实时计算)、CQRS (Command Query Responsibility Separation 读写分离)
Google 的冗余请求是指：客户端首先发送一个请求，并等待服务器返回，如果一定时间内未返回，则马上给另外一台服务器发送同样的请求，客户端等待第一个响应到达之后，终止其他请求的处理。这个一定的时间是指：95% 请求的响应时间。
微博的重写轻读方案：
每个人的收件箱是存储在内存中的，需要为这个队列 (Redis 的 &amp;lt;key, list&amp;gt; 实现) 设置一个上限，比如 Twitter 设置的上限是 800。
超出 800 的微博放到 MySQL 中，可以按照 user_id、time 等同时进行分片，然后可以再引入二级索引表：&amp;lt;user_id, month, count&amp;gt; 来查询某个用户在某个月份发表的微博的总数量，根据这个表可以快速定位到 offset = 5000 的微博发生在哪个月份，也就是数据库的分片。
至于粉丝数量比较大的，可以读的时候实时聚合，或者叫做拉。
一个人关注的人当中，有的人是推给他的，有的人是需要去拉的，需要聚合两者，再按时间排序，然后分页显示，这就是推拉结合。
读写分离的典型架构：
高并发写 一般采用的思路就是：数据分片 (分库分表)、任务分片 (Map/Reduce、Tomcat 1+N+M)、异步化 (通过队列发送短信验证码)、串行化+多进程单线程+异步I/O
发送短信验证码是异步的：
广告系统的扣费是异步化的：
LSM 树是异步落盘，提高写入性能的：
Pipeline 也属于异步化，Leader 一批批地处理消息：
容量规划  机器数量怎么计算?
 机器数 = 预估总流量/单机容量  分母是预估(通过历史数据估算，过去24小时的调用量分布，取其中的峰值，再乘以一个系数，比如2倍、3倍)的值 分子通过压力测试得到  必须使用峰值测算，不能用均值，虽然持续时间短，可是没办法，的确需要这么多台机器，这也正是云计算 (弹性计算) 要解决的问题。</description>
    </item>
    
    <item>
      <title>分布式锁 🔒</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/distributed-lock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/distributed-lock/</guid>
      <description>分布式锁 🔒 MySQL 分布式锁 表记录 CREATE TABLE `database_lock` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `resource` int NOT NULL COMMENT &amp;#39;锁定的资源&amp;#39;, `description` varchar(1024) NOT NULL DEFAULT &amp;#34;&amp;#34; COMMENT &amp;#39;描述&amp;#39;, PRIMARY KEY (`id`), UNIQUE KEY `uiq_idx_resource` (`resource`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&amp;#39;数据库分布式锁表&amp;#39;; 获取锁：
INSERT INTO database_lock(resource, description) VALUES (1, &amp;#39;lock&amp;#39;); 释放锁的时，可以删除这条数据：
DELETE FROM database_lock WHERE resource = 1; 缺点：
 这种锁没有失效时间，一旦释放锁的操作失败就会导致锁记录一直在数据库中，其它线程无法获得锁。这个缺陷也很好解决，比如可以做一个定时任务去定时清理。 这种锁的可靠性依赖于数据库。建议设置备库，避免单点，进一步提高可靠性。 这种锁是非阻塞的，因为插入数据失败之后会直接报错，想要获得锁就需要再次操作。如果需要阻塞式的，可以弄个for循环、while循环之类的，直至INSERT成功再返回。 这种锁也是非可重入的，因为同一个线程在没有释放锁之前无法再次获得锁，因为数据库中已经存在同一份记录了。想要实现可重入锁，可以在数据库中添加一些字段，比如获得锁的主机信息、线程信息等，那么在再次获得锁的时候可以先查询数据，如果当前的主机信息和线程信息等能被查到的话，可以直接把锁分配给它。  悲观锁 我们必须关闭 MySQL 数据库的自动提交属性，因为 MySQL 默认使用autocommit 模式，也就是说，当你执行一个更新操作后，MySQL 会立刻将结果进行提交。</description>
    </item>
    
    <item>
      <title>分布式 ID 设计</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/distributed-id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/distributed-id/</guid>
      <description>分布式 ID 设计 Twitter Snowflake 整体长度通常是 64 （1 + 41 + 10+ 12 = 64）位，适合使用 Java 语言中的 long 类型来存储。头部是 1 位的正负标识位。紧跟着的高位部分包含 41 位时间戳，通常使用 System.currentTimeMillis()。后面是 10 位的 WorkerID，标准定义是 5 位数据中心 + 5 位机器 ID，组成了机器编号，以区分不同的集群节点。最后的 12 位就是单位毫秒内可生成的序列号数目的理论极限。
基于 Java 的实现有很多：Snowflake。
MongoDB 的 ObjectId 提供了一个 12 byte（96 位）的 ID 定义，其中 32 位用于记录以秒为单位的时间，机器 ID 则为 24 位，16 位用作进程 ID，24 位随机起始的计数序列。
微信的 seqsvr  微信序列号生成器架构设计及演变  百度的 UidGenerator  UidGenerator  </description>
    </item>
    
    <item>
      <title>设计微博系统</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/design-weibo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/design-weibo/</guid>
      <description>设计微博系统 架构 信息流聚合一般有三种架构：推模式、拉模式以及推拉结合。
针对关注的粉丝量大的用户采用拉模式，而对于一般用户来说，他们的粉丝量有限，采用推模式问题不大，这样的话一个用户要获取所有关注人的微博，一方面要请求粉丝量大的关注人的发件箱列表，另一方面要请求自己的收件箱列表，再把两者聚合在一起就可以得到完整的 Feed 了。虽然推拉结合的方式看似更加合理，但是由此带来的业务复杂度就比较高了，因为用户的粉丝数是不断变化的，所以对于哪些用户使用推模式，哪些用户使用拉模式，维护起来成本就很高了。所以综合考量下来，微博 Feed 采用了拉模式。
前面提到采用拉模式的话，需要拉取所有关注人的发件箱，在关注人只有几十几百个的时候，获取效率还是非常高的。但是当关注人上千以后，耗时就会增加很多，实际验证获取超过 4000 个用户的发件箱，耗时要几百 ms，并且长尾请求（也就是单次请求耗时超过 1s）的概率也会大大增加。为了解决关注人数上千的用户拉取 Feed 效率低的问题，我们采用了分而治之的思想，在拉取之前把用户的关注人分为几组，并行拉取，这样的话就把一次性的聚合计算操作给分解成多次聚合计算操作，最后再把多次聚合计算操作的结果汇总在一起，类似于 MapReduce 的思路。经过我们的实际验证，通过这种方法可以有效地降级关注人数上千用户拉取 Feed 的耗时，长尾请求的数量也大大减少了。
存储 UID range 作为分片 UID hash 作为分片 </description>
    </item>
    
    <item>
      <title>排查问题</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/troubleshoot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/troubleshoot/</guid>
      <description>排查问题 一个根本原因，经过一条或几条传播路径，最后表现出某些现象。
监控  服务表现：问题的具体表现（出错、超时），应用日志、依赖服务的状态等 系统状态：操作系统指标（各种资源状态、系统日志等）、VM 指标（主要是 GC） 硬件指标：CPU、内存、网络、硬盘是否达到瓶颈  业务指标可以通过框架输出日志 + ELK/graphite 之类生成图形，系统监控可以用 Cacti/Zabbix 进行监控。
3 分钟  30 秒获取整体服务情况：请求量、响应时间分布、错误码分布，主要利用的就是业务的监控系统 3 分钟了解某台机器的负载情况：最耗 CPU 的线程和函数（CPU）、TCP 连接状态统计和 buffer 堆积状态 （网络）、程序的内存分布、最耗内存的对象（内存）、当前是哪个程序在占用磁盘 I/O、GC 情况。主要用的就是 Linux 和 Java 的一些工具：top、perf、netstat、iftop、jmap、jstat 等 3 分钟了解请求的链路情况：网络传输、系统调用、库函数调用、应用层函数调用的调用链、输入、输出、时长。TCPdump/strace/ltrace/btrace/housemd 等 3 分钟检索当前系统的快照情况：线程栈情况、某个变量的值、存储或缓存里的某个值是什么。jmap/jstack/gdb/pmap 等  保留现场 系统出错，首先要解决问题，通过运维的介入把服务恢复，同时尽量保留现场 （比如保留一台出问题的机器，只摘除不重启）。其次是通过监控、日志初步定为问题原因后，在线下使用测试环境压测、TCPcopy 等复现问题，这时再排除就没什么心理负担了。
请求 block 或者变慢的时候，用 jstack/jmap/jstat 之类的都来一遍，其他类型的 Linux 程序主要会留 gcore 和各种指标类的数据，top/perf/strace。
jdump 命令需要很长时间，线上无法服务，应该先摘掉机器，再进行 dump，如果无法摘，则考虑 btrace/housemd 挂到进程上分析，不过 btrace 可能会导致应用假死，几率是几十分之一，慎用。
 参考  《高可用系统 - 第一卷》  </description>
    </item>
    
  </channel>
</rss>
