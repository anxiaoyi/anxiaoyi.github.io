<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式系统与架构设计 on 赵坤的个人网站</title>
    <link>https://kunzhao.org/docs/tutorial/distributed/</link>
    <description>Recent content in 分布式系统与架构设计 on 赵坤的个人网站</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="https://kunzhao.org/docs/tutorial/distributed/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>服务治理</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/it-govern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/it-govern/</guid>
      <description>服务治理 企业治理：对企业 IT 的问题梳理、改进、优化，IT 治理是为业务服务的，涉及到了组织、管理效能、架构、基础资源、应用、数据等治理。
 SOA 治理，技术栈太重
  推荐的微服务工程组织模式
  DevOps 最核心的工作就是构建标准化、规范化和自动化的研发流水线或工具链，实现计划、设计、开发、测试、发布和运维的紧密协同。
 DevOps 通常包含如下工作：
 测试用例管理 测试环境管理 自动化持续构建 (CI) 持续部署 (CD) 发布管理 负载测试 应用系统监控 反馈管理  微服务架构 代理模式 Spring Cloud 使用 Zuul 组件实现代理网关。
缺点：
 网络上多了一次请求，比直连模式慢 网关存在单点隐患  直连模式 缺点：
 服务方、调用方耦合性较强  边车模式 弱耦合 SDK 微服务框架，将直连模式的 SDK 拆分出来，以独立进程和微服务应用部署在同一个操作系统中，使其免受技术选型和开发语言的限制，业界称之为 ServiceMesh。
直连模式架构  服务提供方的 SDK 做了什么?
 将业务逻辑封装成一个远程服务，然后暴露出去。Java 普遍采用的手段是：
 Instrumentation 字节码替换技术 InvocationHandler 动态代理技术，生成代理类，让代理类来负责远程请求的解析匹配和本地真实服务的调用。   服务调用放的 SDK 做了什么?</description>
    </item>
    
    <item>
      <title>服务度量</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/service-measurement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/service-measurement/</guid>
      <description>服务度量  如果你不能度量它，你就无法改进它。&amp;mdash; 彼得德鲁克
 调用量、延时、异常 点: 单次请求指标采集 线：单服务一分钟指标叠加统计 面：单服务时间纬度汇总统计 对性能进行度量 调用耗时分区统计 部分请求落到了远离中心的 256 ~ 512 ms 这个长尾区间，这就意味着系统中存在异常的延时 &amp;ldquo;毛刺&amp;rdquo;，周期性出现的毛刺，和系统脆弱性有关，在高并发、大负载情况下，这种脆弱性会被放大，给系统造成严重影响。
性能横向对比 服务异常纬度 整体错误分部 </description>
    </item>
    
    <item>
      <title>服务管控</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/service-control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/service-control/</guid>
      <description>服务管控 服务负载 随机策略 各个服务器处理能力不同，处理性能弱的会被打趴。可以加上权重：
collection = {A: 5, B:2, C:2, D:1}(1) 策略1
{A,A,A,A,A,B,B,C,C,D}
random.nextInt(10) 缺点就是这个 collection 集合可能会比较大，内存占用大
(2) 策略2
权重换算成长度，先算出总长度，然后再计算出一个偏移量
totalWeight = sum(collection) offset = random.nextInt(totalWeight) 缺点就是选取的时候，需要遍历集合，复杂度 O(n)
轮询策略 如果各个节点权重一致：
[total_request_count + 1] % node_count 如果权重不同，
collection = {A: 5, B:2, C:2, D:1}那么，最高的权重是 5：
[total_request_count + 1] % maxWeight = currentWeight那么 [currentWeight, maxWeight] 就是可用的权重范围。
一致性 Hash 策略 节点数少，节点变动，大量键发生波动，造成数据倾斜，因此可以引入虚拟节点，每个节点通过引入编号计算多个 Hash 值，模拟多个虚拟节点。
限流 漏桶算法 算法类似于餐厅排号就餐，整个餐厅所能容纳的顾客数量是有限的，有出才能有进。
Semaphore sem = new Semaphore(30); if (sem.</description>
    </item>
    
    <item>
      <title>APM 及调用链跟踪</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/apm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/apm/</guid>
      <description>APM 及调用链跟踪  APM: Application Performance Management
 日志埋点  字节码适配自动插码埋点 中间件自动埋点 基于环境语义构建 TraceId  日志采集  RingBuffer 作为日志缓存代替 BlockingQueue，避免锁冲突 避免频繁 I/O：秒级刷盘 压缩：LZO 算法或 Snappy 压缩 无 I/O：  </description>
    </item>
    
    <item>
      <title>体系的深度治理</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/deep-govern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/deep-govern/</guid>
      <description>体系的深度治理 服务分层与业务中台 DevOps DevOps 工具生态 脱颖而出的工具：
 持续集成工具和工具流引擎：Jenkins 环境隔离&amp;amp;构建：Docker Iaas、Paas、Cloud 基础设施即代码 版本管理工具：Git 协同开发工具：Jira  </description>
    </item>
    
    <item>
      <title>分布式事务解决方案汇总</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/transaction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/transaction/</guid>
      <description>分布式事务解决方案汇总  如何保证转账这个操作在两个系统中同时成功呢？
 2PC 每个参与者要实现三个接口：Prepare、Commit、Rollback 三个接口，这就是 XA 协议。
XA 则规范了 TM 与 RM 之间的通信接口，在 TM 与多个 RM 之间形成一个双向通信桥梁，从而在多个数据库资源下保证 ACID 四个特性。
主要的缺点就是：
 性能问题 事务执行到中间，事务协调者宕机，整个事务处于悬而不决的状态。 一个参与者超时了，那么其它参与者应该提交还是回滚呢？ 2PC 主要用在两个数据库之间，而非两个系统之间。  3PC 3PC 把 2PC 的准备阶段分为了准备阶段和预处理阶段，在第一阶段只是询问各个资源节点是否可以执行事务，而在第二阶段，所有的节点反馈可以执行事务，才开始执行事务操作，最后在第三阶段执行提交或回滚操作。并且在事务管理器和资源管理器中都引入了超时机制，如果在第三阶段，资源节点一直无法收到来自资源管理器的提交或回滚请求，它就会在超时之后，继续提交事务。
所以 3PC 可以通过超时机制，避免管理器挂掉所造成的长时间阻塞问题，但其实这样还是无法解决在最后提交全局事务时，由于网络故障无法通知到一些节点的问题，特别是回滚通知，这样会导致事务等待超时从而默认提交。
消息中间件-最终一致性 消息中间件本身如 Kafka 不提供事务消息功能，那么解决步骤如下：
 系统 A 增加一张消息表，消息写入到消息表中和 DB1 的扣钱操作放到一个数据库的事务里，保证原子性。 系统 A 后台程序源源不断地将消息表中的消息传送给消息中间件，失败了也尝试重传。 系统 B 通过消息中间件的 ACK 机制，明确自己是否消费成功。 系统 B 增加判重表，记录处理成功的消息 ID 和消息中间件对应的 offset，实现业务幂等性，应对重复消费问题；如果业务本身有业务数据，可以判断是否重复，那么就无需这个判重表。  消息中间件如 RocketMQ 本身提供事务消息：
RocketMQ 会定期 (默认 1min) 扫描所有的预发送但是还没有确认的消息，回调给发送方，询问这条消息是要发送出去，还是取消。发送方根据自己的业务数，知道这条消息是应该发送出去 (DB 更新成功)，还是应该取消 (DB 更新失败)。</description>
    </item>
    
    <item>
      <title>副本一致性算法</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/multi-replica-consistency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/multi-replica-consistency/</guid>
      <description>副本一致性算法 Paxos 角色 协议将系统中的节点分为三种角色：Proposer（提案者）、Acceptor（决议者）、Leaner（学习者），他们的具体职责为：
 提案者：对key的值提出自己的值； 决议者：对提案者的提议进行投票，选定一个提案，形成最终决策； 学习者：学习决议者达成共识的决策。  决策 在 Paxos 中，一个决策过程（Round、Phase）分为两个阶段：
(1) phase1（准备阶段）：
Proposer向超过半数（n/2+1）Acceptor发起prepare消息(发送编号)； 如果Acceptor 收到一个编号为 N 的 Prepare 请求，且 N 大于该 Acceptor 已经响应过的所有 Prepare 请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给 Proposer，同时该Acceptor 承诺不再接受任何编号小于 N 的提案。否则拒绝返回。
(2) phase2（决议阶段或投票阶段）：
如果超过半数 Acceptor 回复 promise，Proposer向Acceptor发送accept消息。注意此时的V：V 就是收到的响应中编号最大的提案的，如果响应中不包含任何提案，那么V 就由 Proposer 自己决定； Acceptor检查accept消息是否符合规则，只要该 Acceptor 没有对编号大于 N 的 Prepare 请求做出过响应，它就接受该提案。
在实际发展中，Paxos算法也演变出一系列变种：
PBFT（实用拜占庭容错）算法：是一种共识算法，较高效地解决了拜占庭将军问题； Multi-Paxos算法：优化了prepare阶段的效率，同时允许多个Leader并发提议；以及FastPaxos、EPaxos等，这些演变是针对某些问题进行的优化，内核思想还是依托于Paxos。
Raft  Raft 之所以会出现，主要是因为 Paxos 晦涩难懂，大家表示很难看懂。
  Paxos 可以多点写入，无需选举 Leader，每个节点都可以接受写请求。虽然为了避免活锁问题，Multi Paxos 可以选举一个 Leader，但是也不是强制执行的，允许同一时间有多个 Leader 同时存在。多点写入，这增加了复杂度。 Raft 只能单点写入 任意时刻只能有一个有效的 Leader，只能 Leader 接受写请求，Leader 同步给超过半数的 Follower  角色 在 Raft 中，节点被分为 Leader、Follower、Candidate 三种角色：</description>
    </item>
    
    <item>
      <title>存储高可用方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/storage-high-availablity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/storage-high-availablity/</guid>
      <description>存储高可用方案 数据如何复制 主备复制 最简单的复制方案就是主备复制，MySQL、Redis、MongoDB 都提供了这种复制方案。
缺点：故障的时候，需要人工干预，硬件上有浪费。适合于内部的后台管理系统。
主从复制  从意味着是要干活的，也就是从节点提供读的能力，主提供了读写能力。
 缺点：故障的时候，需要人工干预。一般适用于写少读多的业务。
主备倒换/主从倒换 主主复制 因为数据必须满足可以双向复制，因此适合临时性、可丢失、可覆盖的数据场景
数据集群 参考  从零开始学架构 : 照着做，你也能成为架构师  </description>
    </item>
    
    <item>
      <title>业务高可用方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/business-high-availablity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/business-high-availablity/</guid>
      <description>业务高可用方案 高可用是系统级工程 应对机房断电、火灾、城市地震、水灾等极端情况，就需要异地多活架构。
跨城异地和同城异区 银行存款余额、支付宝余额无法做到跨城异地多活。例如，挖掘机挖断光缆后，广州机房和北京机房是不是可以同时转出去1W元？因此只能做同城异区架构(应对机房级别故障)。
 跨城异地和同城异区，是完全两套不同的架构。距离数字上的变化，量变引起了质变，架构复杂度大大提升，网络传输速度降低，中间不可控因素增多。上述这些问题，同城异区也会遇到，但是概率小很多，而且同城异区还可以搭建多套互联通道，成本可控，搭建同城异区，架构上可以将两个机房当作本地机房来设计，无需额外考虑。
 跨城异地多活  优先实现核心业务的异地多活架构 异地多活理论上就不可能很快，物理因素决定的，因此只同步核心业务的数据，保证最终一致性，不保证实时一致性 多种手段同步数据：消息队列、B 中心本机读取失败再去 A 中心读取一次、重新生成数据方式、数据库同步等 只保证大部分用户的异地多活：异地多活无法保证 100% 的业务可用  异地多活设计步骤  业务分级，挑选核心业务：访问量大的业务、核心业务、产生大量收入的业务 数据分类：数据量、数据是否必须唯一 (例如用户 ID)、实时性、可丢失性 (session)、可恢复性 数据同步方案：MySQL 数据同步、消息队列同步、重复生成 异常处理：避免整体业务不可用、修正异常数据、弥补用户损失  通过多个通道同步的方式，来进行异常处理：
 一个走公网，一个走内网 数据可以重复覆盖  通过同步和访问结合方案的设计，来进行异常处理：
 接口走公网，同步走内网 数据有路由规则 优先读取本地数据，然后再通过接口访问  接口级故障应对  核心思想：优先保证核心业务，优先保证绝大部分用户。
 异地多活架构应对系统级别故障，另外一种常见的是接口级别的故障 (访问超时、异常、响应缓慢)。
具体措施：降级 (应对系统自身故障)、熔断 (应对依赖的外部系统故障)、限流 (性能压测确定阈值)、排队 (限流的变种)
参考  《从零开始学架构 : 照着做，你也能成为架构师》  </description>
    </item>
    
    <item>
      <title>高并发设计方案</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/high-concurrency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/high-concurrency/</guid>
      <description>高并发设计方案 高并发读 高并发读的设计思路主要是：加缓存 (Redis、MySQL 的 Slave、CDN)、并发读 (异步 RPC、Google 提出的冗余请求)、重写轻读、提前计算好多个表的关联查询 (定时计算、实时计算)、CQRS (Command Query Responsibility Separation 读写分离)
Google 的冗余请求是指：客户端首先发送一个请求，并等待服务器返回，如果一定时间内未返回，则马上给另外一台服务器发送同样的请求，客户端等待第一个响应到达之后，终止其他请求的处理。这个一定的时间是指：95% 请求的响应时间。
微博的重写轻读方案：
每个人的收件箱是存储在内存中的，需要为这个队列 (Redis 的 &amp;lt;key, list&amp;gt; 实现) 设置一个上限，比如 Twitter 设置的上限是 800。
超出 800 的微博放到 MySQL 中，可以按照 user_id、time 等同时进行分片，然后可以再引入二级索引表：&amp;lt;user_id, month, count&amp;gt; 来查询某个用户在某个月份发表的微博的总数量，根据这个表可以快速定位到 offset = 5000 的微博发生在哪个月份，也就是数据库的分片。
至于粉丝数量比较大的，可以读的时候实时聚合，或者叫做拉。
一个人关注的人当中，有的人是推给他的，有的人是需要去拉的，需要聚合两者，再按时间排序，然后分页显示，这就是推拉结合。
读写分离的典型架构：
高并发写 一般采用的思路就是：数据分片 (分库分表)、任务分片 (Map/Reduce、Tomcat 1+N+M)、异步化 (通过队列发送短信验证码)、串行化+多进程单线程+异步I/O
发送短信验证码是异步的：
广告系统的扣费是异步化的：
LSM 树是异步落盘，提高写入性能的：
Pipeline 也属于异步化，Leader 一批批地处理消息：
容量规划  机器数量怎么计算?
 机器数 = 预估总流量/单机容量  分母是预估(通过历史数据估算，过去24小时的调用量分布，取其中的峰值，再乘以一个系数，比如2倍、3倍)的值 分子通过压力测试得到  必须使用峰值测算，不能用均值，虽然持续时间短，可是没办法，的确需要这么多台机器，这也正是云计算 (弹性计算) 要解决的问题。</description>
    </item>
    
    <item>
      <title>分布式锁 🔒</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/distributed-lock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/distributed-lock/</guid>
      <description>分布式锁 🔒 MySQL 分布式锁 表记录 CREATE TABLE `database_lock` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `resource` int NOT NULL COMMENT &amp;#39;锁定的资源&amp;#39;, `description` varchar(1024) NOT NULL DEFAULT &amp;#34;&amp;#34; COMMENT &amp;#39;描述&amp;#39;, PRIMARY KEY (`id`), UNIQUE KEY `uiq_idx_resource` (`resource`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&amp;#39;数据库分布式锁表&amp;#39;; 获取锁：
INSERT INTO database_lock(resource, description) VALUES (1, &amp;#39;lock&amp;#39;); 释放锁的时，可以删除这条数据：
DELETE FROM database_lock WHERE resource = 1; 缺点：
 这种锁没有失效时间，一旦释放锁的操作失败就会导致锁记录一直在数据库中，其它线程无法获得锁。这个缺陷也很好解决，比如可以做一个定时任务去定时清理。 这种锁的可靠性依赖于数据库。建议设置备库，避免单点，进一步提高可靠性。 这种锁是非阻塞的，因为插入数据失败之后会直接报错，想要获得锁就需要再次操作。如果需要阻塞式的，可以弄个for循环、while循环之类的，直至INSERT成功再返回。 这种锁也是非可重入的，因为同一个线程在没有释放锁之前无法再次获得锁，因为数据库中已经存在同一份记录了。想要实现可重入锁，可以在数据库中添加一些字段，比如获得锁的主机信息、线程信息等，那么在再次获得锁的时候可以先查询数据，如果当前的主机信息和线程信息等能被查到的话，可以直接把锁分配给它。  悲观锁 我们必须关闭 MySQL 数据库的自动提交属性，因为 MySQL 默认使用autocommit 模式，也就是说，当你执行一个更新操作后，MySQL 会立刻将结果进行提交。</description>
    </item>
    
    <item>
      <title>分布式 ID 设计</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/distributed-id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/distributed-id/</guid>
      <description>分布式 ID 设计 自己设计 ID 类型  QPS 高：那么粒度要粗一些 粒度小：到达毫秒级，每个毫秒预留 10 位顺序号，所以 QPS 最高达到 1024。每毫秒最多产生 1000 多个 ID。  时间同步 使用 Linux 的 crontab 周期性核准服务器时间：
ntpupdate -u pool.ntp.orgpool.ntp.org ReentrantLock 生成序列 long sequence = 0; long lastTimestamp = -1; Lock = new ReentrantLock(); public void populateId(Id id, IdMeta idMeta) { lock.lock(); try { long timestamp = TimeUtils.genTime(); if (timestamp == lastTimestamp) { sequence++; sequence &amp;amp;= idMeta.geSeqBitsMask(); // 比如最多 1024 个  if (sequence == 0) { timestamp = TimeUtils.</description>
    </item>
    
    <item>
      <title>设计微博系统</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/design-weibo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/design-weibo/</guid>
      <description>设计微博系统 架构 信息流聚合一般有三种架构：推模式、拉模式以及推拉结合。
针对关注的粉丝量大的用户采用拉模式，而对于一般用户来说，他们的粉丝量有限，采用推模式问题不大，这样的话一个用户要获取所有关注人的微博，一方面要请求粉丝量大的关注人的发件箱列表，另一方面要请求自己的收件箱列表，再把两者聚合在一起就可以得到完整的 Feed 了。虽然推拉结合的方式看似更加合理，但是由此带来的业务复杂度就比较高了，因为用户的粉丝数是不断变化的，所以对于哪些用户使用推模式，哪些用户使用拉模式，维护起来成本就很高了。所以综合考量下来，微博 Feed 采用了拉模式。
前面提到采用拉模式的话，需要拉取所有关注人的发件箱，在关注人只有几十几百个的时候，获取效率还是非常高的。但是当关注人上千以后，耗时就会增加很多，实际验证获取超过 4000 个用户的发件箱，耗时要几百 ms，并且长尾请求（也就是单次请求耗时超过 1s）的概率也会大大增加。为了解决关注人数上千的用户拉取 Feed 效率低的问题，我们采用了分而治之的思想，在拉取之前把用户的关注人分为几组，并行拉取，这样的话就把一次性的聚合计算操作给分解成多次聚合计算操作，最后再把多次聚合计算操作的结果汇总在一起，类似于 MapReduce 的思路。经过我们的实际验证，通过这种方法可以有效地降级关注人数上千用户拉取 Feed 的耗时，长尾请求的数量也大大减少了。
存储 UID range 作为分片 UID hash 作为分片 关系的存储 (1) 最简单的只需要两张表就够了：
用户信息表：
| user_id | user_info | ...用户关系表，表示，follower_user 关注了 followee_user：
| id | follower_user_id | followee_user_id |
查看 user_a 粉丝多少人：
SELECT COUNT(*) FROM table_relation WHERE followee_user = `user_a`; 查看 user_a 关注了多少人：
SELECT COUNT(*) FROM table_relation WHERE follower_user = `user_a`;  (2) 不过随着用户增长，比如达到1亿，那么平均一对用户关系可能就会有100条关系，那么将会扩展到百亿级别。所以必须水平拆分。用户表好选取拆分的键，就是 user_id。不过关系表，根据 follower_user_id 拆分，那么查询这个人关注了多少人好查询，但是查询某个人有多少粉丝，就需要去所有分片上查询汇总了，相反按照 followee_user_id 拆分，那么这个人查询关注了多少人，就不好查询了。也就是总有一半的场景查询效率低下。</description>
    </item>
    
    <item>
      <title>排查问题</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/troubleshoot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/troubleshoot/</guid>
      <description>排查问题 一个根本原因，经过一条或几条传播路径，最后表现出某些现象。
监控  服务表现：问题的具体表现（出错、超时），应用日志、依赖服务的状态等 系统状态：操作系统指标（各种资源状态、系统日志等）、VM 指标（主要是 GC） 硬件指标：CPU、内存、网络、硬盘是否达到瓶颈  业务指标可以通过框架输出日志 + ELK/graphite 之类生成图形，系统监控可以用 Cacti/Zabbix 进行监控。
3 分钟  30 秒获取整体服务情况：请求量、响应时间分布、错误码分布，主要利用的就是业务的监控系统 3 分钟了解某台机器的负载情况：最耗 CPU 的线程和函数（CPU）、TCP 连接状态统计和 buffer 堆积状态 （网络）、程序的内存分布、最耗内存的对象（内存）、当前是哪个程序在占用磁盘 I/O、GC 情况。主要用的就是 Linux 和 Java 的一些工具：top、perf、netstat、iftop、jmap、jstat 等 3 分钟了解请求的链路情况：网络传输、系统调用、库函数调用、应用层函数调用的调用链、输入、输出、时长。TCPdump/strace/ltrace/btrace/housemd 等 3 分钟检索当前系统的快照情况：线程栈情况、某个变量的值、存储或缓存里的某个值是什么。jmap/jstack/gdb/pmap 等  保留现场 系统出错，首先要解决问题，通过运维的介入把服务恢复，同时尽量保留现场 （比如保留一台出问题的机器，只摘除不重启）。其次是通过监控、日志初步定为问题原因后，在线下使用测试环境压测、TCPcopy 等复现问题，这时再排除就没什么心理负担了。
请求 block 或者变慢的时候，用 jstack/jmap/jstat 之类的都来一遍，其他类型的 Linux 程序主要会留 gcore 和各种指标类的数据，top/perf/strace。
jdump 命令需要很长时间，线上无法服务，应该先摘掉机器，再进行 dump，如果无法摘，则考虑 btrace/housemd 挂到进程上分析，不过 btrace 可能会导致应用假死，几率是几十分之一，慎用。
 参考  《高可用系统 - 第一卷》  </description>
    </item>
    
    <item>
      <title>Hystrix</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/hystrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/hystrix/</guid>
      <description>Hystrix Hystrix 隔离策略  THREAD 隔离：在单独的线程上执行，并发请求受到线程池中线程数量的限制。 SEMAPHORE 隔离：在调用线程上执行，并发请求受到信号计数的限制。  通常情况下，使用信号量隔离的场景，调用量非常的，线程切换开销太高，只适用于非网络调用。</description>
    </item>
    
    <item>
      <title>架构案例参考</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/architecture-cases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/architecture-cases/</guid>
      <description>架构案例参考 本文收集了一些其他文章中所写的，在做系统设计时候的对于一些案例的思考。
在腾讯 QQ 的红包活动中，对于活动的展示、错峰的内容、预下载资源的内容等，都是采取的全局配置策略进行全局控制的。而为了让用户能够错峰进入活动，避免给后台带来瞬间冲击的问题，其采用 uid % gap 的方式对用户的进入时间进行了错峰划分，但是这样带来的问题是有可能身边的人都看到活动入口了，但是自己还看不到入口，所以还应该将地理位置因素也考虑进去：
 根据用户地理位置 adcode 和错峰配置进行映射，得到映射后的分区索引 i； 计算得到一次错峰时间：T1 = T0 + i*interval； 对于同一批次的用户，通过随机时间，将这些用户随机均匀地映射分布到对应较小的时间段内，计算得到二次错峰时间：T2 = T1 + hash(uin)%interval； 得到的二次错峰时间T2即为用户实际可以看到入口参与活动的时间：T = T2； 对于地理位置一次错峰可能出现的异常情况，如用户未授权获取地理位置（占比30%左右）、国外用户无adcode未匹配到分区索引等，客户端可采取一定的兜底策略，如根据用户账号uin进行随机映射到某个分区：i = hash(uin) % regions.count 。  而在活动期间产生的数据的上报流程如下所示，可以看到其上报的时机、上报的方式等都支持非常灵活的调整策略：
而在上报的链路中，SSO接入层和上报服务后台也分别用过载策略和降级策略来应对过载的风险，其后台接口中包含了 reportLevel 和 reportLevelTime 这两个上报降级信息字段。
参考  大流量冲击下，腾讯QQ客户端如何保障春节红包活动的用户体验？  </description>
    </item>
    
    <item>
      <title>设计 Youtube</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/design-youtube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/design-youtube/</guid>
      <description>设计 Youtube 参考  How to Design Youtube (Part II)  </description>
    </item>
    
    <item>
      <title>设计 tinyURL</title>
      <link>https://kunzhao.org/docs/tutorial/distributed/design-tinyurl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/tutorial/distributed/design-tinyurl/</guid>
      <description>设计 TinyURL 设计一个 TinyURL 服务，当输入 https://leetcode.com/problems/design-tinyurl 时，返回 http://tinyurl.com/4e9iAk。
基础想法，a-zA-Z0-9 有 62 个字符，所以可以随机生成一个长度为 6 位的字符串。只要重复了，就一直不停的循坏调用直到生成一个不重复的。
import java.util.concurrent.ThreadLocalRandom; class Codec { static final Map&amp;lt;String, String&amp;gt; shortToLongMap = new HashMap&amp;lt;String, String&amp;gt;(); static final Map&amp;lt;String, String&amp;gt; LongToShortMap = new HashMap&amp;lt;String, String&amp;gt;(); static final String BASE_HOST = &amp;#34;http://tinyurl.com/&amp;#34;; static final int K = 6; // Encodes a URL to a shortened URL.  public String encode(String longUrl) { if(LongToShortMap.containsKey(longUrl)) return LongToShortMap.get(longUrl); String shortUrl = generateRandomShortUrl(); while(shortToLongMap.</description>
    </item>
    
  </channel>
</rss>
