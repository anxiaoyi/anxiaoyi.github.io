<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>云&#43;社区技术沙龙 on 赵坤的个人网站</title>
    <link>https://kunzhao.org/docs/cloud-plus-bbs/</link>
    <description>Recent content in 云&#43;社区技术沙龙 on 赵坤的个人网站</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="https://kunzhao.org/docs/cloud-plus-bbs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>B站高可用架构实践</title>
      <link>https://kunzhao.org/docs/cloud-plus-bbs/bilibili-high-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/cloud-plus-bbs/bilibili-high-availability/</guid>
      <description>B站高可用架构实践 流量洪峰下要做好高服务质量的架构是一件具备挑战的事情，从Google SRE的系统方法论以及实际业务的应对过程中出发，分享一些体系化的可用性设计。对我们了解系统的全貌上下游的联防有更进一步的了解。
负载均衡 BFE 就是指边缘节点，BFE 选择下游 IDC 的逻辑权衡：
 离 BFE 节点比较近的 基于带宽的调度策略 某个 IDC 的流量已经过载，选择另外一个 IDC  当流量走到某个 IDC 时，这个流量应该如何进行负载均衡？
问题：RPC 定时发送的 ping-pong，也即 healthcheck，占用资源也非常多。服务 A 需要与账号服务维持长连接发送 ping-pong，服务 B 也需要维持长连接发送 ping-pong。这个服务越底层，一般依赖和引用这个服务的资源就越多，一旦有任何抖动，那么产生的这个故障面是很大的。那么应该如何解决？
解决：以前是一个 client 跟所有的 backend 建立连接，做负载均衡。现在引入一个新的算法，子集选择算法，一个 client 跟一小部分的 backend 建立连接。图片中示例的算法，是从《Site Reliability Engineering》这本书里看的。
如何规避单集群抖动带来的问题？多集群。
如上述图片所示，如果采用的是 JSQ 负载均衡算法，那么对于 LBA 它一定是选择 Server Y 这个节点。但如果站在全局的视角来看，就肯定不会选择 Server Y 了，因此这个算法缺乏一个全局的视角。
如果微服务采用的是 Java 语言开发，当它处于 GC 或者 FullGC 的时候，这个时候发一个请求过去，那么它的 latency 肯定会变得非常高，可能会产生过载。
新启动的节点，JVM 会做 JIT，每次新启动都会抖动一波，那么就需要考虑如何对这个节点做预热？
如上图所示，采用 &amp;ldquo;the choice-of-2&amp;rdquo; 算法后，各个机器的 CPU 负载趋向于收敛，即各个机器的 CPU 负载都差不多。Client 如何拿到后台的 Backend 的各项负载？是采用 Middleware 从 Rpc 的 Response 里面获取的，有很多 RPC 也支持获取元数据信息等。</description>
    </item>
    
    <item>
      <title>穗康小程序口罩预约前后端架构及产品设计</title>
      <link>https://kunzhao.org/docs/cloud-plus-bbs/suikang-mini-program-design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kunzhao.org/docs/cloud-plus-bbs/suikang-mini-program-design/</guid>
      <description>穗康小程序口罩预约前后端架构及产品设计 在战“疫”期间，腾讯与广州市政府合作，在小程序“穗康”上，2天内上线了全国首款口罩预约功能，上线首日访问量1.7亿，累计参与口罩预约人次1400万+。那么，它是如何在2天内开发上线，扛住了超大并发量呢？其背后的前后端架构是怎样的？
无损服务设计 整个流程下来需要 3 个实时接口：
 药店当前口罩的库存情况 哪个时间段有名额 提交预约实时返回结果  有损服务设计 结果，口罩预约关注度远超预期：
下面展示的 UI 的设计：
为什么有 “损” 平衡的理论就是 CAP 理论、BASE 最终一致性：
牺牲强一致换取高可用 两个机房需要同步，并发性差。以下是优化后的代码，引入计时器：
降低了专线依赖。
怎么 &amp;ldquo;损&amp;rdquo;  放弃绝对一致，追求高可用和快速响应 万有一失，用户重试 伸缩调度，降级服务  （1）穗康小程序 引入消息队列，最终一致：
（2）QQ 相册负载高 选择扩容？带宽和存储成本高。
（3）转账 用户重试极少量消息。再想一下微信的红色感叹号，点一下重新发送。
（4）穗康的预约重试 （5）QQ 相册降级 </description>
    </item>
    
  </channel>
</rss>
