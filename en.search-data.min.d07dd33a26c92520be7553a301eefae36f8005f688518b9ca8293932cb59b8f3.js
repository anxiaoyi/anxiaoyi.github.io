'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/posts/war/','title':"War 文件格式",'content':"WAR file (Web Application Resource or Web application ARchive)。WAR 组织文件的标准方式：\nWEB-INF 存储在这个文件夹内的文件，默认情况下浏览器访问不到。\nweb.xml Tomcat 需要\nclasses 所有编译的 class 文件\nlib 包含项目依赖的所有的 JAR 库\ntags 包含 Tag 文件\n参考  Web Modules  "});index.add({'id':1,'href':'/posts/jsp/','title':"JSP",'content':"JSP 脚本 脚本程序可以包含任意量的Java语句、变量、方法或表达式，只要它们在脚本语言中是有效的。\n\u0026lt;% 代码片段 %\u0026gt;\rJSP 声明 \u0026lt;%! int i = 0; %\u0026gt; \u0026lt;%! int a, b, c; %\u0026gt; \u0026lt;%! Circle a = new Circle(2.0); %\u0026gt; JSP 表达式 \u0026lt;p\u0026gt; 今天的日期是: \u0026lt;%= (new java.util.Date()).toLocaleString()%\u0026gt; \u0026lt;/p\u0026gt; JSP 注释 \u0026lt;%-- 该部分注释在网页中不会被显示--%\u0026gt; JSP 指令 \u0026lt;%@ directive attribute=\u0026quot;value\u0026quot; %\u0026gt;\r三种 directive：\n   指令 描述     \u0026lt;%@ page \u0026hellip; %\u0026gt; 定义页面的依赖属性，比如脚本语言、error页面、缓存需求等等   \u0026lt;%@ include \u0026hellip; %\u0026gt; 包含其他文件   \u0026lt;%@ taglib \u0026hellip; %\u0026gt; 引入标签库的定义，可以是自定义标签    \u0026lt;%@ page import=\u0026quot;java.io.*,java.util.*\u0026quot; %\u0026gt;\rJSP 隐含对象 JSP支持九个自动定义的变量：\n   对象 描述     request HttpServletRequest类的实例   response HttpServletResponse类的实例   out PrintWriter类的实例，用于把结果输出至网页上   session HttpSession类的实例   application ServletContext类的实例，与应用上下文有关   config ServletConfig类的实例   pageContext PageContext类的实例，提供对JSP页面所有对象以及命名空间的访问   page 类似于Java类中的this关键字   Exception Exception类的对象，代表发生错误的JSP页面中对应的异常对象    IF \u0026hellip; ELSE \u0026hellip; \u0026lt;% if (day == 1 | day == 7) { %\u0026gt;\r\u0026lt;p\u0026gt;今天是周末\u0026lt;/p\u0026gt;\r\u0026lt;% } else { %\u0026gt;\r\u0026lt;p\u0026gt;今天不是周末\u0026lt;/p\u0026gt;\r\u0026lt;% } %\u0026gt;\r\u0026lt;/body\u0026gt; WHILE \u0026lt;%! int fontSize; %\u0026gt; \u0026lt;%for ( fontSize = 1; fontSize \u0026lt;= 3; fontSize++){ %\u0026gt;\r\u0026lt;font color=\u0026quot;green\u0026quot; size=\u0026quot;\u0026lt;%= fontSize %\u0026gt;\u0026quot;\u0026gt;\r菜鸟教程\r\u0026lt;/font\u0026gt;\u0026lt;br /\u0026gt;\r\u0026lt;%}%\u0026gt;\r中文编码 如果我们要在页面正常显示中文，我们需要在 JSP 文件头部添加以下代码：\n\u0026lt;%@ page language=\u0026quot;java\u0026quot; contentType=\u0026quot;text/html; charset=UTF-8\u0026quot; pageEncoding=\u0026quot;UTF-8\u0026quot;%\u0026gt;\r参考  JSP 语法  "});index.add({'id':2,'href':'/posts/ibmmq/','title':"IBM MQ",'content':"IBM MQ 架构 IBM WebSphere MQ 架构：\nIBM WheSphere MQ 特性：\n 支持事务 具有特殊的技术防止消息重复传送，确保消息一次且仅一次传递  概念 消息 队列  本地队列：位于本地物理磁盘 远程队列：本地应用程序只能往里面放消息，不能直接读消息。只能从本地队列读取消息 传输队列：临时存储将要发送到远程队列的消息 启动队列：触发中使用的队列，触发器触发事件时，将触发器消息发送到启动队列 死信队列：存储无法正确发送到目的地的消息的队列  通道 通道：提供从一个队列管理器到另外一个队列管理器的通信路径\n通道如何使用：\nMQSC MQSC 是用来管理队列管理器等对象的脚本命令，可以使用 runmqsc 向队列管理器发出 MQSC 命令。\nMQSC 的官方命令文档：The MQSC Commands\nMQSC 的一些规则：\n 关键字不区分大小写：ALTER、alter、AlteR 都是一样的 很多命令都有同义词：例如 DEFINE CHANNEL 可以写为 def chl 用单引号引用的字符串，IBM MQ 不做转换处理 每条命令必须以新行开始  Control Commands Control Commands 的官方文档：The control commands\n发送消息步骤 发送消息前，需要启动队列管理器、启动监听器（监听在某个端口）。\n为了把消息从一个队列管理器发送到另一个队列管理器，您需要定义两个通道；一个是在源队列管理器（指明传输队列名、目标系统的IP:PORT），另一个是在目标队列管理器。\n为把消息从一个队列管理器发送到另一个队列管理器，您需要定义六个队列；在源队列管理器需要定义四个（远程队列、启动队列、传输队列、死信队列-推荐），目标队列管理器要定义两个（本地队列、死信队列-推荐）。\n编程接口 Queue Manager // declare an object of type queue manager MQQueueManager queueManager = new MQQueueManager(); MQQueueManager queueManager = new MQQueueManager(\u0026#34;qMgrName\u0026#34;); ... // do something... ... // disconnect from the queue manager queueManager.disconnect(); MQEnvironment MQEnvironment.hostname = \u0026#34;host.domain.com\u0026#34;; MQEnvironment.channel = \u0026#34;java.client.channel\u0026#34;; // 默认 port 是 1414 MQEnvironment.port = nnnn; MQEnvironment.CCSID = 1381;//这个是编码格式的代号,UTF-8为1381 MQEnvironment.userID = \u0026#34;MUSR_MQADMIN\u0026#34;;//这个是登录MQ服务器用的用户名，需要在MQ服务器上设置 MQEnvironment.password = \u0026#34;123456\u0026#34;;//这个是登录MQ服务器用的密码 Queue // 打开队列 MQQueue queue = queueManager.accessQueue(\u0026#34;qName\u0026#34;,CMQC.MQOO_OUTPUT); queue.close(); 参考  《IBM WebShpere MQ 入门教程》 整体教程参考：Writing IBM MQ classes for Java applications  "});index.add({'id':3,'href':'/posts/springboot/','title':"Spring Boot",'content':"Spring Boot 提供了两个接口 CommandLineRunner 和 ApplicationRunner，用以当 Spring Boot 应用程序完全启动之前运行指定的代码。\nCommandLineRunner @Component public class CommandLineAppStartupRunner implements CommandLineRunner { private static final Logger logger = LoggerFactory.getLogger(CommandLineAppStartupRunner.class); @Override public void run(String...args) throws Exception { logger.info(\u0026#34;Application started with command-line arguments: {} . \\n To kill this application, press Ctrl + C.\u0026#34;, Arrays.toString(args)); } } ApplicationRunner 将参数封装为一个对象，可以调用 getOptionNames()、getOptionValues() 和 getSourceArgs() 等便捷的方法。\n@Component public class AppStartupRunner implements ApplicationRunner { private static final Logger logger = LoggerFactory.getLogger(AppStartupRunner.class); @Override public void run(ApplicationArguments args) throws Exception { logger.info(\u0026#34;Your application started with option names : {}\u0026#34;, args.getOptionNames()); } } 排序 你可以注册任意多的 Runners，可以使用 @Order 注解来声明它们运行的顺序。\n参考  Spring Boot: ApplicationRunner and CommandLineRunner  "});index.add({'id':4,'href':'/posts/jax-ws/','title':"JAX-WS",'content':"JAX-WS JAX-WS 代表 Java API for XML Web Service。\nWebService  @WebService 用来将**某个类(一个 Interface)**声明为一个 Web Service EndPoint，这个类的实现类也得需要声明 @WebService 接口类的方法必须 public，并且不能使用 static 或 final 来修饰 接口类的方法必须声明 @WebMethod 实现类必须有一个默认的 public 构造器 实现类不要定义 finalize 方法  Apache CXF 定义 Endpoint，此处的 endpointInterface 非常重要，指向的是 Interface 类全称。\n@WebService(endpointInterface = \u0026#34;com.baeldung.cxf.introduction.Baeldung\u0026#34;) public class BaeldungImpl implements Baeldung {} 查看 WSDL 信息 URL 后面往往跟一个 ?wsdl 字符串。\n底层数据传输 GET WSDL 发送 POST 请求 接受 POST 响应 阅读更多 java 实现WebService 以及不同的调用方式、JAX-WS Web 服务开发调用和数据传输分析\nSOAP SOAP 是 Simple Object Access Protocol 的简称，是基于 XML 的简易协议，可使应用程序在 HTTP 之上进行信息交换。\n参考  Java WS Tutorial Introduction to Apache CXF  "});index.add({'id':5,'href':'/posts/oracle/','title':"Oracle",'content':"内置数据类型    分类 数据类型 介绍     字符 CHAR [(size [BYTE | CHAR])] 定长字符串，占据 n 字节    NCHAR[(size)] 定长字符串，占据 2n 字节    VARCHAR2(size) 可变长度的字符串    NVARCHAR2(size) 可变长度的 UNICODE 字符串   数值 NUMBER(p,s) p 代表精度(1 - 38)，s 代表 scale (-84 - 127)    FLOAT [(p)] 小数，精度不高    LONG 仅仅为了兼容   日期 DATE 大小固定占用 7 bytes    TIMESTAMP    字节 RAW(size) 定长    LONG RAW 变长，图像、声音、文档、数组，建议使用 LOB    LOB     ROWID 伪列 SELECT ROWID from your_table;  ROWID 不能被用作主键。\n ROWNUM 伪列 ROWNUM 是 Oracle 对查询结果自动添加的一个伪列，编号从 1 开始，每一次查询动态生成。\n非排序查询 Top N SELECT stu_no, stu_name, score FROM student WHERE ROWNUM \u0026lt;= 5; 排序查询 Top N SELECT stu_no, stu_name, score FROM ( SELECT stu_no, stu_name, score FROM student ORDER BY score DESC ) WHERE ROWNUM \u0026lt;= 5; 分页查询 SELECT rn, stu_no, stu_name, score FROM ( SELECT ROWNUM rn, stu_no, stu_name, score FROM ( SELECT stu_no, stu_name, score FROM student ORDER BY score DESC ) t1 ) t2 WHERE rn \u0026gt;= 4 AND rn \u0026lt;= 6; 参考自 ORACLE中的TOP-N查询（TOP-N分析）、分页查询\n汉字占用长度  N 开头的字段类型(比如 NCHAR, NVARCHAR2)中，任何一个字符(包括一个汉字)占2个字节，统一的。不以 N 开头的字段类型(比如 CHAR, VARCHAR2)中，unicode 字符(比如汉字)占3个字节，其他字符占1个字节。\n 如何求一个字符串占用的字符数和字节数？ length 函数求得是占用字符数，lengthb或者 vsize 函数求得是占用字节数。你说 中华12 这个字符串占用了多少字符，字节？看 sql 返回值便清楚了。\nselect length(\u0026#39;中华12\u0026#39;) from dual --返回4，也就是占用4个字符 select lengthb(\u0026#39;中华12\u0026#39;) from dual --返回8，也就是占用8个字节，其中中华儿子各占3个字节，而12两个字符各占一个字节 select lengthb(N\u0026#39;中华1\u0026#39;) from dual --返回6，这是将字符串转换成为unicode字符串后，每个字符占用2个字节，3个就是6个字节 select length(N\u0026#39;中华1\u0026#39;) from dual --返回3，因为只有3个字符嘛。 事务 oracle 数据库支持 READ COMMITTED 和 SERIALIZABLE这两种事务隔离级别。默认系统事务隔离级别是 READ COMMITTED，也就是读已提交。\nsetAutoCommit(false) 如下，关闭自动 commit，对于同一个 connection，先 insert，然后 select 是可以查询到刚才 insert 的这行数据的，原因是这行数据在服务器已经执行了，只是对其它事务不可见（事务的隔离性）。比如使用 Navicat 或 DBVisualizer 是查询不到你刚刚 insert 的这条数据的。\ncon.setAutoCommit(false); insertPreparedStatement.executeUpdate(); ResultSet rs = selectPreparedStatement.executeQuery(); boolean exist = rs.next(); // 可以查询到 setAutoCommit(true) 未关闭自动 commit，那么当 executeUpdate 这句话执行结束的时候，就会自动 commit，其他事务（Navicat 或 DBVisualizer）也就可以立即查询到了。\ncon.setAutoCommit(true); insertPreparedStatement.executeUpdate(); // 会自动 commit 参考  ORACLE中一个字符占多少字节？（中文存储） Data Types ROWID Pseudocolumn  "});index.add({'id':6,'href':'/posts/ant/','title':"Ant",'content':"Ant 官方教程 官方教程链接\nAnt Properties 提供一些键值对，使用 ${key} 来获取其 value。官网 列举了很多内置的 properties。\nAnt Classpath 定义 classpath：\n\u0026lt;project name=\u0026#34;HelloWorld\u0026#34; basedir=\u0026#34;.\u0026#34; default=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;path id=\u0026#34;classpath\u0026#34;\u0026gt; \u0026lt;fileset dir=\u0026#34;${lib.dir}\u0026#34; includes=\u0026#34;**/*.jar\u0026#34;/\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;target name=\u0026#34;compile\u0026#34;\u0026gt; \u0026lt;mkdir dir=\u0026#34;${classes.dir}\u0026#34;/\u0026gt; \u0026lt;javac srcdir=\u0026#34;${src.dir}\u0026#34; destdir=\u0026#34;${classes.dir}\u0026#34; classpathref=\u0026#34;classpath\u0026#34;/\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;target name=\u0026#34;run\u0026#34; depends=\u0026#34;jar\u0026#34;\u0026gt; \u0026lt;java fork=\u0026#34;true\u0026#34; classname=\u0026#34;${main-class}\u0026#34;\u0026gt; \u0026lt;classpath\u0026gt; \u0026lt;path refid=\u0026#34;classpath\u0026#34;/\u0026gt; \u0026lt;path location=\u0026#34;${jar.dir}/${ant.project.name}.jar\u0026#34;/\u0026gt; \u0026lt;/classpath\u0026gt; \u0026lt;/java\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;/project\u0026gt; Ant Targets Target 是多个 tasks 的容器，这个 Target 用来完成在整个 build 过程中的某个任务，使之达到某个状态。\n\u0026lt;target name=\u0026#34;A\u0026#34;/\u0026gt; \u0026lt;target name=\u0026#34;B\u0026#34; depends=\u0026#34;A\u0026#34;/\u0026gt; \u0026lt;target name=\u0026#34;C\u0026#34; depends=\u0026#34;B\u0026#34;/\u0026gt; \u0026lt;target name=\u0026#34;D\u0026#34; depends=\u0026#34;C,B,A\u0026#34;/\u0026gt; 调用链：\nCall-Graph: A → B → C → D\r \u0026lt;Target\u0026gt; 示例：\n\u0026lt;project\u0026gt; \u0026lt;target name=\u0026#34;clean\u0026#34;\u0026gt; \u0026lt;delete dir=\u0026#34;build\u0026#34;/\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;target name=\u0026#34;compile\u0026#34;\u0026gt; \u0026lt;mkdir dir=\u0026#34;build/classes\u0026#34;/\u0026gt; \u0026lt;javac srcdir=\u0026#34;src\u0026#34; destdir=\u0026#34;build/classes\u0026#34;/\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;target name=\u0026#34;jar\u0026#34;\u0026gt; \u0026lt;mkdir dir=\u0026#34;build/jar\u0026#34;/\u0026gt; \u0026lt;jar destfile=\u0026#34;build/jar/HelloWorld.jar\u0026#34; basedir=\u0026#34;build/classes\u0026#34;\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;attribute name=\u0026#34;Main-Class\u0026#34; value=\u0026#34;oata.HelloWorld\u0026#34;/\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/jar\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;target name=\u0026#34;run\u0026#34;\u0026gt; \u0026lt;java jar=\u0026#34;build/jar/HelloWorld.jar\u0026#34; fork=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;/project\u0026gt; 参考 Targets\nAnt 所有支持的 Tasks 参考 Overview of Apache Ant Tasks\n"});index.add({'id':7,'href':'/posts/struts2/','title':"Struts 2",'content':"Action 访问 Servlet API 使用 ActionContext 来访问 Servlet API。\n    Servlet API JSP 对象     HttpServletRequest request   HttpSession session   ServletContext application     操作 Session：\nActionContext.getContext().getSession().put(\u0026#34;user\u0026#34;, userName); 在 JSP 页面中可以通过\n${sessionScope.user}\r来输出userName。\n数据校验 ActionSupport 是一个工具类，已经实现了 Action 接口，实现了 Validatable 接口，提供数据校验功能。\n@Override public void validate() { if (getUserName() == null || getUserName().trim().equals(\u0026#34;\u0026#34;)) { addFieldError(\u0026#34;username\u0026#34;, getText(\u0026#34;user.required\u0026#34;)); } } struts.xml 配置文件 分为多个配置文件：\n\u0026lt;struts\u0026gt; \u0026lt;include file=\u0026#34;struts-part1.xml\u0026#34; /\u0026gt; \u0026lt;/struts\u0026gt;  Struts 2 不支持为单独的 Action 设置命名空间，而是通过为包指定 namespace 属性来为包下面的所有 Action 指定共同的命名空间。\n\u0026lt;package name=\u0026#34;book\u0026#34; extends=\u0026#34;struts-default\u0026#34; namespace=\u0026#34;/book\u0026#34;\u0026gt; 当指定了命名空间后，该包下所有的 Action 处理的 URL 应该是命名空间 + Action 名。\nhttp://localhost:8888/namespace/book/getBooks.action\r 默认的命名空间是 \u0026quot;\u0026rdquo;\n Action Action 类是一个普通的 POJO 类，来封装 HTTP 请求参数，并为请求参数对应的属性封装对应的 setter 和 getter 方法。\n在 JSP 中输出：\n\u0026lt;s:property value=\u0026#34;tip\u0026#34;\u0026gt;  不推荐在 \u0026lt;action name=\u0026quot;xxx\u0026quot;\u0026gt; 的 name 属性中包含 . 或 -，可能引发未知异常。\n 通过指定 \u0026lt;action method=\u0026quot;login\u0026quot; /\u0026gt; 就可以让 Action 类调用指定方法，而非 execute 方法来处理用户请求。\nname 属性支持通配符：\n\u0026lt;action name=\u0026#34;*Pro\u0026#34; class=\u0026#34;com.zk.LoginAction\u0026#34; method=\u0026#34;{1}\u0026#34;\u0026gt; 另外一个示例：\n\u0026lt;action name=\u0026#34;*_*\u0026#34; method=\u0026#34;{2}\u0026#34; class=\u0026#34;actions.{1}\u0026#34;\u0026gt; 类型转换 类 Struts 2 内建类型转换器可以完成基本数据类型转换、Date 与字符串之间的转换。\n\u0026lt;form method=\u0026#34;post\u0026#34; action=\u0026#34;regist\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;user.name\u0026#34; /\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;user.pass\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; 对应的 User 类：\nprivate User user; User 类必须提供无参构造器。\nMap private Map\u0026lt;String, User\u0026gt; users; 设置 map：\n读取 map：\nList 设置 List：\n读取 List：\n参考  Struts 2.x权威指南  "});index.add({'id':8,'href':'/posts/maintaining-cache-consistency/','title':"如何维持缓存的一致性？",'content':"Phil Karlton 曾经说过，“计算机科学中只有两件困难的事情：缓存失效和命名问题。” 这句话还有其他很好的举例。我个人最喜欢 Jeff Atwood 的一句话：“计算机科学中有两件困难的事情：缓存失效、命名和一个错误就关闭。”显然，缓存是困难的。就像分布式系统中的几乎所有东西一样，它甚至可能一眼就看不清。我将介绍分布式系统中几种常见的缓存方法，这些方法应该涵盖您将使用的绝大多数缓存系统。具体来说，我将关注如何维护缓存一致性。\n缓存 \u0026amp; 缓存一致性 在讨论不同的缓存方式之前，我们需要非常精确地说明缓存和缓存一致性的含义，特别是因为一致性是一个严重超载的术语。\n这里我们将缓存定义为：\n 一个单独的系统，它存储一个视图，这个视图是底层完整数据存储的一部分。\n 注意，这是一个非常笼统和轻松的定义。它包括通常被认为是缓存的内容，它存储与（通常是持久的）数据存储相同的值。它甚至包括一些人们通常不认为是缓存的东西。例如，数据库的非聚集二级索引。在我们的定义中，它也可以是一个缓存，保持缓存的一致性很重要。\n这里我们称缓存为一致的：\n 如果 k 存在于缓存中，则键 k 的值最终应与基础数据存储相同。\n 有了这个定义，如果缓存不存储任何内容，它总是一致的。但那根本没什么意思，因为它完全没用。\n为什么使用缓存 通常部署缓存是为了提高读写性能。这里的性能可以是延迟、吞吐量、资源利用率等，并且通常是相关的。保护数据库通常也是构建缓存的一个非常重要的动机。但你可以说这也是它正在解决的一个性能问题。\n不同类型的缓存 Look-aside / demand-fill 缓存 对于 look aside 缓存，客户端将在查询数据存储之前首先查询缓存。如果命中，它将返回缓存中的值。如果是未命中，它将从数据存储返回值。它没有说明缓存应该如何填充。它只是指定如何查询它。但通常情况下，是 demand-fill (按需填充)。Demand-fill 意味着在未命中的情况下，客户端不仅使用数据存储中的值，而且还将该值放入缓存中。通常，如果您看到一个look-aside 缓存，它也是一个 demand-fill 缓存。但这不一定。例如，你可以让缓存和数据存储订阅同一个日志（如Kafka）并独立实现。这是一个非常合理的设置。在本例中，缓存是一个 look-aside 缓存，而不是 demand-fill。而且缓存甚至可以拥有比持久数据存储更新鲜的数据。\n很简单，对吧？不过，简单的 Look aside/demand fill 缓存可能会有永久的不一致性！由于 look aside 缓存的简单性，这常常被人们忽略。根本上是因为当客户端将一些值放入缓存时，该值可能已经过时。具体来说\n- client gets a MISS (客户端未命中) - client reads DB get value `A` (客户端从数据库读取值：A) - someone updates the DB to value `B` and invalidates the cache entry (某人刷新了数据库，值变为了 B) - client puts value `A` into cache (客户端将 A 放入了缓存) 从那时起，客户端将继续从缓存中获取A，而不是B，后者是最新的值。取决于您的用例，这可能是正常的，也可能不是。它还取决于缓存条目是否有 TTL。但在使用 look aside/demand fill 缓存之前，您应该知道这一点。\n这个问题可以解决。Memcache使用 lease 来解决这个问题。因为从根本上讲，客户端在缓存上执行read-modify-write操作，而不使用原语来保证操作的安全性。在此设置中，read 从缓存中读取。modify 从数据库中读取。write 就是写回缓存。执行read-modify-write的一个简单解决方案是保留某种 “ticket” 来表示 read 时的缓存的状态，并比较 write 时的“ticket”。这就是 Memcache 解决问题的有效方法。Memcache 将其称为 lease，您可以将其作为简单的计数器，在每次缓存改变时都会碰到它。因此，在 read 时，它从 Memcache 主机获取 lease，在 write 时，客户端将 lease 一起传递。如果主机上的 lease 已更改，Memcache 将无法写入。现在回到前面的例子：\n- client gets a MISS with lease `L0` (客户端未命中，租约: L0) - client reads DB get value `A` (客户端从数据库读取值: A) - someone updates the DB to value `B` and invalidates the cache entry, which sets lease to `L1` (某人更新了数据库，最新值：B，租约：L1) - client puts value `A` into cache and fails due to lease mismatch (客户端放入 A 值到缓存失败，因为租约不匹配) 事情维持了一致：）\nWrite-through / read-through 缓存 Write-through 缓存方式意味着变异，客户端直接写入缓存。缓存负责同步写入到数据库中。它没有提到如何读取值的问题。客户端可以执行 look-aside 读或 read-through。\nRead-through 缓存意味着读取，客户端直接从缓存中读取。如果是未命中，cache 负责填充数据存储中的数据并回复客户端的查询。它没有提到写作。客户端可以 demand-fill 写入缓存或 write-through。\n现在你得到一张表格 (TAO: Facebook’s Distributed Data Store for the Social Graph)：\n同时有 write-through 和 look-aside 缓存并不常见。既然您已经构建了一个位于客户端和数据存储中间的服务，知道如何与数据存储对话，那么为什么不同时为读写操作这样做呢。也就是说，在有限的缓存大小下，根据查询模式的不同，write-through 和 look-aside 缓存可能是命中率的最佳选择。例如，如果大多数读操作在写操作之后立即执行，那么 write-through 和 look-aside 缓存可能提供最佳命中率。Read-through 和 demand-fill 的结合没有意义。\n现在让我们来看看 write-through 和 read-through 缓存的一致性。对于单个问题，只要正确获取 write 的 update lock 和 read 的 fill-lock，就可以序列化对同一个 key 的读写操作，并且不难看出缓存的一致性将得到维护。如果存在多个缓存副本，这将成为一个分布式系统问题，可能存在一些潜在的解决方案。保持缓存的多个副本一致的最直接的解决方案是拥有一个突变/事件日志，并基于该日志更新缓存。此日志用于单点序列化。它可以是 Kafka 甚至 MySQL binlog。只要突变是以易于重放这些事件的方式进行了全局的排序，就可以保持最终的缓存一致性。注意，这背后的推理与分布式系统中的同步相同。\nWrite-back / memory-only 缓存 还有一类缓存会遭受数据丢失的影响。例如，Write-back 缓存会在写入持久数据存储之前确认写入，如果在两者之间崩溃，则很明显会遭受数据丢失。这种类型的缓存有自己的使用场景，通常用于非常高的吞吐量和qps。但不一定太在意持久性和一致性。关闭持久性的 Redis 就属于这一类。\n译文来源  Different ways of caching and maintaining cache consistency  扫描下面二维码在手机端阅读：\n"});index.add({'id':9,'href':'/posts/help-the-world-by-healing-your-nginx-configuration/','title':"如何改进 NGINX 配置文件节省带宽？",'content':"2014年，Admiral William H. McRaven 在得克萨斯大学发表了著名的演讲，他说，如果你想改变世界，就从整理床铺开始。有时候小事情会有很大的影响——不管是在早上整理床铺，还是对网站的HTTP服务器配置做一些更改。\n这是不是有点言过其实了？2020年的头几个月，我们对世界上正常和合理的事物的所有定义都付之东流。由于COVID-19大流行，地球上几乎一半的人口被锁在家里，互联网已经成为他们唯一的交流、娱乐、购买食物、工作和教育方式。每周互联网的网络流量和服务器负载都比以往任何时候都要高。根据BroadbandNow在3月25日发表的一份报告，“在我们分析的200个城市中，有88个（44%）在过去一周内，相比之前的十周，经历了某种程度的网络退化”。\n为了保护网络链接，Netflix 和 YouTube 等主要媒体平台正在限制其传输质量，为人们工作、与家人交流或在学校上虚拟课程提供更多带宽。但这仍然不够，因为网络质量逐渐恶化，许多服务器变得过载。\n你可以通过优化你的网站来提供帮助 如果您拥有一个网站并可以管理其HTTP服务器配置，则可以提供帮助。一些小的更改可以减少用户生成的网络带宽和服务器上的负载。这是一个双赢的局面：如果你的网站目前负载很重，你可以减少它，使你能够为更多的用户服务，并可能降低你的成本。如果不是在高负载下，更快的加载可以改善用户的体验（有时会对你在谷歌搜索结果中的位置产生积极影响）。\n如果你有一个每月拥有数百万用户的应用程序，或者一个有烤菜谱的小博客，那就没什么关系了——每千字节的网络流量，你就消除了那些迫切需要在线检查医疗检测结果或创建包裹标签以向亲属发送重要信息的人的空闲容量。\n在这个博客中，我们提供了一些简单但强大的更改，您可以对您的 NGINX 配置。作为一个真实世界的例子，我们使用了 Rogalove 的朋友的电子商务网站，Rogalove 是一家位于波兰的生态化妆品制造商。该网站是一个相当标准的 woomerce 安装，运行 NGINX 1.15.9 作为其web服务器。为了便于我们的计算，我们假设网站每天有100个独立用户，30%的用户是经常访问的，每个用户在会话期间平均访问4个页面。\n这些技巧是您可以立即采取的简单步骤，以提高性能和减少网络带宽。如果要处理大量流量，可能需要实现更复杂的更改以产生重大影响，例如调整操作系统和 NGINX、提供正确的硬件容量，以及（最重要的）启用和调整缓存。查看以下博客文章了解详细信息：\n 调整 NGINX 的性能 性能调整-提示和技巧 10倍应用程序性能的10个技巧 在裸机服务器上部署 NGINX Plus 的大小调整指南 NGINX 和 NGINX Plus 缓存指南 NGINX 微缓存的优点  为HTML、CSS和JavaScript文件启用Gzip压缩 如您所知，用于在现代网站上构建页面的HTML、CSS和JavaScript文件可能非常庞大。在大多数情况下，web服务器可以动态压缩这些和其他文本文件，以节省网络带宽。\n查看web服务器是否正在压缩文件的一种方法是使用浏览器的开发工具。对于许多浏览器，可以使用F12键访问工具，相关信息位于“网络”选项卡上。下面是一个例子：\n正如您在左下角看到的，没有压缩：文本文件的大小为1.15 MB，并且传输了这么多数据。\n默认情况下，NGINX 中禁用压缩，但根据安装或Linux发行版的不同，可以在默认的 nginx.conf 文件中启用某些设置。在这里，我们在 NGINX 配置文件中启用 gzip 压缩：\ngzip on; gzip_types application/xml application/json text/css text/javascript application/javascript; gzip_vary on; gzip_comp_level 6; gzip_min_length 500; 正如您在下面的屏幕截图中看到的，通过压缩，数据传输仅下降到260kb，降幅约为80%！对于页面上的每个新用户，您可以节省大约917KB的数据传输。对于我们的 Woocomerce 安装，每天62MB，每月1860MB。\n设置缓存头 当浏览器检索网页的文件时，它会将副本保存在本地磁盘缓存中，这样当您再次访问该网页时，它就不必从服务器重新提取该文件。每个浏览器都使用自己的逻辑来决定何时使用文件的本地副本，以及在服务器上发生更改时何时再次获取该文件。但是作为网站所有者，您可以在发送的HTTP响应中设置缓存控制和过期头，以使浏览器的缓存行为更加高效。从长远来看，不必要的HTTP请求要少得多。\n一个好的开始，您可以为字体和图像设置一个很长的缓存过期时间，这些字体和图像可能不会经常更改（即使更改，它们通常也会得到一个新的文件名）。在下面的示例中，我们指示客户端浏览器将字体和图像保存在本地缓存中一个月：\nlocation ~* \\.(?:jpg|jpeg|gif|png|ico|woff2)$ { expires 1M; add_header Cache-Control \u0026#34;public\u0026#34;; } 启用 HTTP/2 协议支持 HTTP/2 是下一代网页服务协议，旨在提高网络和主机服务器的利用率。根据Google文档，它可以更快地加载页面：\n 由此产生的协议对网络更友好，因为与HTTP/1.x相比，使用的TCP连接更少。这意味着与其他流和存活时间更久的连接的竞争更少，进而导致可用网络容量的更好利用。\n NGINX 1.9.5 及更高版本（以及 NGINX Plus R7 及更高版本）支持 HTTP/2 协议，您只需启用它😀. 为此，请在 NGINX 配置文件的 listen 指令中包含 http2 参数：\nlisten 443 ssl http2; 注意，在大多数情况下，还需要启用 TLS 以使用HTTP/2。\n您可以使用 HTTP2.Pro 服务验证您的（或任何）站点是否支持HTTP/2：\n优化日志记录 给自己准备一杯你最喜欢的饮料，舒舒服服地坐着，想想：你最后一次查看访问日志文件是什么时候？上周，上个月，从来没有？即使您将它用于站点的日常监视，您可能只关注错误（400 和 500 状态代码，等等），而不是成功的请求。\n通过减少或消除不必要的日志记录，可以节省服务器上的磁盘存储、CPU和I/O操作。这不仅使您的服务器更快—如果您部署在云环境中，释放的I/O吞吐量和CPU周期可以更好地服务驻留在同一物理机上的另一个虚拟机或应用程序。\n有几种不同的方法可以减少和优化日志记录。在这里我们强调三点。\n方法1:禁用页面资源请求的日志记录\n如果不需要记录检索普通页面资源（如图像、JavaScript文件和CSS文件）的请求，这是一个快速而简单的解决方案。您只需创建一个与这些文件类型匹配的新位置块，并禁用其中的日志记录。（您也可以将此访问日志指令添加到上面设置缓存控制头的位置块中。）\nlocation ~* \\.(?:jpg|jpeg|gif|png|ico|woff2|js|css)$ { access_log off; } 方法2:禁用成功请求的日志记录\n这是一个更强大的方法，因为它放弃了带有2xx或3xx响应代码的查询，只记录错误。它比方法1稍微复杂一些，因为它取决于NGINX日志的配置方式。在我们的例子中，我们使用包含在Ubuntu服务器发行版中的标准nginx.conf文件，因此不管虚拟主机是什么，所有请求都会记录到/var/log/nginx/access.log.\n使用官方NGINX文档中的一个示例，我们打开条件日志记录。创建一个变量$loggable，对于带有2xx和3xx响应代码的请求，将其设置为0，否则设置为1。然后在access_log指令中将此变量作为条件引用。\n下面是位于 /etc/nginx/nginx.conf 文件中的 http 上下文的的原始指令：\naccess_log /var/log/nginx/access.log; 添加一个映射块并从 access_log 指令中引用它：\nmap $status $loggable { ~^[23] 0; default 1; } access_log /var/log/nginx/access.log combined if=$loggable; 注意，尽管 combined 是默认的日志格式，但是在包含 if 参数时需要显式地指定它。\n方法3：使用缓冲以减少I/O操作\n即使要记录所有请求，也可以通过打开访问日志缓冲来减少I/O操作。使用此指令，NGINX 将暂时不将日志数据写入磁盘，直到512KB缓冲区被填满或自上次刷新以来已过1分钟（以先发生者为准）。\naccess_log /var/log/nginx/access.log combined buffer=512k flush=1m; 限制特定URL的带宽 如果您的服务器提供了较大的文件（或较小但非常流行的文件，如表单或报表），则设置客户端下载这些文件的最大速度可能会很有用。如果您的站点已经处于高网络负载，限制下载速度会留下更多带宽，以保持应用程序的关键部分响应。这是硬件制造商使用的一个非常流行的解决方案-虽然有成千上万的其他人同时下载，您仍然可以获得您的下载，只是您可能需要等待更长的时间才能为打印机下载3GB驱动程序。😉\n使用 limit_rate 指令来限制特定URL的带宽。在这里，我们将每个文件在/download 下的传输速率限制为每秒50kb。\nlocation /download/ { limit_rate 50k; } 您可能还希望只对较大的文件进行速率限制，可以使用 limit-rate-after 指令执行此操作。在本例中，每个文件（从任何目录）的前500kb传输没有速度限制，之后的所有文件都限制在50kb/s。这样可以加快网站关键部分的传输速度，同时减慢其他部分的传输速度。\nlocation / { limit_rate_after 500k; limit_rate 50k; } 请注意，速率限制适用于浏览器和 NGINX 之间的各个HTTP连接，因此不要阻止用户使用下载管理器绕过速率限制。\n最后，还可以限制到服务器的并发连接数或请求速率。有关详细信息，请参阅我们的文档。\n总结 我们希望这五个技巧有助于优化您的网站的性能。速度和带宽增益因网站而异。即使优化 NGINX 配置似乎并没有显著地释放带宽或提高速度，数千个网站单独调整 NGINX 配置的总体影响也会增加。我们的全球网络使用效率更高，这意味着在需要时提供最关键的服务。\n如果您在您的网站上对 NGINX 有任何问题，我们将提供帮助！在COVID-19大流行期间，NGINX 员工和社区正在监视 Stackoverflow 网站上的 Nginx 板块，并尽快响应问题和请求。\n如果您在流行病前线的组织工作，并且有高级需求，那么您可以获得最多5个免费的NGINX Plus许可证以及更高级别的F5 DNS负载平衡器云服务。有关详细信息，请参阅受COVID-19影响的网站的免费资源。\n还可以查看上述链接，了解其他简单的方法，使用NGINX和F5的免费资源来提高网站性能。\n译文来源  Help the World by Healing Your NGINX Configuration "});index.add({'id':10,'href':'/docs/it-zone/2020-06/','title':"2020-06 文章收录",'content':"2020-06 文章收录 "});index.add({'id':11,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock/','title':"Best Time to Buy and Sell Stock",'content':"Best Time to Buy and Sell Stock 题目 LeetCode 地址：Best Time to Buy and Sell Stock\n有一个数组，第 i 个元素的值代表第 i 天的股票价格，如果你最多只能进行一次交易（某天买入一支股票，然后过几天卖掉），请问你能收获的最大利润是多少？\n分析 这道题有两个简单做法：状态机和动态规划。\n使用状态机的做法的好处是，这种思路可以延续到其它几个买卖股票的问题上。关键是要想清楚，某一天有几种状态，在这道题是三种：\n 状态 s0: 不买也不卖，无操作。s0 的值只能有一个来源，就是和昨天保持一致，不买也不卖 状态 s1: 买入了股票。s1 的值有两个来源：1. 与昨天一致，即已经买入了，且只能买一次，所以不能再买了，s1 = s1；2. 买入今天的股票，花了 price[i] 钱，s1 = s0 - price[i] 状态 s2: 卖出了股票。s2 的值有两个来源：1. 之前已经卖出了，所以维持卖出状态，不能再次卖了，s2 = s1；2. 卖出之前买入的股票，挣 price[i] 钱，s2 = s1 + price[i]  所以，我们可以得到如下状态转移关系：\n s0 = s0 s1 = s1 s1 = s0 - price[i] s2 = s2 s2 = s1 + price[i]  在这整个过程中，我们都要保证每一天的 s0、s1、s2 都是 max 状态，s2 是最终卖完后的收益，所以返回这个结果就行。\n 动态规划的想法：\n我们不要考虑每一天的价格，只需要考虑今天与昨天的价格之差：diff[i]。最终的收益是从这个 diff 数组中取出连续的一段，这一段累加起来的和最大。\n对于股票这道题而言，如果累加起来的最大值为负数，那么我们还不如全程不买卖股票，这样收益至少不是负值，也就是0。所以当 currMax 的值变为负数的时候，我们就废弃掉之前的结果，从此刻开始进行新的累加。\n答案 // MaximumSubarray public class BestTimetoBuyandSellStock { // 状态机 State Machine 做法  public int maxProfit(int[] prices) { if (prices == null || prices.length \u0026lt;= 1) { return 0; } //  // Buy Sell  // s0 ----\u0026gt; s1 ------\u0026gt; s2 (end)  // ↑__| ↑___|  //  int s0 = 0; // 初始状态  int s1 = -prices[0]; // 买入这支股票，花费 prices[i] 钱  int s2 = 0; // 一开始也是 0  for (int i = 1; i \u0026lt; prices.length; i++) { // ====================  // s0: 自始至终从未有过买卖  // ====================  s0 = s0; /** 维持自身 */ // ====================  // s1: 买入某支股票。我们只能买一次，然后一直维持。而每一次买都让我们花费了 prices[i] 元  // ====================  s1 = Math.max(s1 /** 维持自身 */, s0 - prices[i] /** 花费 prices[i] 买入这支股票 */); // ====================  // s2: 卖出某支股票。我们只能卖一次，然后一直维持。而每一次卖都让我们盈利 s1 + prices[i] 元  // ====================  s2 = Math.max(s2 /** 维持自身 */, s1 + prices[i] /** 卖掉这支股票，赚取 prices[i] */); } return s2; } public int maxProfit0(int[] prices) { if (prices == null || prices.length == 0) { return 0; } int max = 0; int currMax = 0; // 股票每天都在降低的话，那么最大利润为 0  // 也就是说这个地方每一次都可以不买  //  // 但是 MaximumSubarray 这道题，每次都必须选择一个，所以这个 MaximumSubarray 可以是负数  for (int i = 1; i \u0026lt; prices.length; i++) { currMax = Math.max(currMax + (prices[i] - prices[i - 1]) /** 当前累加的总收益 */, 0); max = Math.max(currMax, max); /** 在这个中间过程中，记录最大的值 */ } return max; } } 扫描下面二维码，在手机上阅读这篇文章：\n"});index.add({'id':12,'href':'/docs/tutorial/zipkin/brave/','title':"Brave 收集数据",'content':"Brave 收集数据 在 Java 生态世界中，Zipkin 团队官方提供了 Brave 用来收集数据到 Zipkin Server 中。其它的用来收集数据的框架还有 cassandra-zipkin-tracing、Dropwizard Zipkin、htrace、Spring Cloud Sleuth 以及 Wingtips 等。\n示例代码 配置 Tracer 配置 Tracer 以向 Zipkin Server 上传数据。\n// Configure a reporter, which controls how often spans are sent // (this dependency is io.zipkin.reporter2:zipkin-sender-okhttp3) sender = OkHttpSender.create(\u0026#34;http://127.0.0.1:9411/api/v2/spans\u0026#34;); // (this dependency is io.zipkin.reporter2:zipkin-reporter-brave) zipkinSpanHandler = AsyncZipkinSpanHandler.create(sender); // Create a tracing component with the service name you want to see in Zipkin. tracing = Tracing.newBuilder() .localServiceName(\u0026#34;my-service\u0026#34;) .addSpanHandler(zipkinSpanHandler) .build(); // Tracing exposes objects you might need, most importantly the tracer tracer = tracing.tracer(); // Failing to close resources can result in dropped spans! When tracing is no // longer needed, close the components you made in reverse order. This might be // a shutdown hook for some users. tracing.close(); zipkinSpanHandler.close(); sender.close(); 进程内跟踪 // Start a new trace or a span within an existing trace representing an operation ScopedSpan span = tracer.startScopedSpan(\u0026#34;encode\u0026#34;); try { // The span is in \u0026#34;scope\u0026#34; meaning downstream code such as loggers can see trace IDs  return encoder.encode(); } catch (RuntimeException | Error e) { span.error(e); // Unless you handle exceptions, you might not know the operation failed!  throw e; } finally { span.finish(); // always finish the span } 也可以通过如下更为高级、更为灵活的方式跟踪数据：\n// Start a new trace or a span within an existing trace representing an operation Span span = tracer.nextSpan().name(\u0026#34;encode\u0026#34;).start(); // Put the span in \u0026#34;scope\u0026#34; so that downstream code such as loggers can see trace IDs try (SpanInScope ws = tracer.withSpanInScope(span)) { return encoder.encode(); } catch (RuntimeException | Error e) { span.error(e); // Unless you handle exceptions, you might not know the operation failed!  throw e; } finally { span.finish(); // note the scope is independent of the span. Always finish a span. } 主要类讲解 Span Span 是存储跟踪数据的容器，其主要属性和行为如下：\n// 主要属性 public abstract TraceContext context(); @Override public abstract Span name(String name); @Override public abstract Span annotate(String value); @Override public abstract Span tag(String key, String value); // 主要行为 public abstract Span start(); public abstract void finish(); public abstract void abandon(); public abstract void flush(); 跟踪器 Tracer Tracer 可以创建各种各样的 Span。其主要字段：\npublic class Tracer { final Clock clock; final Propagation.Factory propagationFactory; final SpanHandler spanHandler; // only for toString  final PendingSpans pendingSpans; final Sampler sampler; final CurrentTraceContext currentTraceContext; final boolean traceId128Bit, supportsJoin, alwaysSampleLocal; final AtomicBoolean noop; } 下面以伪代码说明执行 tracer.startScopedSpan(\u0026quot;encode\u0026quot;) 做了哪些事情：\n// 获取 parent 上下文 TraceContext parent = currentTraceContext.get(); // 装饰 parent 上下文或创建新的 root 上下文 TraceContext context = parent != null ? decorateContext(parent, parent.spanId()) : newRootContext(0); // 创建 RealScopedSpan Scope scope = currentTraceContext.newScope(context); PendingSpan pendingSpan = pendingSpans.getOrCreate(parent, context, true); Clock clock = pendingSpan.clock(); MutableSpan state = pendingSpan.state(); state.name(name); return new RealScopedSpan(context, scope, state, clock, pendingSpans); 上述建立 Context 的过程，spanId 和 traceId 等 id 的生成方式如下：\n// 创建 64-bit spanId if (spanId == 0L) spanId = nextId(); // 创建 TraceId if (traceId == 0L) { // make a new trace ID  traceIdHigh = traceId128Bit ? Platform.get().nextTraceIdHigh() : 0L; traceId = spanId; } // localRootId if (localRootId == 0L) { localRootId = spanId; } /** Generates a new 64-bit ID, taking care to dodge zero which can be confused with absent */ long nextId() { long nextId = Platform.get().randomLong(); while (nextId == 0L) { nextId = Platform.get().randomLong(); } return nextId; } 上下文 CurrentTraceContext 包含了 Trace ID 、采集的数据等信息。主要字段如下：\npublic final class TraceContext extends SamplingFlags { final long traceIdHigh, traceId, localRootId, parentId, spanId; final List\u0026lt;Object\u0026gt; extraList; } 采样器 Sampler 采集上来的跟踪数据，要每一条都要发送到服务器吗？量会不会特别大？会不会有许多冗余重复的数据？采样器 Sampler 让你自主选择哪些数据需要发送，哪些不需要发送。\npublic abstract class Sampler { // 这条 traceId 对应的数据，是否需要统计  public abstract boolean isSampled(long traceId); } Brave 自带的几个采样器：\n跟踪监听器 SpanHandler public abstract class SpanHandler { public boolean begin(TraceContext context, MutableSpan span, @Nullable TraceContext parent) { return true; } public boolean end(TraceContext context, MutableSpan span, Cause cause) { return true; } } 计时器 Clock public interface Clock { long currentTimeMicroseconds(); } 其只提供了一个实现 TickClock：\nfinal class TickClock implements Clock { final long baseEpochMicros; final long baseTickNanos; TickClock(long baseEpochMicros, long baseTickNanos) { this.baseEpochMicros = baseEpochMicros; this.baseTickNanos = baseTickNanos; } @Override public long currentTimeMicroseconds() { return ((System.nanoTime() - baseTickNanos) / 1000) + baseEpochMicros; } } Propagation 该类用来将 TraceContext 中携带的信息转为文本信息，以放到 Request 中，用以跨进程跟踪。\n"});index.add({'id':13,'href':'/docs/cloud-plus-bbs/bilibili-high-availability/','title':"B站高可用架构实践",'content':"B站高可用架构实践 流量洪峰下要做好高服务质量的架构是一件具备挑战的事情，从Google SRE的系统方法论以及实际业务的应对过程中出发，分享一些体系化的可用性设计。对我们了解系统的全貌上下游的联防有更进一步的了解。\n负载均衡 BFE 就是指边缘节点，BFE 选择下游 IDC 的逻辑权衡：\n 离 BFE 节点比较近的 基于带宽的调度策略 某个 IDC 的流量已经过载，选择另外一个 IDC  当流量走到某个 IDC 时，这个流量应该如何进行负载均衡？\n问题：RPC 定时发送的 ping-pong，也即 healthcheck，占用资源也非常多。服务 A 需要与账号服务维持长连接发送 ping-pong，服务 B 也需要维持长连接发送 ping-pong。这个服务越底层，一般依赖和引用这个服务的资源就越多，一旦有任何抖动，那么产生的这个故障面是很大的。那么应该如何解决？\n解决：以前是一个 client 跟所有的 backend 建立连接，做负载均衡。现在引入一个新的算法，子集选择算法，一个 client 跟一小部分的 backend 建立连接。图片中示例的算法，是从《Site Reliability Engineering》这本书里看的。\n如何规避单集群抖动带来的问题？多集群。\n如上述图片所示，如果采用的是 JSQ 负载均衡算法，那么对于 LBA 它一定是选择 Server Y 这个节点。但如果站在全局的视角来看，就肯定不会选择 Server Y 了，因此这个算法缺乏一个全局的视角。\n如果微服务采用的是 Java 语言开发，当它处于 GC 或者 FullGC 的时候，这个时候发一个请求过去，那么它的 latency 肯定会变得非常高，可能会产生过载。\n新启动的节点，JVM 会做 JIT，每次新启动都会抖动一波，那么就需要考虑如何对这个节点做预热？\n如上图所示，采用 \u0026ldquo;the choice-of-2\u0026rdquo; 算法后，各个机器的 CPU 负载趋向于收敛，即各个机器的 CPU 负载都差不多。Client 如何拿到后台的 Backend 的各项负载？是采用 Middleware 从 Rpc 的 Response 里面获取的，有很多 RPC 也支持获取元数据信息等。\n还有就是 JVM 在启动的时候做 JIT，以前的预热做法：手动触发预热代码，然后再引入流量，再进行服务发现注册等，不是非常通用。通过改进负载均衡算法，引入惩罚值的方式，慢慢放入流量进行预热。\n限流 用 QPS 限制的陷阱：\n 不同的参数，请求的数据量是不同的，对一个进程的一个吞吐是有影响的。 业务是经常迭代的，配一个静态的阈值，这个非常困难。能否按照每一个服务用多少个 CPU 来做限流？  每一个 API 都是有重要性的：非常重要、次重要，这样配置限流、做过载保护的时候，可以使用不同的阈值。\n每个服务都要配一个限流，是非常烦人的，需要压测，是不是可以自适应去限流？\n每个 Client 如何知道自己这一次需要申请多少 Quota ？基于历史数据窗口的 QPS。\n节点与节点之间是有差异的，分配算法不够好，会导致某些节点产生饥饿。那么可以采用最大最小公平算法，尽可能地比较公平地去分配资源，来解决这个问题。\n当量再大一点的时候，如果 Backend 一直忙着拒绝请求，比如发送 503，那么它还是会挂掉。这种情况就要考虑从 Client 去截流。此处，又提到了 Google 《Site Reliability Engineering》这本书里面的一个算法，即 Client 是按照一定概率去截流。那么这个概率怎么计算？一个是总请求量：requests，一个是成功的请求量：accepts。如果服务报错率比较高，意味着 accepts 不怎么增长，requests 一直增长，最终这个公式求极限，它会等于 1，所以它的丢弃概率是非常高的。基于这么一个简单的公式，不需要依赖什么 ZooKeeper，什么协调器之类的，就可以得到一个概率丢弃一些请求。它尽可能的在服务不挂掉的情况下，放更多的流量进去，而不是像 Netflix 一样全部拒掉。\n连锁故障通常都是某一个节点过载了挂掉，流量又会去剩下的 n - 1 个节点，又扛不住，又挂掉，所以最终一个一个挨着雪崩。所以过载保护的目的是为了自保。\nB 站参考了阿里的 Sentinel 框架、Netflix 的一些文章等，最终采用的是类似于 TCP BBR 探测的思路和算法。简单说：当 CPU 达到 80% 的时候，这个时候我们认为流量过载，如果此时吞吐量比如 100，用它作为阈值，瞬时值的请求比如是 110，那就可以丢掉 10 个流量。这样就可以实现一个限流算法。\nCPU 抖来抖去，使用 CPU 滑动均值（绿色线）可以跳动的没有这么厉害。这个 CPU 针对不同接口的优先级，例如低优先级 80% 触发，高优先级 90% 触发，可以定为一个阈值。\n那么吞吐如何计算？利特尔法则。当前的 QPS * 延迟 = 吞吐，可以用过去的一个窗口作为指标。一旦丢弃流量，CPU 立马下来，算法抖动非常厉害。图二右侧黄色线表示抖动非常高，绿色线表示放行的流量也是抖动非常高，所以又加了冷却时间，比如持续几秒钟，再重新判断。\n重试  BFE: 动态 CDN SLB: LVS + Nginx 实现，四七层负载均衡 BFF: 业务逻辑组装、编排  问题：每一层都重试，这一层 3 次，那一层 3 次，会指数级的放大。解决：只在失败这一层重试，如果重试之后失败，请返回一个全局约定好的错误码，比如说：过载，无需重试，发现这个错误码，通通放行，避免级联重试。\n重试都应该无脑的重试三次吗？API 级别的重试需要考虑集群的过载情况。是不是可以约定一个重试比例呢？比如只允许 10% 的流量进行重试，Client 端做统计，当发现有 10% 都是重试，那么剩下的都拒绝掉。这样最多产生 1.1 倍的放大，重试 3 次，极端情况下，会产生 3 倍放大。还有在重试的时候，尽量引入随机、指数递增的一个重试周期，大家不要都重试 1 秒钟，有可能会堆砌一个重试的波峰。\n重试的统计图和记录 QPS 的图分开。问题诊断的时候，可以知道它是来自流量重试导致的问题放大。\n某个服务不可用的时候，用户总是会猛点，那么这个时候，需要去限制它的频次，一个短周期内不允许发重复请求。这种策略，有可能会根据不同的过载情况经常调这种策略，那么可以挂载到每一个 API 里面。\n超时 大部分的故障都是因为超时控制不合理导致的。\n 某个高延迟服务可能会导致 Client 堆积，Client 线程会阻塞，上游流量不断进来，下游的消费速度跟不上上游的流入速度，进程会堆积越来越多请求，可能会 OOM。 超时的策略本质是就是为了丢弃或者消耗掉请求。 下游 2 秒返回，上游配置了 1 秒，上游超时已经返回给用户，下游还在执行，浪费资源。  某个服务需要在 1 秒返回，内部可能需要访问 Redis，需要访问 RPC，需要访问数据库，时间加起来就超过 1 秒，那么访问完每一层，应该计算供下一层使用的超时时间还剩多少可用。在 go 语言里，可能会使用 Context，每一个网络请求开始的阶段，都要根据配置文件配置的超时时间，和当前剩余多少，取一个最小值，最终整个超时时间不会超过 1 秒。\n通过 RPC 的元数据传递，类似 HTTP 的 request header，带给其它服务。例如在图中，就是把 700ms 这个配额传递给 Service B。\n下游服务作为服务提供者，在他的 RPC.IDL 文件中把自己的超时要配上，那么用 IDL 文件的时候，就知道是 200 ms，不用去问。\n应对连锁故障 优雅降级：一开始千人千面，后来只返回热门的\n参考 QA  Q: 请问负载均衡依据的 metric 是什么？ A: 服务端主要用 CPU，客户端用的是健康度，指连接的成功率，延迟也很重要，每个 Client 往不同的 Backend 发了多少个请求，四个指标归一，写一个线性方程，进行打分。    Q: BFE 到 SLB 走公网还是专线？ A: 既有公网，又有专线。    Q: Client 几千量级，每 10 秒 ping-pong 一下，会不会造成蛮高的 CPU？ A: 如果 Backend 很多的话，那么这个的确会造成。    Q: 多集群切换是否有阻塞的点？ A: 一个 Client 连接到各个集群，subset 算法，每个集群都有 Cache    Q: 负载均衡的探针是怎么做的？ A: 惩罚值，比如 5 秒，慢慢放流量    Q: Quota-Server 限流有开源实现吗？ A: 目前看到的都是针对单节点的。    Q: 客户端统计是否有点太多？ A: 可以做到 Sidecar、Service Mesh 里面    Q: 超时传递是不是太严格？ A: 有些情况下即便超时也要运行，可以通过 RPC Context 管控    Q: 每个 RPC 都获取 CPU 会不会很昂贵？ A: 后台开启线程定时计算 CPU 平滑均值    Q: 线上压测和测试环境压测 CPU 不一致 A: RPC 路由加影子库    Q: CC 攻击 A: 边缘节点或者核心机房都有防止 CC 攻击的一些手段，只要不是分布式搞你，都能找到流量特征进行管控  "});index.add({'id':14,'href':'/docs/tutorial/distributed-storage/c_cpp/','title':"C \u0026 C++",'content':"C \u0026amp; C++  补充 C \u0026amp; C++ 知识点\n xx.a 文件 这是静态链接库文件。\nundefined reference to pthread_create g++ -pthread ... 链接静态文件 文件组织形式：\n|- include | - leveldb | - db.h | - cache.h | - xxx.h |- build | - libleveldb.a |test.c g++ test.c -I include/ -L build/ -l leveldb -pthread -o test.out ./test.out  include/ 文件夹中包含了头文件 build/ 文件夹包含了 libleveldb.a  参考\n"});index.add({'id':15,'href':'/docs/tutorial/vue3/createapp/','title':"createApp",'content':"createApp 一个简单的计数器例子 创建渲染器 挂载 App 上下文 // apiCreateApp.ts export function createAppContext(): AppContext { return { app: null as any, config: { isNativeTag: NO, performance: false, globalProperties: {}, optionMergeStrategies: {}, isCustomElement: NO, errorHandler: undefined, warnHandler: undefined }, mixins: [], components: {}, directives: {}, provides: Object.create(null) } } "});index.add({'id':16,'href':'/docs/tutorial/devops/intro/','title':"DevOps 简介",'content':"DevOps 简介 DevOps (Development 与 Operations) 是一种文化，这种文化旨在建立一个使得软件构建、测试、发布等得以快速、稳定，实施交付的环境。\n那么如何做到快速？如何做到稳定？答案是自动化工具。开发到测试到上线之间的所有需要手动处理的环节，都是可以尝试优化的点。\nDevOps 能力成熟度模型 全球首个 DevOps 标准，即《研发运营一体化（DevOps）能力成熟度模型》，由中国信息通信研究院牵头，云计算开源产业联盟、高效运维社区、 DevOps 时代社区联合 Google、BATJ、清华大学、南京大学、通信及金融等行业顶尖企事业单位专家共同制定。\n目前很多公司都在参考这套模型进行实践。\nDevOps 工具集锦 信通院整理的的 DevOps 工具集锦（看不清的话，图片上右击，在新标签页中打开图像）：\n持续集成  每次提交代码，就会触发完整的流水线。这需要打通版本控制系统和持续集成系统，例如 GitLab 和 Jenkins 集成。 每次流水线，触发自动化测试。 出了问题，第一时间修复。  推荐书籍  《持续交付 2.0》 《DevOps 实践指南》  "});index.add({'id':17,'href':'/docs/tutorial/network/dhcp/','title':"DHCP",'content':"DHCP DHCP 是 Dynamic Host Configuration Protocol (动态主机配置协议) 的缩写。\n作用 手机、电脑或其它网络设备想要与其它计算机进行通讯，就需要配置 IP 地址，DHCP 协议就是为网络设备动态分配 IP 地址的一种协议。DHCP 底层基于 UDP 传输层协议，端口 67 是 DHCP Server 端使用的端口，端口 68 是 DHCP Client 端使用的接口。\n工作方式 DHCP 协议分配 IP 地址可以分为 4 个步骤：\nDiscovery 网络中新加入的某个设备（DHCP 客户端），会使用 IP 地址 0.0.0.0 向该网络发送一个广播包，这个包的目的 IP 地址是 255.255.255.255。这个 UDP 包封装的内容如下所示：\n   头 内容     MAC 头 源 MAC：设备自身的 MAC 地址，目的 MAC 地址：FF:FF:FF:FF:FF:FF   IP 头 源 IP: 0.0.0.0，目的 IP: 255.255.255.255   UDP 头 源端口：68，目的端口：67   BOOTP 头 DHCP Discover    Offer DHCP Server 接受到这个包以后，\n"});index.add({'id':18,'href':'/docs/it-zone/2020-06/fastjson-high-risk-vulnerability/','title':"fastjson 又现高危漏洞！",'content':"fastjson 又现高危漏洞！ 日期：2020-06-01\n 5 月 28 日，据 360 网络安全响应中心发布《Fastjson远程代码执行漏洞通告》显示，由阿里巴巴开源的 fastjson 库 又现高危漏洞，该漏洞可导致不法分子远程执行服务器命令等严重后果。\nfastjson 是阿里巴巴的开源JSON解析库，它可以解析JSON格式的字符串，支持将Java Bean序列化为JSON字符串，也可以从JSON字符串反序列化到 JavaBean。\nfastjson 存在远程代码执行漏洞，autotype 开关的限制可以被绕过，链式的反序列化攻击者精心构造反序列化利用链，最终达成远程命令执行的后果。此漏洞本身无法绕过 fastjson 的黑名单限制，需要配合不在黑名单中的反序列化利用链才能完成完整的漏洞利用。\n该漏洞影响的版本：\u0026lt;= 1.2.68\n该漏洞修复建议：\n 升级到 fastjson 1.2.69/1.2.70 版本，下载地址为 Releases · alibaba/fastjson 或者通过配置以下参数开启 SafeMode 来防护攻击：ParserConfig.getGlobalInstance().setSafeMode(true);（safeMode 会完全禁用 autotype，无视白名单，请注意评估对业务影响）  "});index.add({'id':19,'href':'/docs/tutorial/git/config-user-and-email/','title':"Git 配置用户名和邮箱",'content':"Git 配置用户名和邮箱 假设你的用户名是 zk，邮箱账号是 xxx@163.com，那么需要提前配置 Git 的用户名和邮箱帐号：\ngit config --global user.name \u0026#34;zk\u0026#34; git config --global user.email \u0026#34;xxx@163.com\u0026#34; Git 需要知道谁对代码做出了变更，对代码做出变更的这个人的邮件联系方式是什么，以方便追踪。\nGit Config 有三个作用域：\n git config --local：只对某个仓库有效 git config --global：对当前用户所有仓库有效 git config --system：对系统所有登录的用户有效  如何查看当前设置的 Git 配置？\ngit config --list --local git config --list --global git config --list --system  --local 针对的是某个仓库，配置 --local 作用域的时候，需要进入到项目所在的目录才能配置或显示。\n "});index.add({'id':20,'href':'/docs/tutorial/unix-command/grep/','title':"grep",'content':"grep grep 命令如何使用？grep 命令的常见用法？\n简介 grep 命令用于搜索文本。它在给定文件中搜索包含与给定字符串或单词匹配的行。它是 Linux 和类 Unix 系统中最有用的命令之一。让我们看看如何在 Linux 或类 Unix 系统上使用 grep。\ngrep 命令是一个包含 grep、egrep 和 fgrep 命令的大家族，都用于搜索文本。\n常见用法 下面是一些标准的 grep 命令，通过示例说明了如何在Linux、macOS和Unix上使用 grep：\n（1）在文件 foo.txt 中搜索单词 word\ngrep \u0026#39;word\u0026#39; foo.txt （2）在文件 foo.txt 中搜索单词 word，并且忽略大小写\ngrep -i \u0026#39;word\u0026#39; foo.txt 上述命令会把位于 foo.txt 文件中的 WORD、Word、word 等忽略大小写的 word 全部搜索出来。\n（3）在当前目录以及所有子目录中查找单词 word\ngrep -R \u0026#39;word\u0026#39; .  注意：最后面有一个点，代表当前目录。-r 命令也是递归搜索，只是 -r 不会搜索符号链接文件。\n （4）搜索并显示单词 word 出现的次数\ngrep -c \u0026#39;word\u0026#39; foo.txt （5）只匹配单词 word\ngrep -w \u0026#39;word\u0026#39; foo.txt 这意味着 fooword、word123 等单词不会被搜索出来，而只会将 a word 这种类型的 word 搜索出来。\n（6）搜索单词 word1 或 word2\negrep -w \u0026#39;word1|word2\u0026#39; foo.txt （7）结果显示行号\ngrep -n \u0026#39;root\u0026#39; /etc/passwd （8）反匹配搜索\ngrep -v word foo.txt foo.txt 文件中，不包含 word 的行，会被搜索出来。\n（9）显示匹配行的上下文\n当展示结果行的时候，顺便将 word 所在行的前面 3 行，也显示出来：\ngrep -B 3 \u0026#39;word\u0026#39; foo.txt 将 word 所在航的后 4 行，显示出来：\ngrep -A 4 \u0026#39;word\u0026#39; foo.txt 使用 -C 命令同时展示出前 3 行和后 4 行：\ngrep -C 3 \u0026#39;word\u0026#39; foo.txt （10）与其它 Shell 命令结合\n显示 CPU 型号：\ncat /proc/cpuinfo | grep -i \u0026#39;Model\u0026#39; （11）仅显示匹配的文件名\ngrep -l \u0026#39;main\u0026#39; *.c （12）搜索结果高亮显示\ngrep --color vivek /etc/passwd （13）搜索多个文件\ngrep word *.txt （14）排除/引入某些文件\n只在 rootdir 文件夹内的 *.cpp 和 *.h 文件中搜索 abc 这个关键字：\ngrep \u0026#39;abc\u0026#39; -r --include=\u0026#34;*.{cpp,h}\u0026#34; rootdir # 或 grep \u0026#39;abc\u0026#39; -r --include=*.cpp --include=*.h rootdir 在当前文件夹，只搜索 *.js 文件，但是排除 *js/lib/* 路径和 *.min.js 这些文件：\ngrep \u0026#34;z-index\u0026#34; . --include=*.js --exclude=*js/lib/* --exclude=*.min.js 排除多个模式，比如 --exclude=pattern1 --exlucde=pattern2 ，可以使用 {} 把多个 pattern 包括起来：\n--exclude={pattern1,pattern2,pattern3} 正则表达式 grep 支持三种类型的正则表达式语法：\n basic (BRE) extended (ERE) perl (PCRE)  （1）匹配行的开头\ngrep ^vivek /etc/passwd vivek 仅作为行的开头的时候，才会被搜索出来。\n（2）匹配行结尾\ngrep \u0026#39;foo$\u0026#39; filename foo 仅作为行的结尾的时候，才会被搜索出来。\n（3）点需要被转义\n在 grep 中，. 有特殊含义，它可以匹配任何字符，所以如果需要匹配 .，那么需要使用反斜杠 \\ 对其进行转义：\ngrep \u0026#39;192\\.168\\.1\\.254\u0026#39; hosts （4）搜索多个字符串\ngrep -E \u0026#39;word1|word2\u0026#39; filename # 或 egrep \u0026#39;word1|word2\u0026#39; filename （5）搜索带有横杠的字符串\ngrep -e \u0026#39;--test--\u0026#39; filename  如果不加 -e 选项，那么 --test 将会按照 grep 命令的参数 –test– 来处理\n 参考  How To Use grep Command In Linux / UNIX Regular expressions in grep ( regex ) with examples Use grep \u0026ndash;exclude/\u0026ndash;include syntax to not grep through certain files  扫描下面二维码，在手机端阅读：\n"});index.add({'id':21,'href':'/docs/programmer-interview/java/','title':"Java",'content':"初级/中级/高级 Java 面试题  收录全网最全 Java 相关的 (分布式、MqSQL、中间件) 面试题\n Java 基础    并发          多线程实现方式 synchronized ThreadLocal 锁 线程池 volatile   ConcurrentHashMap 1.7 ConcurrentHashMap 1.8       CyclicBarrier            集合类         集合类关系图 ArrayList LinkedList HashMap TreeMap       基础         char finally Java 异常 Java GC 类加载    中间件    Redis            Kafka      Kafka 高吞吐量怎么实现的 为什么阿里要自研 RocketMQ       Sentinel     Sentinel 与 Hystrix 的对比    架构    系统设计     秒杀系统    发散思维         点是否在三角形内    "});index.add({'id':22,'href':'/docs/programmer-interview/data-structure/map/','title':"Map",'content':"Map "});index.add({'id':23,'href':'/docs/tutorial/eureka/intro/','title':"Netflix Eureka 简介",'content':"Netflix Eureka 简介 Eureka 干什么 Eureka 是一套服务治理框架，其包含 Server 端和 Client 端。\n架构 为什么需要服务治理 A 服务需要调用 B 服务，B 服务的 URL （可能不止一个）可以通过静态配置来维护，但是随着微服务越来越复杂，静态配置就会越来越难以维护，且维护需要耗费好多人力。\n服务治理包括什么    组件 描述     注册中心 每个服务需要向注册中心注册自己提供的服务   服务发现 调用方向注册中心咨询自己调用的服务的地址是什么    "});index.add({'id':24,'href':'/docs/rocketmq/rocketmq-send-message-flow/','title':"RocketMQ 消息发送流程",'content':"RocketMQ 消息发送流程 本文讲述 RocketMQ 发送一条普通消息的流程。\n一、服务器启动 我们可以参考官方文档来启动服务:\n 启动 Name 服务器:  sh bin/mqnamesrv  启动 Broker 服务器:  sh bin/mqbroker -n localhost:9876 二、构建消息体 一条消息体最少需要指定两个值:\n 所属话题 消息内容  如下就是创建了一条话题为 “Test”，消息体为 “Hello World” 的消息:\nMessage msg = new Message( \u0026#34;Test\u0026#34;, \u0026#34;Hello World\u0026#34;.getBytes() ); 三、启动 Producer 准备发送消息 如果我们想要发送消息呢，我们还需要再启动一个 DefaultProducer (生产者) 类来发消息:\nDefaultMQProducer producer = new DefaultMQProducer(); producer.start(); 现在我们所启动的服务如下所示:\n四、Name 服务器的均等性 注意我们上述开启的是单个服务，也即一个 Broker 和一个 Name 服务器，但是实际上使用消息队列的时候，我们可能需要搭建的是一个集群，如下所示:\n在 RocketMQ 的设计中，客户端需要首先询问 Name 服务器才能确定一个合适的 Broker 以进行消息的发送:\n然而这么多 Name 服务器，客户端是如何选择一个合适的 Name 服务器呢?\n首先，我们要意识到很重要的一点，Name 服务器全部都是处于相同状态的，保存的都是相同的信息。在 Broker 启动的时候，其会将自己在本地存储的话题配置文件 (默认位于 $HOME/store/config/topics.json 目录) 中的所有话题加载到内存中去，然后会将这些所有的话题全部同步到所有的 Name 服务器中。与此同时，Broker 也会启动一个定时任务，默认每隔 30 秒来执行一次话题全同步:\n五、选择 Name 服务器 由于 Name 服务器每台机器存储的数据都是一致的。因此我们客户端任意选择一台服务器进行沟通即可。\n其中客户端一开始选择 Name 服务器的源码如下所示:\npublic class NettyRemotingClient extends NettyRemotingAbstract implements RemotingClient { private final AtomicInteger namesrvIndex = new AtomicInteger(initValueIndex()); private static int initValueIndex() { Random r = new Random(); return Math.abs(r.nextInt() % 999) % 999; } private Channel getAndCreateNameserverChannel() throws InterruptedException { // ...  for (int i = 0; i \u0026lt; addrList.size(); i++) { int index = this.namesrvIndex.incrementAndGet(); index = Math.abs(index); index = index % addrList.size(); String newAddr = addrList.get(index); this.namesrvAddrChoosed.set(newAddr); Channel channelNew = this.createChannel(newAddr); if (channelNew != null) return channelNew; } // ...  } } 以后，如果 namesrvAddrChoosed 选择的服务器如果一直处于连接状态，那么客户端就会一直与这台服务器进行沟通。否则的话，如上源代码所示，就会自动轮寻下一台可用服务器。\n六、寻找话题路由信息 当客户端发送消息的时候，其首先会尝试寻找话题路由信息。即这条消息应该被发送到哪个地方去。\n客户端在内存中维护了一份和话题相关的路由信息表 topicPublishInfoTable，当发送消息的时候，会首先尝试从此表中获取信息。如果此表不存在这条话题的话，那么便会从 Name 服务器获取路由消息。\npublic class DefaultMQProducerImpl implements MQProducerInner { private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) { TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); if (null == topicPublishInfo || !topicPublishInfo.ok()) { this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic); topicPublishInfo = this.topicPublishInfoTable.get(topic); } // ...  } } 当尝试从 Name 服务器获取路由信息的时候，其可能会返回两种情况:\n(1) 新建话题 这个话题是新创建的，Name 服务器不存在和此话题相关的信息：\n(2) 已存话题 话题之前创建过，Name 服务器存在此话题信息：\n服务器返回的话题路由信息包括以下内容:\n“broker-1”、”broker-2” 分别为两个 Broker 服务器的名称，相同名称下可以有主从 Broker，因此每个 Broker 又都有 brokerId 。默认情况下，BrokerId 如果为 MixAll.MASTER_ID （值为 0） 的话，那么认为这个 Broker 为 MASTER 主机，其余的位于相同名称下的 Broker 为这台 MASTER 主机的 SLAVE 主机。\npublic class MQClientInstance { public String findBrokerAddressInPublish(final String brokerName) { HashMap\u0026lt;Long/* brokerId */, String/* address */\u0026gt; map = this.brokerAddrTable.get(brokerName); if (map != null \u0026amp;\u0026amp; !map.isEmpty()) { return map.get(MixAll.MASTER_ID); } return null; } } 每个 Broker 上面可以绑定多个可写消息队列和多个可读消息队列，客户端根据返回的所有 Broker 地址列表和每个 Broker 的可写消息队列列表会在内存中构建一份所有的消息队列列表。之后客户端每次发送消息，都会在消息队列列表上轮循选择队列 (我们假设返回了两个 Broker，每个 Broker 均有 4 个可写消息队列):\npublic class TopicPublishInfo { public MessageQueue selectOneMessageQueue() { int index = this.sendWhichQueue.getAndIncrement(); int pos = Math.abs(index) % this.messageQueueList.size(); if (pos \u0026lt; 0) pos = 0; return this.messageQueueList.get(pos); } } 七、给 Broker 发送消息 在确定了 Master Broker 地址和这个 Broker 的消息队列以后，客户端才开始真正地发送消息给这个 Broker，也是从这里客户端才开始与 Broker 进行交互:\n这里我们暂且先忽略消息体格式的具体编/解码过程，因为我们并不想一开始就卷入这些繁枝细节中，现在先从大体上了解一下整个消息的发送流程，后续会写专门的文章来说明。\n八、Broker 检查话题信息 刚才说到，如果话题信息在 Name 服务器不存在的话，那么会使用默认话题信息进行消息的发送。然而一旦这条消息到来之后，Broker 端还并没有这个话题。所以 Broker 需要检查话题的存在性:\npublic abstract class AbstractSendMessageProcessor implements NettyRequestProcessor { protected RemotingCommand msgCheck(final ChannelHandlerContext ctx, final SendMessageRequestHeader requestHeader, final RemotingCommand response) { // ...  TopicConfig topicConfig = this.brokerController .getTopicConfigManager() .selectTopicConfig(requestHeader.getTopic()); if (null == topicConfig) { // ...  topicConfig = this.brokerController .getTopicConfigManager() .createTopicInSendMessageMethod( ... ); } } } 如果话题不存在的话，那么便会创建一个话题信息存储到本地，并将所有话题再进行一次同步给所有的 Name 服务器:\npublic class TopicConfigManager extends ConfigManager { public TopicConfig createTopicInSendMessageMethod(final String topic, /** params **/) { // ...  topicConfig = new TopicConfig(topic); this.topicConfigTable.put(topic, topicConfig); this.persist(); // ...  this.brokerController.registerBrokerAll(false, true); return topicConfig; } } 话题检查的整体流程如下所示:\n九、消息存储 当 Broker 对消息的一些字段做过一番必要的检查之后，便会存储到磁盘中去:\n十、整体流程 发送消息的整体流程:\n扫描下面二维码，在手机端阅读：\n"});index.add({'id':25,'href':'/docs/programmer-interview/front-end/','title':"前端",'content':"初级/中级/高级前端面试进阶题  收录各大公司最全前端面试题\n HTML    HTML       HTML 语义化 HTML5 新特性 meta    CSS    布局          移动端响应式布局 垂直居中 CSS 盒模型 position display CSS 选择器   BFC 和 IFC 左固定右自适应 两侧固定中间自适应 CSS 九宫格 flex 布局 清除浮动   CSS3 新特性 CSS 画三角形 三栏等宽 CSS 画半圆 画正方形 CSS 画扇形   动画 画圆 CSS 单位 (px/em/rem/\u0026hellip;)       JavaScript    Array        检测数组 数组去重 数组乱序 数组扁平化       Basic          继承 typeof this 柯里化 - Currying new 数据类型   instanceof let、const、var setTimeout Strict Mode 深浅拷贝 ES6 新特性   ES2020 创建对象 Object.freeze() Cookie 和 Session call、apply、bind prototype   实现 retry 实现 sleep 实现 Promise.all 实现观察者/PubSub/EventBus模式      浏览器    浏览器          事件循环 缓存 HTTP CDN 存储 DOM 操作 API   Event 重绘和重排 HTTP2 HTTP3 HTTPS 冒泡捕获   输入 URL 后发生了什么 浏览器如何渲染页面 DNS Web 安全      综合性解决方案    Solution      跨域 CORS 节流和防抖    框架    VUE      VUE 生命周期 nextTick    "});index.add({'id':26,'href':'/docs/tutorial/front-end-optimization-guide/','title':"前端优化指南",'content':"前端优化指南  图片优化 HTML 优化 CSS 优化 JS 优化  "});index.add({'id':27,'href':'/docs/tutorial/front-end-optimization-guide/image-optimization/','title':"图片优化",'content':"图片优化 图片在网页数据的传输中占据了非常大的流量，如何优化图片，对于前端页面加载的性能极其重要。本文讲述了比较常见的几种优化图片的技巧。\n图片格式介绍 （1）JPEG\nJPEG 是 Joint Photographic Experts Group 的缩写，不支持透明度，常用于网站的 Banner 图。JPEG 使用的是一种有损图像质量的压缩算法，压缩的越狠，图片的质量损失也就越大，图片的尺寸也就越小。根据你网站所能忍受的图片质量，来相应的选择压缩比：\n（2）PNG\n支持透明度，支持无损压缩，一般图片的尺寸都比较大。\n（3）GIF\n适合放动画图片。\n（4）WebP\n🔥Google 2010 年提出的新的图像压缩格式算法，在 2013 年又推出 Animated WebP，即支持动画的 Webp。优点：更优的图像数据压缩算法、拥有肉眼识别无差异的图像质量、具备了无损和有损的压缩模式、Alpha 透明以及动画的特性。\nPNG、JPG、WebP 压缩对比：\nGIF 和 WebP 对比：\n不同网络环境，加载不同尺寸图片 如下是京东网站首页占据 C 位的宣传图：\n它的 URL 地址如下，你任意改变这张图片的 URL 里面的宽、高，放到浏览器里面重新进行请求，就可以得到相应大小的图片：\n响应式图片 不同平台设备加载不同大小、甚至不同内容的图片！\nCSS 媒体查询 @media all and (max-width: 600px) { img { width: 300px; } } @media all and (min-width: 600px) and (max-width: 1200px) { img { width: 900px; } } srcset、sizes、picture 和 source （1）srcset 属性\nimg 标签的 srcset 属性定义了我们允许浏览器选择的图像集，以及每个图像的大小：\n\u0026lt;img srcset=\u0026#34;images/team-photo.jpg 1x, images/team-photo-retina.jpg 2x, images/team-photo-full.jpg 2048w, images/team-photo-default.jpg\u0026#34; src=\u0026#34;team-photo.jpg\u0026#34;\u0026gt; 上述代码，srcset 属性给出了四个图像的 URL。1x 或 2x 这个 x 代表 pixel density descriptor，代表当当前设备的像素密度是标准像素密度的 1 倍或 2 倍时，去加载对应的图片，这个值可以是浮点数。2048w 的 w 代表 width descriptor，也就是图片的以像素为单位的宽度，当浏览器的渲染器需要去渲染宽度为 2048 像素的图片的时候，就会使用这张图片，这个值必须是正整数。\n最后一个图像 URL images/team-photo-default.jpg 没有带 x 或者 w，这表示这张图片是一张候选图片，当 srcset 其它图片都没有匹配上的时候，那么就会使用这种图片。当然，候选图片是可选的，如果不提供，就会选择使用 src 图片作为默认图片。\n（2）sizes 属性\nsrcset 提供了一个图片的候选集，但是选择哪个图片是浏览器决定的，我们无法灵活控制。如果想要自己根据屏幕的大小，来告诉渲染器去渲染多大尺寸的图片，可以引入 sizes 属性。\n\u0026lt;img src=\u0026#34;/files/16870/new-york-skyline-wide.jpg\u0026#34; srcset=\u0026#34;/files/16870/new-york-skyline-wide.jpg 3724w, /files/16869/new-york-skyline-4by3.jpg 1961w, /files/16871/new-york-skyline-tall.jpg 1060w\u0026#34; sizes=\u0026#34;((min-width: 50em) and (max-width: 60em)) 50em, ((min-width: 30em) and (max-width: 50em)) 30em, (max-width: 30em) 20em\u0026#34;\u0026gt; sizes 的格式：媒体查询条件 目标值。\n上述的 sizes 属性表示，当屏幕宽度位于 50em ~ 60em 的时候，渲染所需图片宽度为 50em，当屏幕宽度位于 30em ~ 50em 的时候，渲染所需图片宽度为 30em，当屏幕宽度小于 30em，渲染所需图片宽度为 20em。当渲染器所需渲染的图片宽度确定的时候，再去 srcset 中去匹配最接近该宽度的图片 URL 即可。\n目标值的单位可以是：\n 相对字体大小的单位：em 或 ex 绝对单位：px 或 cm Viewport(窗口) 百分比单位：vw  （3）picture 和 source\nsrcset 和 sizes 可以让网站根据屏幕大小来加载**内容相同的（虽然也可以不同，但不应该这样用）**不同大小的图片，如果还想根据不同的屏幕，显示不同内容的图片，例如电脑上显示如下比较宽的图：\n而手机上为了突出人物，显示比较瘦的图：\n可以使用 \u0026lt;picture\u0026gt; 和 \u0026lt;source\u0026gt; 标签来解决这个问题：\n\u0026lt;picture\u0026gt; \u0026lt;source media=\u0026#34;(max-width: 799px)\u0026#34; srcset=\u0026#34;elva-480w-close-portrait.jpg\u0026#34;\u0026gt; \u0026lt;source media=\u0026#34;(min-width: 800px)\u0026#34; srcset=\u0026#34;elva-800w.jpg\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;elva-800w.jpg\u0026#34; alt=\u0026#34;Chris standing up holding his daughter Elva\u0026#34;\u0026gt; \u0026lt;/picture\u0026gt; 上述代码的 \u0026lt;img\u0026gt; 标签是必须提供，否则不会显示任何图片，当 \u0026lt;source\u0026gt; 的媒体查询条件都不满足的时候，\u0026lt;img\u0026gt; 将会显示 src 所指向的 URL 图片。\n除此之外，\u0026lt;picture\u0026gt; 标签还可以根据浏览器所能渲染的图片格式来选择不同的图片：\n\u0026lt;picture\u0026gt; \u0026lt;source type=\u0026#34;image/svg+xml\u0026#34; srcset=\u0026#34;pyramid.svg\u0026#34;\u0026gt; \u0026lt;source type=\u0026#34;image/webp\u0026#34; srcset=\u0026#34;pyramid.webp\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;pyramid.png\u0026#34; alt=\u0026#34;regular pyramid built from four equilateral triangles\u0026#34;\u0026gt; \u0026lt;/picture\u0026gt; 当浏览器按照 \u0026lt;source\u0026gt; 出现的顺序，依次探测是否满足条件，如果满足，就加载相应图片。\n懒加载 有很多库可以实现懒加载图片，例如 lazysizes。对于每一张图片，需要确定在其原图内容还未被浏览器渲染出来之前，这张图应该显示什么占位符，目前有两个库可以生成图像的占位符。当然，如果对于所有图片，都像是用一个统一的图片作为占位符，也未尝不可。\nLQIP LQIP 就是 Low Quality Image Placeholders (低质量图像占位符) 的缩写。\nSQIP 上述 LQIP 是基于像素的解决方案，而 SQIP 是 SVG-based LQIP (基于 SVG 的低质量图像占位符) 的缩写，其实基于 SVG 的，在任何设备上都看起来很清晰。\n下图展示的是一个效果图：\n换一种方式来表达图片 Data URL 对于一些较小的图片，可以将内容直接内嵌在 URL 中，常见格式如下：\ndata:[\u0026lt;mediatype\u0026gt;][;base64],\u0026lt;data\u0026gt; mediatype 标识图片的格式，例如 image/jpeg 等。\nImage sprites (雪碧图) 对于 HTTP 1.X 协议，雪碧图（即多张小图片合成一张大图）的加载方式，是很多网站都会采用的技巧，这种方式的缺点是，如果某个应用图标需要更新，那么整张大图都需要替换。\n然而对于 HTTP 2.0 协议的到来，加载多张小图是比较推荐的做法，因为多张图片本身的下载都会复用同一个连接。\n参考  HTMLImageElement.srcset Responsive images Essential Image Optimization  扫描下面二维码，在手机端阅读：\n"});index.add({'id':28,'href':'/docs/books/beauty_of_mathematics/','title':"数学之美",'content':"数学之美 2000多年前，古埃及人在罗塞塔石碑上，用三种文字记录了托勒密五世登基的诏书，这帮助后人破解了古埃及的象形文字，让我们了解了5000年前古埃及的历史。可见信息冗余是信息安全的保障，这对于信息编码具有重要指导意义。\n犹太人为了避免抄错《圣经》，发明了一种校验码的方法，他们把每一个希伯来字母对应于一个数字，这样每行文字加起来便得到一个特殊的数字，这样的数字变成为了这一行的校验码。\n隐含马尔可夫链成功应用在机器翻译、拼写纠错、手写体识别、图像处理、基因序列分析、股票预测和投资等方面。\n如何准确的识别出一个快递地址，写一个分析器去分析这些描述恐怕是不行的，因为地址是比较复杂的上下文有关的文法。答案是使用有限状态机。当用户输入的地址不太标准或有错别字的时候，有限状态机会束手无措，因为有限状态机是严格匹配的，所以科学家提出了基于概率的有限状态机。\n2002 年，Google 想要做一个全新的中、日、韩搜索算法，吴军写的算法比较简单，但是占用内存比较多，Google 服务器数量还没有那么多。辛格提出，用一个拟合函数替换很耗内存的语言模型，无需增加任何服务器，但是搜索质量会降到 80%。辛格指出，这样可以提早两个月将这个新算法提供给中国的用户，用户体验会有质的提高。辛格做事情的哲学，先帮助用户解决 80% 的问题，再慢慢解决剩下的 20% 的问题，是在工业界成功的秘诀之一。\n新闻分类的关键在于计算出两篇新闻的相似度，每篇新闻变成一个向量，最后余弦定理可以计算出来相似度。但两两计算的迭代次数太多，如何一次性就把所有新闻的相关性计算出来呢？答案是矩阵运算中的奇异值分解。\n如何判断两个集合是否相同？一种答案是双层 for 循环一一比较，复杂度 O(N^2)；稍好一点的办法是对集合进行排序，然后顺序比较，时间复杂度 O(NlogN)；还可以将一个集合的元素放到散列表里面，另外一个与之一一对比，时间复杂度 O(N)，但是额外使用了 O(N) 的空间，不完美；最完美的是计算这两个集合的指纹，对一个集合中的元素分别计算指纹，然后一一相加。\n如何判断两个集合基本相同？答案是 Simhash。判断两个网页是否重复，也没有必要完全从头比到尾，只需要每个网页挑选出几个词 (IDF 最大的几个词)，构成特征词，然后计算信息指纹即可。判断一篇文章是否抄袭另外一篇文章，每篇文章切成小的片段，挑选特征词，并计算指纹。YouTuBe 如何从上百万视频中找出一个视频是否另外一个视频的盗版？其核心在于关键帧的提取和特征的提取。关键帧对于视频的重要性，就如同主题词对于新闻的重要性一样。\n最大熵原理指出，对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设，这种情况下，概率分布最均匀，预测的风险最小。例如拼音输入法，Wang-Xiao-Bo 转换为王晓波和王小波，唯一确定用户需要的是哪一个，非常难。\n"});index.add({'id':29,'href':'/docs/programmer-interview/data-structure/','title':"数据结构",'content':"数据结构         Map    "});index.add({'id':30,'href':'/docs/tutorial/sentinel/architecture/','title':"架构",'content':"架构 随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 是面向分布式服务架构的流量控制组件，主要以流量为切入点，从限流、流量整形、熔断降级、系统负载保护、热点防护等多个维度来帮助开发者保障微服务的稳定性。\n有关 Sentinel 更为详细的使用文档和介绍请移至 Sentinel Github Wiki。\n主要特性 流量控制 流量控制在网络传输中是一个常用的概念，它用于调整网络包的发送数据。然而，从系统稳定性角度考虑，在处理请求的速度上，也有非常多的讲究。任意时间到来的请求往往是随机不可控的，而系统的处理能力是有限的。我们需要根据系统的处理能力对流量进行控制。Sentinel 作为一个调配器，可以根据需要把随机的请求调整成合适的形状，如下图所示：\n熔断降级 除了流量控制以外，及时对调用链路中的不稳定因素进行熔断也是 Sentinel 的使命之一。由于调用关系的复杂性，如果调用链路中的某个资源出现了不稳定，可能会导致请求发生堆积，进而导致级联错误。\n网关限流 Sentinel 支持对 Spring Cloud Gateway、Zuul 等主流的 API Gateway 进行限流。\n架构 另外一幅更为漂亮的图：\n可扩展性 开源生态 参考  Sentinel Wiki  "});index.add({'id':31,'href':'/docs/javascript/understand-this-keyword/','title':"理解 This 关键字",'content':"理解 This 关键字 JavaScript 中的 this 所指向的对象，取决于上下文以及函数被调用的方式，本文列举了几种常见的情况，帮助大家理解。\n一、全局上下文 当直接在一个全局的上下文中，使用 this 指针的时候，this 指针会指向到全局对象上。例如在浏览器的调试工具栏中直接打印 this 指针，其指向的是 Window 对象：\n在 node 中打印 this 指针，其指向的是 node 提供的全局对象，其中包含了进程信息等：\n二、Function 上下文 在 Function 上下文中，this 的值取决于 function 是如何被调用的。\n(1) Function 调用 当 this 指针定义在一个 function 中，那么此 this 仍然会指向全局对象：\nfunction foo() { console.log(this) } foo(); // Window {parent: Window, postMessage: ƒ, blur: ƒ, focus: ƒ, close: ƒ, …} (2) 严格模式下的 Function 调用 如果在严格模式下定义的 function 的话，this 指针的值将会是 undefined：\nfunction foo() { \u0026#39;use strict\u0026#39;; console.log(this) } foo(); // undefined (3) Method 调用 Method 调用指的是，function 作为一个对象的属性而存在。当 this 指针被定义在一个对象内的时候，那么其将会指向紧紧包裹自己的这个对象。\nvar obj = { name: \u0026#39;outerObj\u0026#39;, innerObj: { name: \u0026#39;innerObj\u0026#39;, foo: function() { console.log(this.name) } } }; console.log(obj.innerObj.foo()) // innerObj (4) 构造器调用 当 function 被用于构造器的时候，那么定义在构造器内部的 this 指针将会指向此构造器新 new 出来的实例对象。\nfunction Person(name) { this.name = name console.log(this) } console.log(new Person(\u0026#34;Tom\u0026#34;)) // Person {name: \u0026#34;Tom\u0026#34;} (5) call()、apply()、bind() 调用 这三个函数最大的特点就是，你可以通过参数为他们指定 this 指针所需要指向的对象：\nfunction add(inc1, inc2) { var value = this.a + inc1 + inc2; console.log(this) return value; } var o = { a : 4 }; console.log(add.call(o, 5, 6)) // {a: 4} console.log(add.apply(o, [5, 6])) // {a: 4}  var g = add.bind(o, 5, 6) console.log(g()) // {a: 4} (6) ES6 箭头函数调用 当你使用 ES6 箭头函数的时候，this 指针返回的总是箭头函数定义所在位置的上一级的函数作用域的 this 对象，是箭头函数被 function() { } 包裹的作用域中的 this 对象。如下面示例，this 指向的是 log() 函数内部的 this 指针的值：\nclass Student { log() { // 这个地方的 this 的值  setTimeout(() =\u0026gt; console.log(this === student), 100) } } const student = new Student() student.log() // true 但是如果上一级并不是位于函数作用域中，而是位于 Object 对象嵌套层级中，则需要继续向上找函数作用域，因为 Object 嵌套层级不构成单独的作用域。如下所示 this 指针指向的是 Window 对象，而非 o 对象：\nvar o = { b: () =\u0026gt; { console.log(\u0026#39;this is\u0026#39;, this); // this is Window  } } o.b(); 三、参考  How does the “this” keyword work? Gentle Explanation of \u0026ldquo;this\u0026rdquo; in JavaScript MDN this 箭头函数this的指向问题  扫描下面二维码，在手机端阅读：\n"});index.add({'id':32,'href':'/docs/tutorial/awk/intro/','title':"简介",'content':"简介 AWK 是一门编程语言。\n开始 假设您有一个名称为 emp.data 文件，里面存储的内容包含姓名、每小时的薪资、工作的小时，如下所示：\nBeth 4.00 0 Dan 3.75 0 Kathy 4.00 10 Mark 5.00 20 Mary 5.50 22 Susie 4.25 18 现在你想要打印工作超过 0 小时的员工的姓名和薪资，对于 AWK 而言，这相当简单：\nawk `$3 \u0026gt; 0 { print $1, $2 * $3 }` emp.data 你会得到如下输出：\nKathy 40 Mark 100 Mary 121 Susie 76.5 位于引号中的内容就是 AWK 的完整代码。$3 \u0026gt; 0，会匹配文件的每一行，看这每一行的第 3 列是否大于 0。{ print $1, $2 * $3 } 打印第一列，以及第二列和第三列的乘积。\n如果你想要打印出工作小时数是 0 的员工姓名：\nawk `$3 == 0 { print $1 }` emp.data AWK 程序结构 pattern { action } pattern { action } ... 程序中 pattern 会对文件的每一行进行测试，测试为 true，才会对这一行执行后面的 action。\n运行 AWK awk `program` input_file 如果 AWK 代码太长，假设你将其代码存储到了 progfile 文件中，那么可以这样运行：\nawk -f progfile input_file "});index.add({'id':33,'href':'/docs/programmer-interview/algorithm/','title':"算法",'content':"算法面试题 本专栏的面试题来自于牛客网、一亩三分地、LeetCode、LintCode等网站，覆盖了一线互联网如BAT、TMD、微软、亚马逊等巨头，在校招或者社招的时候最容易出的算法面试题。\n数组    数组       环形数组（微软） 最大子数组之和为 K（微软） 下一个排列（微软）   两个有序数组合并后的中位数（微软） 买卖股票（微软） 生成螺旋矩阵（微软）   两个有序数组第 K 大的数（微软）      树    树       二叉搜索树中删除一个节点（微软） 二叉搜索树中新增一个节点（微软） 二叉树的直径（微软、头条）   中序遍历的下一个节点 二叉树最大路径和（微软） 二叉树非递归中序遍历（微软）   二叉树最近公共祖先（微软） 一颗二叉树是否是另外一颗的子树（微软） 二叉树右视图（微软）    DFS    DFS      WordLadder（微软、阿里） 二维数组寻找最长的单调递增序列（微软）    栈    栈      写一个计算器(逆波兰表达式)（微软） MinStack（微软）    排序    排序       PancakeSorting（微软） 颜色排序（网易、阿里） 堆排序（微软）   链表归并排序 快排序 归并排序   栈排序（美团） 磁盘归并排序（微软）     字符串    字符串      找出最多 K 个不同字符的最长子串（微软） 两个字符串整数相加（微软）    动态规划    动态规划       二维矩阵数值和最小的路径（微软） 最小火车票费用（亚马逊） 最长递增子序列 LIS（微软）    链表    链表      链表是否有环（微软） 找出链表环的入口节点（微软）    回溯题    回溯      组合总和 为运算表达式设计优先级（头条）    数学    数学     开根号（微软）    其它         读写：5 个线程读 1 个线程写（微软）    "});index.add({'id':34,'href':'/docs/programmer-interview/design-pattern/observer/','title':"观察者模式",'content':"观察者模式 JavaScript class EventObserver { constructor() { this.observers = []; } subscribe(fn) { this.observers.push(fn); } unsubscribe(fn) { this.observers = this.observers.filter((subscriber) =\u0026gt; subscriber !== fn); } broadcast(data) { this.observers.forEach((subscriber) =\u0026gt; subscriber(data)); } } 发布订阅者模式 其实遵循的就是观察者模式\n// publisher // Subscriber // unsubscribe // Some place to store callbacks that are registered from subscribers.  function pubSub() { // object which will track of all events and subscription  const subscribers = {} // Publisher:  function publish(eventName, data) { // return if event is not subscribed  if (!Array.isArray(subscribers[eventName])) { return } // Whenever you publish any event, it will trigger callback for all stored event in subscriber object  subscribers[eventName].forEach((callback) =\u0026gt; { callback(data) }) } // Subscriber  function subscribe(eventName, callback) { if (!Array.isArray(subscribers[eventName])) { subscribers[eventName] = [] } //on subscribe we will we will push callback to subscribers[eventName] array  subscribers[eventName].push(callback); const index = subscribers[eventName].length - 1 // subscribed callbacks to be removed when they are no longer necessary.  return { unsubscribe() { subscribers[eventName].splice(index, 1); }, } } return { publish, subscribe, } } 实现一个 EventBus 其实就是让你写观察者模式。\n参考  The Observer Pattern JavaScript Design Patterns: The Observer Pattern Design a pub sub pattern in vanilla JS  "});index.add({'id':35,'href':'/docs/tutorial/','title':"💡教程",'content':"💡教程  前端优化指南 UNIX 常用命令大全 Git 教程 网络协议  "});index.add({'id':36,'href':'/posts/half-life-of-information/','title':"信息的半衰期",'content':"今天，我想与您讨论一下信息能存活多久的问题，这个问题又会如何影响我们工作的方式。\n信息衰减 你可能不止一遍地听闻我们生活在一个“信息时代”，信息无所不在，无论是个人还是公司，掌握了信息就能让你事半功倍。而与此同时，随着信息被源源不断的生产出来，其又会很快变为过时信息，对人们能够起到的帮助越来越少。\n对于公司而言，有许多因素都会加速信息衰减的速度：员工离职、文档过时、1对1的知识分享等。而采用远程办公进行协作的团队，他们更为担心这个问题，员工可能不在一个时区，彼此之间也不能进行面对面的鼓励，知识分享也存在一些额外地物理限制。\n所以，我们应该如何避免或减缓知识衰减的速度呢？\n信息的半衰期 在核物理学中，放射性物质的半衰期是指它将衰变减少到一半所需的时间。同样，知识、信息或事实的半衰期是指知识的一半变得不相关或过时所需的时间。\n让我举几个例子来解释这一点。一家公司的新闻稿或投资者最新消息通常会有几个星期的半衰期。另一方面，你公司的 WIKI 或作业指导书的半衰期约为 0.5 - 1 年。\n信息的类型 这篇文章将知识分为三种不同的形式：\n 无意识知识：员工主动使用但可能没有意识到的知识（例如，使用工具或特定工作流的特定方式） 有意识的知识：员工知道的方法、过程、标准操作程序等 记录的知识：以可访问和有形形式捕获的所有信息  你可能已经猜到记录的知识半衰期最高，而无意识的知识半衰期最低。\n有效的知识转移与提高知识半衰期 因此，我们的目标应该是促进知识的流动或转移，主要是从无意识到有记录。实际上，这将增加知识的半衰期，并确保贵公司的信息长期保持相关和更新。\n下面是一些可以立即实现的功能：\n 默认为异步  即时消息或聊天的半衰期通常较短，约为5-10分钟，而长格式消息的半衰期至少为几个小时。另外，当您默认使用异步长格式对话时，您反过来会鼓励更周到的响应，并减少来回的次数。\n 跨团队共享知识  这是扩大公司记录知识的第一步。单个团队通常拥有大量的数据和洞察，如果在公司内部共享这些数据和洞察，可以帮助建立坚实的知识库。\n 开始建立学习型组织  更进一步，你应该致力于在你的组织中建立一种学习的文化。HBR的这篇文章很好地解释了什么是学习型组织，以及它如何促进员工的持续学习。\n 使用协作工作管理工具  像Slack这样的即时消息工具是很好的，并且是您的技术栈的重要组成部分。但是，您还应该投资于协作工作管理工具，如Notion或Slite，以存储、维护、优先排序和共享知识。\n译文来源  Knowledge Decays and Half-life of Information "});index.add({'id':37,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock-2/','title':"Best Time to Buy and Sell Stock Ⅱ",'content':"Best Time to Buy and Sell Stock Ⅱ 题目 LeetCode 地址：Best Time to Buy and Sell Stock Ⅱ\n有一个数组，第 i 个元素的值代表第 i 天的股票价格，如果你可以进行无限次交易（某天买入一支股票，然后过几天卖掉），请问你能收获的最大利润是多少？\n分析 这道题就一个想法，只要今天 price[i] 比昨天 price[i - 1] 的价格涨了，就可以算作是有效的利润，累加到最后的结果中。\n答案 // 假设有一个数组，它的第i个元素是一个给定的股票在第i天的价格。 // 设计一个算法来找到最大的利润。你可以完成尽可能多的交易(多次买卖股票)。 // 然而,你不能同时参与多个交易(你必须在再次购买前出售股票)。 // // https://www.lintcode.com/problem/best-time-to-buy-and-sell-stock-ii/description public class BestTimetoBuyandSellStockII { public int maxProfit(int[] prices) { int max = 0; for (int i = 1; i \u0026lt; prices.length; i++) { int diff = prices[i] - prices[i - 1]; if (diff \u0026gt; 0) { max += diff; } } return max; } } 扫描下面二维码，在手机上阅读这篇文章：\n"});index.add({'id':38,'href':'/docs/tutorial/unix-command/find/','title':"find",'content':"find find 命令的常见用法有哪些？find 命令的例子。\n简介 Linux Find命令是类Unix操作系统中最重要、最常用的命令行实用程序之一。find 命令可以，根据不同的查找条件，来查询匹配不同的文件或目录列表。\nfind 可用于根据各种条件查找，例如您可以按权限、用户、组、文件类型、日期、大小和其他可能的条件查找文件。\n通过本文，我们将以示例的形式分享我们的日常Linux find命令体验及其使用。\n格式 find [path...] [test...] [action...]  path：find 命令的第一件事，查看每个路径 test：对于遇到的每个文件，find 应用测试条件 action：一旦搜索完成，find 对每个文件执行相应的操作  路径示例：\n find /usr/bin find / find . find ~  测试示例：\n -name pattern：包含 pattern 的文件名 -iname pattern：包含 pattern 的文件名（忽略大小写） -type [df]：文件类型，d 代表目录，f 代表文件 -perm mode：设置为 mode 的文件权限 -user userid：用户为 userid -group groupid：组为 groupid -size [-+]n[cbkMG]：大小为 n[字符(字节)、块、千字节、兆字节、吉字节] -empty：空文件 -amin [-+]n：n 分钟之前访问 -anewer file：file 文件之后访问 -atime [-+]n：n 天之前访问 -cmin [-+]n：n 分钟之前状态改变 -cnewer file：file 文件之后状态改变 -ctime [-+]n：n 天状态之前改变 -mmin [-+]n：n 分钟之前修改 -mtime [-+]n：n 天之前修改 -newer file：file 文件之后修改   - 代表：小于，+ 代表：大于\n 另外对于测试条件而言，可以使用 ! 对测试条件求反：\n# 显示扩展名不是 jpg 的文件 find ~ -type f \\! -name \u0026#39;*.jpg\u0026#39; -print 动作示例：\n -print：路径名写入到标准输出 -fprint file：同 -print，将输出写入到 file 中 -ls：显示长目录列表 -fls file：同 -ls，将输出写入到 file 中 -delete：删除文件 -exec command {} \\;：执行 command，{} 代表匹配的文件名 -ok command {} \\;：同 -exec，但是在执行 command 之前进行确认  示例 （1）在当前目录查找文件全名为 \u0026lsquo;abc.txt\u0026rsquo; 的文件\nfind . -name abc.txt  点 . 代表当前目录\n 在 /home 目录查找就是：\nfind /home -name abc.txt （2）忽略文件名的大小写\nfind . -iname abc.txt 可能查找出来的有：abc.txt、Abc.txt\n（3）查找名称为 abc 的文件夹\nfind / -type d -name abc -type 的值代表文件类型，可能的值如下所示：\n b：block special c：character special d：目录 f：普通文件 l：symbolic link p：FIFO s：socket  （4）查找当前目录的所有 Java 文件\nfind . -type f -name \u0026#39;*.java\u0026#39; （5）查找权限不是 777 的文件\nfind / -type f ! -perm 777  感叹号 ! 表示反匹配\n （6）查找权限为 777 的文件，并改为 644 文件\nfind / -type f -perm 0777 -print -exec chmod 644 {} \\; -print 代表匹配的文件打印到命令行中\n（7）查找文件并将其删除\nfind . -type f -name \u0026#39;*.txt\u0026#39; -exec rm -f {} \\; （8）查找所有空文件\nfind /tmp -type f -empty （9）查找所有隐藏文件\nfind /tmp -type f -name \u0026#39;.*\u0026#39; （10）查找 abc.txt 并且是用户 tom 拥有的文件\nfind / -user tom -name abc.txt （11）查找所有过去 50 天内修改过的文件\nfind / -mtime 50 （12）查找所有过去 50 - 100 天内修改过的文件\nfind / -mtime +50 -mtime -100 （13）查找所有过去 50 天内访问过的文件\nfind / -atime 50 （14）找到过去 1 小时之内修改过的文件\nfind / -cmin -60  -cmin 代表 change 的文件（单位：分钟） -mmin 代表 modified 的文件（单位：分钟） -amin 代表 assessed 的文件（单位：分钟）  （15）找到 50MB 大小的文件\nfind . -size 50M （16）找到文件位于 50MB - 100MB\nfind . -size +50M -size -100M exec 命令简介 上述有几个例子，在搜索到指定文件后，使用 exec 命令对这些文件做了一些其它操作：比如删除、修改权限等操作。以下面命令为例：\nfind . -type f -name \u0026#39;*.txt\u0026#39; -exec rm -f \u0026#39;{}\u0026#39; \\;  exec 命令，可以对与 find 表达式匹配的每个对象执行命令 {} 是 find 搜索到的文件的占位符 exec 命令以 ; 结束，但是呢需要使用 \\ 来对其进行转义，以防止 ; 被转义 {} 最好使用单引号括起来：'{}'，以避免由于文件的名字格式等问题出现错误  参考  35 Practical Examples of Linux Find Command Find Files in Linux, Using the Command Line  扫描下面二维码，在手机端阅读：\n"});index.add({'id':39,'href':'/docs/tutorial/front-end-optimization-guide/html-optimization/','title':"HTML 优化",'content':"HTML 优化 本文通过一些案例讲述了常见的优化 HTML 的几种小技巧：减少 DOM 树、精简 HTML 文件大小等。\n优化 DOM 节点树 去除页面除首屏外的对于用户不可见的信息区块，可以让页面的 DOM 节点数更少，DOM 树结构更简单，然后再使用懒加载异步化请求，去动态加载这些不可见的信息区块。\n在《大型网站性能优化实战》这本书中，作者为了优化搜索页面的渲染瓶颈问题，将首屏以下的 33 各搜索结果对应的 HTML 代码放到 \u0026lt;textarea\u0026gt; 节点中，当该区域处于可见状态时，再从 TextArea 中取出 HTML 代码，恢复到 DOM 树中进行渲染。这样一来，页面首次渲染的 DOM 树所包含的节点数大幅度减少，从而有效提高了首次渲染速度。\n多个空格合并为一个空格 通过将多个空格合并为一个空格，可以减少 HTML 的大小，从而缩短传输 HTML 文件所需的时间。通常在编写 HTML 文件的时候，总是倾向于格式化它，以方便我们人类阅读，所以这个文件中填充了许多空格，但这些空格对于浏览器来说是用不到的。在替换空格的时候，需要保留 \u0026lt;pre\u0026gt;、\u0026lt;textarea\u0026gt;、\u0026lt;script\u0026gt;、\u0026lt;style\u0026gt; 中的空格。\n不过，如果你的网页中使用了 white-space: pre 这个 CSS 属性就要小心了，这个属性可以避免让多个空格压缩为一个，在实际开发网站的时候，其实也很少用到这个属性。如果确实需要，那么就放弃把 HTML 的多个空格合并为一个空格吧。\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Hello, world! \u0026lt;/title\u0026gt; \u0026lt;script\u0026gt; var x = \u0026#39;Hello, world!\u0026#39;;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Hello, World! \u0026lt;pre\u0026gt; Hello, World! \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 转为：\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Hello, world!\u0026lt;/title\u0026gt; \u0026lt;script\u0026gt; var x = \u0026#39;Hello, world!\u0026#39;;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Hello, World! \u0026lt;pre\u0026gt; Hello, World! \u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 精简属性 \u0026lt;button\u0026gt; 按钮的 disabled 属性：\n\u0026lt;button name=\u0026#34;ok\u0026#34; disabled=\u0026#34;disabled\u0026#34;\u0026gt; 可以写为\n\u0026lt;button name=\u0026#34;ok\u0026#34; disabled\u0026gt; 而 \u0026lt;form\u0026gt; 表单：\n\u0026lt;form method=\u0026#34;get\u0026#34;\u0026gt; 可以写为：\n\u0026lt;form\u0026gt; 即许多 HTML 节点的属性都有默认值，如果指定的属性值等于默认属性的值，那么就可以移除掉。\n移除注释 如下的 HTML 片段带有注释，去除注释，可以减少 HTML 文件的体积：\n\u0026lt;i class=\u0026#34;service_ico\u0026#34;\u0026gt; \u0026lt;!-- 常态 icon --\u0026gt; \u0026lt;img class=\u0026#34;service_ico_img\u0026#34; src=\u0026#34;service_ico_img.png\u0026#34;/\u0026gt; \u0026lt;!-- hover 态 icon --\u0026gt; \u0026lt;img class=\u0026#34;service_ico_img_hover\u0026#34; src=\u0026#34;service_ico_img_hover.png\u0026#34;/\u0026gt; \u0026lt;/i\u0026gt; 移除引号 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;BikeCrashIcn.png\u0026#34; align=\u0026#39;left\u0026#39; alt=\u0026#34;\u0026#34; border=\u0026#34;0\u0026#34; width=\u0026#39;70\u0026#39; height=\u0026#39;30\u0026#39; \u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 转变为\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=BikeCrashIcn.png align=left alt=\u0026#34;\u0026#34; border=0 width=70 height=30 \u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 一方面去除了无关的引用，另外一方面也有利于 Gzip 算法的压缩。\n精简 URL 链接 如果 URL 的协议头与当前页面的协议头一致，那么可以删除协议头；在表达 URL 路径的时候，如果 HOST 与本网页一致，那么此 URL 也没必要使用绝对 URL 路径，使用相对 URL 路径即可。\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;base href=\u0026#34;http://www.example.com/\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;a href=\u0026#34;http://www.example.com/bar\u0026#34;\u0026gt;Link with long URL\u0026lt;/a \u0026gt; \u0026lt;img src=\u0026#34;http://www.othersite.example.org/image.jpg\u0026#34;\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 转变为：\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;base href=\u0026#34;http://www.example.com/\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;a href=\u0026#34;bar\u0026#34;\u0026gt;Link with long URL\u0026lt;/a \u0026gt; \u0026lt;img src=\u0026#34;//www.othersite.example.org/image.jpg\u0026#34;\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 参考  OptimizeHtml 《大型网站性能优化实战》  扫描下面二维码，在手机端阅读本文：\n"});index.add({'id':40,'href':'/docs/tutorial/network/http/','title':"HTTP",'content':"HTTP 常见状态码    状态码 含义     1xx Information response   100 continue   101 Switching Protocols   2xx success   200 OK: 成功响应   201 Created   202 Accepted   204 No Content: 服务器已经成功处理请求，没有返回任何 Body (比如服务器收到一个发邮件的请求，服务器返回 204，表示已经收到请求，邮件后续会发送)   206 Partial Content: 服务器返回了某个文件的一部分   3xx redirection   301 Moved Permanently: 永久重定向   302 Found: 临时重定向   304 Not Modified: 浏览器通过 If-None-Match 头或 If-Modified-Since 头询问，服务器告知文件未改动   4xx client errors   400 Bad Request: 客户端发送的 HTTP 有语法错误、太大、帧错误等   401 Unauthorized   403 Forbidden   404 Not Found   405 Method Not Allowed   429 Too Many Requests   5xx server errors   500 Internal Server Error   502 Bad Gateway   503 Service Unavailable   504 Gateway Timeout    HTTP 方法    方法 含义     GET 获取数据   HEAD 与 GET 类似，但只返回响应头   POST 提交表单   PUT 用一个新的资源完全替换掉服务器的资源   DELETE 删除资源   CONNECT 建立一个 tunnel   OPTIONS 询问服务器支持哪些方法   TRACE 发起环回诊断，主要用于诊断   PATCH 对服务器资源进行部分更新    HTTP 报文 在浏览器中输入 \u0026ldquo;kunzhao.org\u0026rdquo;，然后敲击回车的时候，浏览器发送的请求报文示例：\nGET / HTTP/1.1 Host: kunzhao.org Accept-Language: en-US 服务器对应的返回的响应报文示例：\nHTTP/1.1 200 OK Date: Mon, 23 May 2005 22:38:34 GMT Content-Type: text/html; charset=UTF-8 Content-Length: 155 Last-Modified: Wed, 08 Aug 2020 23:11:55 GMT Server: Apache/1.3.3.7 (Unix) (Red-Hat/Linux) ETag: \u0026quot;3f80f-1b6-3e1cb03b\u0026quot; Accept-Ranges: bytes Connection: close \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;首页|赵坤的个人网站\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;首页内容示例\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 常见 HTTP 头 通用头  通用头：请求和响应都可以使用的头。\n    HTTP 头 示例 含义     Date Date: Wed, 21 Oct 2015 07:28:00 GMT 消息的日期和时间   Cache-Control Cache-Control: no-cache 控制缓存的策略   Connection Connection: keep-alive 服务器/浏览器读/写完消息后，这个 TCP 连接是否应该维持打开的状态，还是说立即关闭    请求头    HTTP 头 示例 含义     User-Agent User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36 Edg/85.0.564.51 包含操作系统、浏览器等字符串形式的描述信息   Host Host: example.com:8001 服务器的域名和端口   Cookie Cookie: PHPSESSID=298zf09hf012fh2; csrftoken=u32t4o3tb3gg43; _gat=1 存储了由服务器 Set-Cookie 头设置的 Cookie 信息   Referer Referer: example.org 哪个 URL 跳转到当前页面的   Accept Accept: text/html;q=0.9,image/webp,image/apng 浏览器期望收到的是什么 MIME 类型的资源   Accept-Charset Accept-Charset: utf-8, iso-8859-1;q=0.5 浏览器理解哪些字符编码   Accept-Encoding Accept-Encoding: deflate, gzip;q=1.0, *;q=0.5 浏览器能够处理哪些压缩算法   Accept-Language Accept-Language: fr-CH, fr;q=0.9, en;q=0.8, de;q=0.7, *;q=0.5 浏览器期望返回的是英文？中文？还是俄文？   If-Match If-Match: \u0026ldquo;67ab43\u0026rdquo;, \u0026ldquo;54ed21\u0026rdquo;, \u0026ldquo;7892dd\u0026rdquo; 只有匹配任意一个 ETag，服务器才会返回资源   If-None-Match If-None-Match: \u0026ldquo;bfc13a64729c4290ef5b2c2730249c88ca92d82d\u0026rdquo; 只要有不匹配的 ETag，服务器就会返回资源，响应码：200   If-Modified-Since If-Modified-Since: Wed, 21 Oct 2015 07:28:00 GMT 只有在这个日期之后修改后，服务器才返回资源，响应码：200   If-Unmodified-Since If-Unmodified-Since: Wed, 21 Oct 2015 07:28:00 GMT 只有在这个日期之后未被修改过，服务器才会返回资源    响应头    HTTP 头 示例 含义     Age Age: 24 这个资源在代理缓存中待了有多少秒了   Location Location: /index.html 页面重定向到哪个 URL 了   Server Server: Apache/2.4.1 (Unix) 服务器用的是哪款 Web 服务器软件/框架   Set-Cookie Set-Cookie: id=a3fWa; Max-Age=2592000 设置 Cookie   Last-Modified Last-Modified: Wed, 21 Oct 2015 07:28:00 GMT 资源的上次修改时间   ETag ETag: \u0026ldquo;33a64df551425fcc55e4d42a148795d9f25f89d4\u0026rdquo; 唯一标识一个资源    Entity 头  Entity 头：请求和响应都可以使用的头，并且是用来描述消息体的头。\n    HTTP 头 示例 含义     Content-Length Content-Length:128 描述 entity body 的大小（单位：字节）   Content-Language Content-Language: en-US 描述这个 entity 面向的受众，是中国用户？还是美国用户？   Content-Encoding Content-Encoding: gzip 采用的什么压缩算法?    HTTP 1.1 upgrade  升级为 HTTP over TLS  浏览器发送：\nGET http://example.bank.com/acct_stat.html?749394889300 HTTP/1.1 Host: example.bank.com Upgrade: TLS/1.0 Connection: Upgrade 服务器响应：\nHTTP/1.1 101 Switching Protocols Upgrade: TLS/1.0, HTTP/1.1 Connection: Upgrade  升级为 WebSocket  浏览器发送：\nGET /chat HTTP/1.1 Host: server.example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Origin: http://example.com 服务器响应：\nHTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk= Sec-WebSocket-Protocol: chat  升级为 HTTP/2  浏览器发送：\nGET / HTTP/1.1 Host: server.example.com Connection: Upgrade, HTTP2-Settings Upgrade: h2c HTTP2-Settings: \u0026lt;base64url encoding of HTTP/2 SETTINGS payload\u0026gt; 服务器响应：\nHTTP/1.1 101 Switching Protocols Connection: Upgrade Upgrade: h2c POST vs GET  在某些浏览器中，通过 Ajax 请求调用 GET 请求会缓存结果，拿到的响应可能只是一个缓存结果 GET 参数暴露在了 URL 中 POST 可以传递更多的信息、更多的数据  GET 存在的必要 既然数据通过 POST 请求也可以获取到，那 GET 存在的必要是什么？\n 浏览器地址栏、\u0026lt;a href=\u0026quot;kunzhao.org\u0026quot;\u0026gt; 这些链接等发送的都是 GET 请求 GET 请求的 URL 可以放到收藏夹 GET 请求的 URL 可以多次刷新  持久连接 HTTP 1.0 Connection: keep-alive HTTP 1.1 所有连接默认都是持久连接。\n参考  List of HTTP status codes Headers What is the difference between POST and GET?  "});index.add({'id':41,'href':'/docs/javascript/javascript-array/','title':"JavaScript 数组",'content':"JavaScript 数组 使用 JavaScript 在编程的时候，我们有很大一部分时间都是在与数组打交道，因此对数组常见的方法做到灵活的运用至关重要。本文整理了和 JavaScript 数组相关的，日常经常需要的功能和使用技巧，供大家参阅。\n从数组中移除指定元素 查阅 JavaScript 的数组 API，发现其并没有提供一个像 remove(obj) 或 removeAll(obj) 此类的方法，供我们方便的删除对象，因此我们需要通过使用其它的 API 来达到我们移出元素的目的。\n(1) 使用 splice 方法 splice 方法可以从指定索引处，向数组中添加元素或者删除元素，其会直接在原数组上改变，因此通过此方法可以达到我们的目的。但是在移除元素之前，我们必须首先通过 indexOf 方法找到我们的元素在数组中处于的索引位置。\nconst array = [2, 5, 9]; const index = array.indexOf(5); if (index \u0026gt; -1) { array.splice(index, 1); // 1 代表删除 1 个元素 } console.log(array) 当然，如果你不想使用 indexOf 的话，也可以直接从后向前遍历整个数组，对每个符合要求的元素都使用 splice 方法：\nconst array = [2, 5, 9]; for (var i = array.length; i--; ) { if (array[i] === 5) { array.splice(i, 1) } } console.log(array) 之所以需要从后向前移除，是因为在移除过程中，数组的 length 和 index 索引都是会改变的。\n(2) 使用 filter 方法 在 ES6 中，你可以使用 filter 函数遍历数组，对不符合元素值的对象进行过滤。filter 方法会返回一个新的数组，并不会直接在原数组上进行操作。\nlet value = 3; let array = [1, 2, 3, 4, 5, 3]; console.log(array.filter(item =\u0026gt; item !== value)) 如何遍历数组元素 JavaScript 数组提供了非常多的方法，这些方法都可以用来遍历数组：\n(1) 使用 forEach 方法 var a = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]; a.forEach(function(entry) { console.log(entry) }) (2) 使用 for 遍历 var a = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]; for (var index = 0; index \u0026lt; a.length; ++index) { console.log(a[index]) } (3) 使用 for 反向遍历 for (var i = array.length; i--; ) { // ... } 上述反向遍历的原理是：i-- 是属于测试条件的一部分，在每一次开始执行方法体之前，i 的值已经提前执行了 -- 这个操作了。当最后一次迭代，发现 i 等于 0 的时候，这个循环自然会停下来。\n(4) 使用 for-of 遍历 ES6 添加了迭代器的概念，当你使用 for-of 遍历的时候，其实已经隐形的在使用迭代器了：\nvar a = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]; for (const val of a) { console.log(val); } (5) 不要使用 for-in 遍历 你或许听到过一些其它人的观点告诉你，使用 for-in 同样可以做到遍历数组。但是 for-in 是为了遍历对象用的，它并不保证按照数组的索引顺序来一一地遍历元素。所以在遍历数组的时候不推荐使用这种做法。\nfor (key in obj) { if (obj.hasOwnProperty(key)) { // ...  } } 如何清空数组 (1) 直接赋值一个空数组 如果你确实不在使用这个数组的话，也能保证其它地方没有引用这个数组，那么完全可以通过为其赋上一个新的空数组，从而置位空。\narr = [] (2) 将长度置为 0 arr.length = 0 (3) 使用 splice 方法 arr.splice(0, arr.length) (4) 使用 pop 方法 一一将元素 pop 出去，可能是最慢的一个方法了。\nwhile (arr.length) { arr.pop(); } 从头部插入元素 (1) 使用 unshift 方法  unshift 方法从头部插入一个元素到数组中 shift 方法从头部删除一个元素 push 方法从尾部插入一个元素到数组中 pop 方法从尾部删除一个元素  JavaScript 提供的从头部删除元素的方法名叫做 unshift，而不是叫做 insertAtHead 之类的，的确是在用到的时候不太容易想到这个名称。之所以这样命名，是因为 JavaScript 的数组的命名方式参考了 C 语言的栈的命名方式:\n array_unshift() array_shift() array_push() array_pop()  (2) 使用 concat 方法 concat() 方法可以用来连接两个数组：\nvar arr = [1, 2, 3, 4, 5, 6, 7]; console.log([0].concat(arr)); (3) 使用 Spread 操作符 在 ES6 中，可以使用 Spread 操作符 \u0026hellip; 来新增元素：\nvar arr = [23, 45, 12, 67]; arr = [34, ...arr]; 提前 break 数组循环 (1) 使用 Exception 对于 forEach 而言，使用 break 是不管用的，需要抛出异常强制终止循环，最好的建议是如果你需要 break 循环，在这种情况下就不要使用 forEach 来循环数组了：\nvar BreakException = {}; try { [1, 2, 3].forEach(function(el) { console.log(el); if (el === 2) throw BreakException; }); } catch (e) { if (e !== BreakException) throw e; } (2) 使用 for-of for (const [index, el] of arr.entries()) { if ( index === 5 ) break; } (3) 使用普通的循环 var array = [1, 2, 3]; for (var i = 0; i \u0026lt; array.length; i++) { if (array[i] === 1){ break; } } 数组去重 (1) 使用 filter 方法 var myArray = [\u0026#39;a\u0026#39;, 1, \u0026#39;a\u0026#39;, 2, \u0026#39;1\u0026#39;]; var unique = myArray.filter((value, index, arr) =\u0026gt; arr.indexOf(value) === index); (2) 使用 Set 构造器 var myArray = [\u0026#39;a\u0026#39;, 1, \u0026#39;a\u0026#39;, 2, \u0026#39;1\u0026#39;]; // unique = Array.from(new Set(myArray)) let unique = [...new Set(myArray)]; 参考  How do I remove a particular element from an array in JavaScript For-each over an array in JavaScript How do I empty an array in JavaScript How can I add new array elements at the beginning of an array in Javascript? Short circuit Array.forEach like calling break Get all unique values in a JavaScript array  扫描下面二维码，在手机端阅读：\n"});index.add({'id':42,'href':'/docs/tutorial/sentinel/leaparray/','title':"LeapArray",'content':"LeapArray Sentinel 底层采用高性能的滑动窗口数据结构 LeapArray 来统计实时的秒级指标数据，可以很好地支撑写多于读的高并发场景。\n主要数据结构 protected int windowLengthInMs; protected int sampleCount; protected int intervalInMs; protected final AtomicReferenceArray\u0026lt;WindowWrap\u0026lt;T\u0026gt;\u0026gt; array;  intervalInMs 代表滑动窗口的总时间长度 sampleCount 表示要将 intervalInMs 切割成多少份，也就是滑动窗口中有多少个 bucket windowLengthInMs 表示每一份代表多长时间 array 表示的是底层的数组，即这么多份 bucket 构成的数组  根据时间戳定位某个 Bucket （1）获取对应的索引\nlong timeId = timeMillis / windowLengthInMs; // Calculate current index so we can map the timestamp to the leap array. return (int)(timeId % array.length()); （2）计算当前 Bucket 开始的时间\nlong windowStart = timeMillis - timeMillis % windowLengthInMs; （3）如果此 Bucket 不存在，创建并 CAS 更新\nWindowWrap\u0026lt;T\u0026gt; old = array.get(idx); if (old == null) { /* * B0 B1 B2 NULL B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * bucket is empty, so create new and update * * If the old bucket is absent, then we create a new bucket at {@code windowStart}, * then try to update circular array via a CAS operation. Only one thread can * succeed to update, while other threads yield its time slice. */ WindowWrap\u0026lt;T\u0026gt; window = new WindowWrap\u0026lt;T\u0026gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); if (array.compareAndSet(idx, null, window)) { // Successfully updated, return the created bucket.  return window; } } （4）如果存在，且 start 一致\nif (windowStart == old.windowStart()) { /* * B0 B1 B2 B3 B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * startTime of Bucket 3: 800, so it\u0026#39;s up-to-date * * If current {@code windowStart} is equal to the start timestamp of old bucket, * that means the time is within the bucket, so directly return the bucket. */ return old; } （5）如果存在，且原 start 已经落后\nif (windowStart \u0026gt; old.windowStart()) { /* * (old) * B0 B1 B2 NULL B4 * |_______||_______|_______|_______|_______|_______||___ * ... 1200 1400 1600 1800 2000 2200 timestamp * ^ * time=1676 * startTime of Bucket 2: 400, deprecated, should be reset * * If the start timestamp of old bucket is behind provided time, that means * the bucket is deprecated. We have to reset the bucket to current {@code windowStart}. * Note that the reset and clean-up operations are hard to be atomic, * so we need a update lock to guarantee the correctness of bucket update. * * The update lock is conditional (tiny scope) and will take effect only when * bucket is deprecated, so in most cases it won\u0026#39;t lead to performance loss. */ if (updateLock.tryLock()) { try { // Successfully get the update lock, now we reset the bucket.  return resetWindowTo(old, windowStart); } finally { updateLock.unlock(); } } } "});index.add({'id':43,'href':'/docs/tutorial/distributed-storage/leveldb/','title':"leveldb 源码分析与实现",'content':"leveldb 源码分析与实现  源代码基于 1.22 之后的版本\n 特性 leveldb 是一个键值对 library，它的键是有序排列的，用户也可以提供自定义的键比较器，多个操作也可以合并为一起，进行原子操作更新。其架构如下:\n编译 mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Release .. \u0026amp;\u0026amp; cmake --build . 如果 Ubuntu 提示 No CMAKE_CXX_COMPILER could be found :\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install build-essential 打开数据库 Options // Options to control the behavior of a database (passed to DB::Open) struct LEVELDB_EXPORT Options { // Create an Options object with default values for all fields.  Options(); // -------------------  // Parameters that affect behavior  // Comparator 用来决定 key 在 table 中的顺序.  // Default: a comparator that uses lexicographic byte-wise ordering  //  // REQUIRES: The client must ensure that the comparator supplied  // here has the same name and orders keys *exactly* the same as the  // comparator provided to previous open calls on the same DB.  const Comparator* comparator; // If true, the database will be created if it is missing.  bool create_if_missing = false; // If true, an error is raised if the database already exists.  bool error_if_exists = false; // If true, the implementation will do aggressive checking of the  // data it is processing and will stop early if it detects any  // errors. This may have unforeseen ramifications: for example, a  // corruption of one DB entry may cause a large number of entries to  // become unreadable or for the entire DB to become unopenable.  bool paranoid_checks = false; // Use the specified object to interact with the environment,  // e.g. to read/write files, schedule background work, etc.  // Default: Env::Default()  Env* env; // Any internal progress/error information generated by the db will  // be written to info_log if it is non-null, or to a file stored  // in the same directory as the DB contents if info_log is null.  Logger* info_log = nullptr; // -------------------  // Parameters that affect performance  // 存储在内存中的数据 (backed by an unsorted log  // on disk) before converting to a sorted on-disk file.  //  // 数值较大有助于提升性能 especially during bulk loads.  // Up to two write buffers may be held in memory at the same time,  // so you may wish to adjust this parameter to control memory usage.  // 不过，数值较大也可能造成在下次打开 leveldb 数据库的时候加载时间过长  size_t write_buffer_size = 4 * 1024 * 1024; // DB 最多可以打开多少文件. You may need to  // increase this if your database has a large working set (budget  // one open file per 2MB of working set).  int max_open_files = 1000; // Control over blocks (user data is stored in a set of blocks, and  // a block is the unit of reading from disk).  // If non-null, use the specified cache for blocks.  // If null, leveldb will automatically create and use an 8MB internal cache.  Cache* block_cache = nullptr; // Approximate size of user data packed per block. Note that the  // block size specified here corresponds to uncompressed data. The  // actual size of the unit read from disk may be smaller if  // compression is enabled. This parameter can be changed dynamically.  size_t block_size = 4 * 1024; // Number of keys between restart points for delta encoding of keys.  // This parameter can be changed dynamically. Most clients should  // leave this parameter alone.  int block_restart_interval = 16; // Leveldb will write up to this amount of bytes to a file before  // switching to a new one.  // Most clients should leave this parameter alone. However if your  // filesystem is more efficient with larger files, you could  // consider increasing the value. The downside will be longer  // compactions and hence longer latency/performance hiccups.  // Another reason to increase this parameter might be when you are  // initially populating a large database.  size_t max_file_size = 2 * 1024 * 1024; // Compress blocks using the specified compression algorithm. This  // parameter can be changed dynamically.  //  // Default: kSnappyCompression, which gives lightweight but fast  // compression.  //  // Typical speeds of kSnappyCompression on an Intel(R) Core(TM)2 2.4GHz:  // ~200-500MB/s compression  // ~400-800MB/s decompression  // Note that these speeds are significantly faster than most  // persistent storage speeds, and therefore it is typically never  // worth switching to kNoCompression. Even if the input data is  // incompressible, the kSnappyCompression implementation will  // efficiently detect that and will switch to uncompressed mode.  CompressionType compression = kSnappyCompression; // EXPERIMENTAL: If true, append to existing MANIFEST and log files  // when a database is opened. This can significantly speed up open.  //  // Default: currently false, but may become true later.  bool reuse_logs = false; // If non-null, use the specified filter policy to reduce disk reads.  // Many applications will benefit from passing the result of  // NewBloomFilterPolicy() here.  const FilterPolicy* filter_policy = nullptr; }; VersionEdit class VersionEdit { private: friend class VersionSet; typedef std::set\u0026lt;std::pair\u0026lt;int, uint64_t\u0026gt;\u0026gt; DeletedFileSet; std::string comparator_; uint64_t log_number_; uint64_t prev_log_number_; uint64_t next_file_number_; SequenceNumber last_sequence_; bool has_comparator_; bool has_log_number_; bool has_prev_log_number_; bool has_next_file_number_; bool has_last_sequence_; std::vector\u0026lt;std::pair\u0026lt;int, InternalKey\u0026gt;\u0026gt; compact_pointers_; DeletedFileSet deleted_files_; std::vector\u0026lt;std::pair\u0026lt;int, FileMetaData\u0026gt;\u0026gt; new_files_; } Put Slice Slice 的底层数据结构：\nclass LEVELDB_EXPORT Slice { private: const char* data_; size_t size_; }; 底层存储数据的地方 // db_impl.h MemTable* mem_; // 缓存 MemTable* imm_ GUARDED_BY(mutex_); // 数据库 写入顺序 // 先写到 Log 文件中 status = log_-\u0026gt;AddRecord(WriteBatchInternal::Contents(write_batch)); bool sync_error = false; if (status.ok() \u0026amp;\u0026amp; options.sync) { status = logfile_-\u0026gt;Sync(); if (!status.ok()) { sync_error = true; } } // 如果写到 Log 文件成功 if (status.ok()) { // 则写到数据库中  status = WriteBatchInternal::InsertInto(write_batch, mem_); } 如下图所示，数据先顺序写入到位于磁盘上的 log 文件中，如果写入成功，则再写入到 MemTable 中:\n写 Log 日志文件 日志存储在 WriteBatch 的 std::string rep_ 里面，其存储的内容如下代码所示：\nvoid WriteBatch::Put(const Slice\u0026amp; key, const Slice\u0026amp; value) { // ...  rep_.push_back(static_cast\u0026lt;char\u0026gt;(kTypeValue)); PutLengthPrefixedSlice(\u0026amp;rep_, key); PutLengthPrefixedSlice(\u0026amp;rep_, value); } // coding.cc void PutLengthPrefixedSlice(std::string* dst, const Slice\u0026amp; value) { PutVarint32(dst, value.size()); dst-\u0026gt;append(value.data(), value.size()); } 写入 Log 日志的时候，对记录的类型和数据计算 CRC 编码，这个编码作为 buf 数组的前 4 位，buf 数组的后 3 位依次填充上:\nbuf[4] = static_cast\u0026lt;char\u0026gt;(length \u0026amp; 0xff); buf[5] = static_cast\u0026lt;char\u0026gt;(length \u0026gt;\u0026gt; 8); buf[6] = static_cast\u0026lt;char\u0026gt;(t); buf 数组的内容就是这条记录的头的内容，ptr 指向的是这条记录的实际数据:\n// log_writer.cc // Write the header and the payload Status s = dest_-\u0026gt;Append(Slice(buf, kHeaderSize)); if (s.ok()) { s = dest_-\u0026gt;Append(Slice(ptr, length)); if (s.ok()) { s = dest_-\u0026gt;Flush(); } } 头的长度为 7 位:\n// log_format.h // Header is checksum (4 bytes), length (2 bytes), type (1 byte). static const int kHeaderSize = 4 + 2 + 1; Append 内部调用了 std:memcpy 函数。\n写到 MemTable 内存数据库中 void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value) { // 使用 Arena 分配内存  char* buf = arena_.Allocate(encoded_len); char* p = EncodeVarint32(buf, internal_key_size); // 将 key 拷贝到 buf 中  std::memcpy(p, key.data(), key_size); // 将 value 拷贝到 buf 中  std::memcpy(p, value.data(), val_size); // 插入到 table 中  table_.Insert(buf); } 如下是 std::memcpy 的签名:\n// Copies count bytes from the object pointed to by src to the object pointed to by dest. void* memcpy( void* dest, const void* src, std::size_t count ); 上述代码最后一行出现的 table_.Insert(buf)，此处的 Table 其实就是跳表:\n// memtable.h typedef SkipList\u0026lt;const char*, KeyComparator\u0026gt; Table; 因此在 MemTable 中分配的 buf 最终将会存储到跳表中。\nDelete Delete 的底层很像 Put 操作:\n// write_batch.cc void Delete(const Slice\u0026amp; key) override { mem_-\u0026gt;Add(sequence_, kTypeDeletion, key, Slice()); sequence_++; } 只是 key 的被标记为 kTypeDeletion 标签了。\nSnapshot 快照 创建快照 从 VersionSet 中获取最后一个序号 LastSequence，然后根据这最后一个序号创建一个快照:\n// db_impl.cc const Snapshot* DBImpl::GetSnapshot() { MutexLock l(\u0026amp;mutex_); return snapshots_.New(versions_-\u0026gt;LastSequence()); } 其中 versions_ 的定义:\nVersionSet* const versions_ GUARDED_BY(mutex_); 数据库文件 数据库所有文件 数据库名字命名的文件夹的目录，包含的文件有如下几种类型:\n// filename.h enum FileType { kLogFile, // 000025.log  kDBLockFile, // LOCK  kTableFile, // xxxxxx.ldb, *.sst  kDescriptorFile, // MANIFEST-000023  kCurrentFile, // CURRENT  kTempFile, // *.dbtmp  kInfoLogFile // LOG, LOG.old }; 下面展示的是一个示例目录结构:\nLOG, LOG.old 文件内容示例:\n2020/10/25-15:37:29.918667 140486263023424 Recovering log #25 2020/10/25-15:37:29.918906 140486263023424 Level-0 table #27: started 2020/10/25-15:37:29.923419 140486263023424 Level-0 table #27: 133 bytes OK 2020/10/25-15:37:29.930591 140486263023424 Delete type=0 #25 2020/10/25-15:37:29.930639 140486263023424 Delete type=3 #23 2020/10/25-15:37:29.930944 140486263019264 Compacting 4@0 + 1@1 files 2020/10/25-15:37:29.931234 140486263019264 compacted to: files[ 4 1 0 0 0 0 0 ] 文件内容是通过 Log 方法写入进去的:\nLog(options_.info_log, \u0026#34;compacted to: %s\u0026#34;, versions_-\u0026gt;LevelSummary(\u0026amp;tmp)); *.log 位于 db_impl.h 中的 logfile_ 和 log_ 指向的文件就是 log 文件:\n// db_impl.h WritableFile* logfile_; log::Writer* log_; log 文件中的内容是是通过 AddRecord 写进去的:\nstatus = log_-\u0026gt;AddRecord(WriteBatchInternal::Contents(write_batch)); 如下是用 Vim 打开的某个 log 文件所看到的内容:\nLOCK // Lock over the persistent DB state. Non-null iff successfully acquired. FileLock* db_lock_; Compaction 当内存中的 MemTable 达到一定大小的时候，Compaction 可以将其内容保持到磁盘中。\n版本管理 环形双端链表 AppendVersion 的过程:\n[prev] \u0026lt;-\u0026gt; [dummy] 转为:\n[prev] \u0026lt;-\u0026gt; [current] \u0026lt;-\u0026gt; [dummy] 参考  impl 数据分析与处理之二（Leveldb 实现原理）  "});index.add({'id':44,'href':'/docs/tutorial/awk/patterns/','title':"Patterns",'content':"Patterns Patterns 控制是否执行 actions，只有当 pattern 匹配的时候，才会执行 action。\n本文介绍六种常用的 pattern：\n BEGIN { statements }：所有行处理之前执行一次 BEGIN END { statements }：所有行处理完了执行一次 END expression { statements }：普通的表达式 /正则表达式/ { statements }：匹配正则 组合表达式 { statements }：使用 \u0026amp;\u0026amp; 或 || 或 ! 进行组合 pattern1, pattern2 { statements }：范围匹配  BEGIN 和 END BEGIN 和 END 只会执行一次，BEGIN 是在开始执行前执行，END 是在结束前执行。\n一种常见的使用 BEGIN 的用法是改变默认的列分割符，列分割符默认被一个内置变量 FS 所控制，这个变量的默认值是空格或者tabs。如下示例在 BEGIN 中设置了 FS 为 \\t，同时打印了表头。在 END 块中打印了面积和人口的总和。\nBEGIN { FS = \u0026#34;\\t\u0026#34; printf(\u0026#34;%10s %6s %5s %s\\n\\n\u0026#34;, \u0026#34;COUNTRY\u0026#34;, \u0026#34;AREA\u0026#34;, \u0026#34;POP\u0026#34;, \u0026#34;CONTINENT\u0026#34;) } { printf(\u0026#34;%10s %6d %5d %s\\n\u0026#34;, $1, $2, $3, $4) area = area + $2 pop = pop + $3 } END { printf(\u0026#34;\\n%10s %6d %5d\\n\u0026#34;, \u0026#34;TOTAL\u0026#34;, area, pop) } expression 表达式示例：\n$3 / $2 \u0026gt;= 0.5 $0 \u0026gt;= \u0026#34;M\u0026#34; $1 \u0026lt; $4 可用的比较符号：\n ~ 和 !~ 是专门用于比较字符串的符号\n 如果表达式返回的是一个布尔值，那么根据此布尔值来决定是否匹配这一行；如果表达式返回的是一个数字，那么数字如果是非 0 数字，会匹配这一行；如果表达式返回的是一个字符串，那么非空字符串，会匹配这一行。\n当比较操作符的两个比较对象，均为数字，那么使用数字的比较原则进行大小比较；否则任何数字将会被转为字符串，使用字符串的方式按照 ASCII 顺序进行比较。\n"});index.add({'id':45,'href':'/docs/rocketmq/rocketmq-message-store-flow/','title':"RocketMQ 消息存储流程",'content':"RocketMQ 消息存储流程 本文讲述 RocketMQ 存储一条消息的流程。\n一、存储位置 当有一条消息过来之后，Broker 首先需要做的是确定这条消息应该存储在哪个文件里面。在 RocketMQ 中，这个用来存储消息的文件被称之为 MappedFile。这个文件默认创建的大小为 1GB。\n一个文件为 1GB 大小，也即 1024 * 1024 * 1024 = 1073741824 字节，这每个文件的命名是按照总的字节偏移量来命名的。例如第一个文件偏移量为 0，那么它的名字为 00000000000000000000；当当前这 1G 文件被存储满了之后，就会创建下一个文件，下一个文件的偏移量则为 1GB，那么它的名字为 00000000001073741824，以此类推。\n默认情况下这些消息文件位于 $HOME/store/commitlog 目录下，如下图所示:\n二、文件创建 当 Broker 启动的时候，其会将位于存储目录下的所有消息文件加载到一个列表中:\n当有新的消息到来的时候，其会默认选择列表中的最后一个文件来进行消息的保存:\npublic class MappedFileQueue { public MappedFile getLastMappedFile() { MappedFile mappedFileLast = null; while (!this.mappedFiles.isEmpty()) { try { mappedFileLast = this.mappedFiles.get(this.mappedFiles.size() - 1); break; } catch (IndexOutOfBoundsException e) { //continue;  } catch (Exception e) { log.error(\u0026#34;getLastMappedFile has exception.\u0026#34;, e); break; } } return mappedFileLast; } } 当然如果这个 Broker 之前从未接受过消息的话，那么这个列表肯定是空的。这样一旦有新的消息需要存储的时候，其就得需要立即创建一个 MappedFile 文件来存储消息。\nRocketMQ 提供了一个专门用来实例化 MappedFile 文件的服务类 AllocateMappedFileService。在内存中，也同时维护了一张请求表 requestTable 和一个优先级请求队列 requestQueue 。当需要创建文件的时候，Broker 会创建一个 AllocateRequest 对象，其包含了文件的路径、大小等信息。然后先将其放入 requestTable 表中，再将其放入优先级请求队列 requestQueue 中:\npublic class AllocateMappedFileService extends ServiceThread { public MappedFile putRequestAndReturnMappedFile(String nextFilePath, String nextNextFilePath, int fileSize) { // ...  AllocateRequest nextReq = new AllocateRequest(nextFilePath, fileSize); boolean nextPutOK = this.requestTable.putIfAbsent(nextFilePath, nextReq) == null; if (nextPutOK) { // ...  boolean offerOK = this.requestQueue.offer(nextReq); } } } 服务类会一直等待优先级队列是否有新的请求到来，如果有，便会从队列中取出请求，然后创建对应的 MappedFile，并将请求表 requestTable 中 AllocateRequest 对象的字段 mappedFile 设置上值。最后将 AllocateRequest 对象上的 CountDownLatch 的计数器减 1 ，以标明此分配申请的 MappedFile 已经创建完毕了:\npublic class AllocateMappedFileService extends ServiceThread { public void run() { // 一直运行  while (!this.isStopped() \u0026amp;\u0026amp; this.mmapOperation()) { } } private boolean mmapOperation() { req = this.requestQueue.take(); if (req.getMappedFile() == null) { MappedFile mappedFile; // ...  mappedFile = new MappedFile(req.getFilePath(), req.getFileSize()); // 设置上值  req.setMappedFile(mappedFile); } // ...  // 计数器减 1  req.getCountDownLatch().countDown(); // ...  return true; } } 其上述整体流程如下所示:\n等待 MappedFile 创建完毕之后，其便会从请求表 requestTable 中取出并删除表中记录:\npublic class AllocateMappedFileService extends ServiceThread { public MappedFile putRequestAndReturnMappedFile(String nextFilePath, String nextNextFilePath, int fileSize) { // ...  AllocateRequest result = this.requestTable.get(nextFilePath); if (result != null) { // 等待 MappedFile 的创建完成  boolean waitOK = result.getCountDownLatch().await(waitTimeOut, TimeUnit.MILLISECONDS); if (!waitOK) { return null; } else { // 从请求表中删除  this.requestTable.remove(nextFilePath); return result.getMappedFile(); } } } } 然后再将其放到列表中去:\npublic class MappedFileQueue { public MappedFile getLastMappedFile(final long startOffset, boolean needCreate) { MappedFile mappedFile = null; if (this.allocateMappedFileService != null) { // 创建 MappedFile  mappedFile = this.allocateMappedFileService .putRequestAndReturnMappedFile(nextFilePath, nextNextFilePath, this.mappedFileSize); } if (mappedFile != null) { // ...  // 添加至列表中  this.mappedFiles.add(mappedFile); } return mappedFile; } } 至此，MappedFile 已经创建完毕，也即可以进行下一步的操作了。\n三、文件初始化 在 MappedFile 的构造函数中，其使用了 FileChannel 类提供的 map 函数来将磁盘上的这个文件映射到进程地址空间中。然后当通过 MappedByteBuffer 来读入或者写入文件的时候，磁盘上也会有相应的改动。采用这种方式，通常比传统的基于文件 IO 流的方式读取效率高。\npublic class MappedFile extends ReferenceResource { public MappedFile(final String fileName, final int fileSize) throws IOException { init(fileName, fileSize); } private void init(final String fileName, final int fileSize) throws IOException { // ...  this.fileChannel = new RandomAccessFile(this.file, \u0026#34;rw\u0026#34;).getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); // ...  } } 四、消息文件加载 前面提到过，Broker 在启动的时候，会加载磁盘上的文件到一个 mappedFiles 列表中。但是加载完毕后，其还会对这份列表中的消息文件进行验证 (恢复)，确保没有错误。\n验证的基本想法是通过一一读取列表中的每一个文件，然后再一一读取每个文件中的每个消息，在读取的过程中，其会更新整体的消息写入的偏移量，如下图中的红色箭头 (我们假设最终读取的消息的总偏移量为 905):\n当确定消息整体的偏移量之后，Broker 便会确定每一个单独的 MappedFile 文件的各自的偏移量，每一个文件的偏移量是通过取余算法确定的:\npublic class MappedFileQueue { public void truncateDirtyFiles(long offset) { for (MappedFile file : this.mappedFiles) { long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; if (fileTailOffset \u0026gt; offset) { if (offset \u0026gt;= file.getFileFromOffset()) { // 确定每个文件的各自偏移量  file.setWrotePosition((int) (offset % this.mappedFileSize)); file.setCommittedPosition((int) (offset % this.mappedFileSize)); file.setFlushedPosition((int) (offset % this.mappedFileSize)); } else { // ...  } } } // ...  } } 在确定每个消息文件各自的写入位置的同时，其还会删除起始偏移量大于当前总偏移量的消息文件，这些文件可以视作脏文件，或者也可以说这些文件里面一条消息也没有。这也是上述文件 1073741824 被打上红叉的原因:\npublic void truncateDirtyFiles(long offset) { List\u0026lt;MappedFile\u0026gt; willRemoveFiles = new ArrayList\u0026lt;MappedFile\u0026gt;(); for (MappedFile file : this.mappedFiles) { long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; if (fileTailOffset \u0026gt; offset) { if (offset \u0026gt;= file.getFileFromOffset()) { // ...  } else { // 总偏移量 \u0026lt; 文件起始偏移量  // 加入到待删除列表中  file.destroy(1000); willRemoveFiles.add(file); } } } this.deleteExpiredFile(willRemoveFiles); } 五、写入消息 一旦我们获取到 MappedFile 文件之后，我们便可以往这个文件里面写入消息了。写入消息可能会遇见如下两种情况，一种是这条消息可以完全追加到这个文件中，另外一种是这条消息完全不能或者只有一小部分只能存放到这个文件中，其余的需要放到新的文件中。我们对于这两种情况分别讨论:\n(1) 文件可以完全存储消息 MappedFile 类维护了一个用以标识当前写位置的指针 wrotePosition，以及一个用来映射文件到进程地址空间的 mappedByteBuffer:\npublic class MappedFile extends ReferenceResource { protected final AtomicInteger wrotePosition = new AtomicInteger(0); private MappedByteBuffer mappedByteBuffer; } 由这两个数据结构我们可以看出来，单个文件的消息写入过程其实是非常简单的。首先获取到这个文件的写入位置，然后将消息内容追加到 byteBuffer 中，然后再更新写入位置。\npublic class MappedFile extends ReferenceResource { public AppendMessageResult appendMessagesInner(final MessageExt messageExt, final AppendMessageCallback cb) { // ...  int currentPos = this.wrotePosition.get(); if (currentPos \u0026lt; this.fileSize) { ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice(); // 更新 byteBuffer 位置  byteBuffer.position(currentPos); // 写入消息内容  // ...  // 更新 wrotePosition 指针的位置  this.wrotePosition.addAndGet(result.getWroteBytes()); return result; } } } 示例流程如下所示:\n(2) 文件不可以完全存储消息 在写入消息之前，如果判断出文件已经满了的情况下，其会直接尝试创建一个新的 MappedFile:\npublic class CommitLog { public PutMessageResult putMessage(final MessageExtBrokerInner msg) { // 文件为空 || 文件已经满了  if (null == mappedFile || mappedFile.isFull()) { mappedFile = this.mappedFileQueue.getLastMappedFile(0); } // ...  result = mappedFile.appendMessage(msg, this.appendMessageCallback); } } 如果文件未满，那么在写入之前会先计算出消息体长度 msgLen，然后判断这个文件剩下的空间是否有能力容纳这条消息。在这个地方我们还需要介绍下每条消息的存储方式。\n每条消息的存储是按照一个 4 字节的长度来做界限的，这个长度本身就是整个消息体的长度，当读完这整条消息体的长度之后，下一次再取出来的一个 4 字节的数字，便又是下一条消息的长度:\n围绕着一条消息，还会存储许多其它内容，我们在这里只需要了解前两位是 4 字节的长度和 4 字节的 MAGICCODE 即可:\nMAGICCODE 的可选值有:\n CommitLog.MESSAGE_MAGIC_CODE CommitLog.BLANK_MAGIC_CODE  当这个文件有能力容纳这条消息体的情况下，其便会存储 MESSAGE_MAGIC_CODE 值；当这个文件没有能力容纳这条消息体的情况下，其便会存储 BLANK_MAGIC_CODE 值。所以这个 MAGICCODE 是用来界定这是空消息还是一条正常的消息。\n当判定这个文件不足以容纳整个消息的时候，其将消息体长度设置为这个文件剩余的最大空间长度，将 MAGICCODE 设定为这是一个空消息文件 (需要去下一个文件去读)。由此我们可以看出消息体长度 和 MAGICCODE 是判别一条消息格式的最基本要求，这也是 END_FILE_MIN_BLANK_LENGTH 的值为 8 的原因:\n// CommitLog.java class DefaultAppendMessageCallback implements AppendMessageCallback { // File at the end of the minimum fixed length empty  private static final int END_FILE_MIN_BLANK_LENGTH = 4 + 4; public AppendMessageResult doAppend(final long fileFromOffset, final ByteBuffer byteBuffer, final int maxBlank, final MessageExtBrokerInner msgInner) { // ...  if ((msgLen + END_FILE_MIN_BLANK_LENGTH) \u0026gt; maxBlank) { // ...  // 1 TOTALSIZE  this.msgStoreItemMemory.putInt(maxBlank); // 2 MAGICCODE  this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE); // 3 The remaining space may be any value  byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank); return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, /** other params **/ ); } } } 由上述方法我们看出在这种情况下返回的结果是 END_OF_FILE。当检测到这种返回结果的时候，CommitLog 接着又会申请创建新的 MappedFile 并尝试写入消息。追加方法同 (1) 相同，不再赘述:\n 注: 在消息文件加载的过程中，其也是通过判断 MAGICCODE 的类型，来判断是否继续读取下一个 MappedFile 来计算整体消息偏移量的。\n 六、消息刷盘策略 当消息体追加到 MappedFile 以后，这条消息实际上还只是存储在内存中，因此还需要将内存中的内容刷到磁盘上才算真正的存储下来，才能确保消息不丢失。一般而言，刷盘有两种策略: 异步刷盘和同步刷盘。\n(1) 异步刷盘 当配置为异步刷盘策略的时候，Broker 会运行一个服务 FlushRealTimeService 用来刷新缓冲区的消息内容到磁盘，这个服务使用一个独立的线程来做刷盘这件事情，默认情况下每隔 500ms 来检查一次是否需要刷盘:\nclass FlushRealTimeService extends FlushCommitLogService { public void run() { // 不停运行  while (!this.isStopped()) { // interval 默认值是 500ms  if (flushCommitLogTimed) { Thread.sleep(interval); } else { this.waitForRunning(interval); } // 刷盘  CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages); } } } 在追加消息完毕之后，通过唤醒这个服务立即检查以下是否需要刷盘:\npublic class CommitLog { public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { // Synchronization flush  if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) { // ...  } // Asynchronous flush  else { if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) { // 消息追加成功后，立即唤醒服务  flushCommitLogService.wakeup(); } else { // ...  } } } } (2) 同步刷盘 当配置为同步刷盘策略的时候，Broker 运行一个叫做 GroupCommitService 服务。在这个服务内部维护了一个写请求队列和一个读请求队列，其中这两个队列每隔 10ms 就交换一下“身份”，这么做的目的其实也是为了读写分离:\n在这个服务内部，每隔 10ms 就会检查读请求队列是否不为空，如果不为空，则会将读队列中的所有请求执行刷盘，并清空读请求队列:\nclass GroupCommitService extends FlushCommitLogService { private void doCommit() { // 检查所有读队列中的请求  for (GroupCommitRequest req : this.requestsRead) { // 每个请求执行刷盘  CommitLog.this.mappedFileQueue.flush(0); req.wakeupCustomer(flushOK); } this.requestsRead.clear(); } } 在追加消息完毕之后，通过创建一个请求刷盘的对象，然后通过 putRequest() 方法放入写请求队列中，这个时候会立即唤醒这个服务，写队列和读队列的角色会进行交换，交换角色之后，读请求队列就不为空，继而可以执行所有刷盘请求了。而在这期间，Broker 会一直阻塞等待最多 5 秒钟，在这期间如果完不成刷盘请求的话，那么视作刷盘超时:\npublic class CommitLog { public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { // Synchronization flush  if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) { // ...  if (messageExt.isWaitStoreMsgOK()) { GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes()); service.putRequest(request); // 等待刷盘成功  boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout()); if (!flushOK) { // 刷盘超时  putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT); } } else { // ...  } } // Asynchronous flush  else { // ...  } } } 通过方法 putRequest 放入请求后的服务执行流程:\n七、消息刷盘理念 我们在这里已经知道消息刷盘有同步刷盘和异步刷盘策略，对应的是 GroupCommitService 和 FlushRealTimeService 这两种不同的服务。\n这两种服务都有定时请求刷盘的机制，但是机制背后最终调用的刷盘方式全部都集中在 flush 这个方法上:\npublic class MappedFileQueue { public boolean flush(final int flushLeastPages) { // ...  } } 再继续向下分析这个方法之前，我们先对照着这张图说明一下使用 MappedByteBuffer 来简要阐述读和写文件的简单过程：\n操作系统为了能够使多个进程同时使用内存，又保证各个进程访问内存互相独立，于是为每个进程引入了地址空间的概念，地址空间上的地址叫做虚拟地址，而程序想要运行必须放到物理地址上运行才可以。地址空间为进程营造出了一种假象：”整台计算机只有我一个程序在运行，这台计算机内存很大”。一个地址空间内包含着这个进程所需要的全部状态信息。通常一个进程的地址空间会按照逻辑分成好多段，比如代码段、堆段、栈段等。为了进一步有效利用内存，每一段又细分成了不同的页 (page)。与此相对应，计算机的物理内存被切成了页帧 (page frame)，文件被分成了块 (block)。既然程序实际运行的时候还是得依赖物理内存的地址，那么就需要将虚拟地址转换为物理地址，这个映射关系是由**页表 (page table)**来完成的。\n另外在操作系统中，还有一层磁盘缓存 (disk cache)的概念，它主要是用来减少对磁盘的 I/O 操作。磁盘缓存是以页为单位的，内容就是磁盘上的物理块，所以又称之为页缓存 (page cache)。当进程发起一个读操作 （比如，进程发起一个 read() 系统调用），它首先会检查需要的数据是否在页缓存中。如果在，则放弃访问磁盘，而直接从页缓存中读取。如果数据没在缓存中，那么内核必须调度块 I/O 操作从磁盘去读取数据，然后将读来的数据放入页缓存中。系统并不一定要将整个文件都缓存，它可以只存储一个文件的一页或者几页。\n如图所示，当调用 FileChannel.map() 方法的时候，会将这个文件映射进用户空间的地址空间中，注意，建立映射不会拷贝任何数据。我们前面提到过 Broker 启动的时候会有一个消息文件加载的过程，当第一次开始读取数据的时候:\n// 首次读取数据 int totalSize = byteBuffer.getInt(); 这个时候，操作系统通过查询页表，会发现文件的这部分数据还不在内存中。于是就会触发一个缺页异常 (page faults)，这个时候操作系统会开始从磁盘读取这一页数据，然后先放入到页缓存中，然后再放入内存中。在第一次读取文件的时候，操作系统会读入所请求的页面，并读入紧随其后的少数几个页面（不少于一个页面，通常是三个页面），这时的预读称为同步预读 (如下图所示，红色部分是需要读取的页面，蓝色的那三个框是操作系统预先读取的):\n当然随着时间推移，预读命中的话，那么相应的预读页面数量也会增加，但是能够确认的是，一个文件至少有 4 个页面处在页缓存中。当文件一直处于顺序读取的情况下，那么基本上可以保证每次预读命中:\n下面我们来说文件写，正常情况下，当尝试调用 writeInt() 写数据到文件里面的话，其写到页缓存层，这个方法就会返回了。这个时候数据还没有真正的保存到文件中去，Linux 仅仅将页缓存中的这一页数据标记为“脏”，并且被加入到脏页链表中。然后由一群进程（flusher 回写进程）周期性将脏页链表中的页写会到磁盘，从而让磁盘中的数据和内存中保持一致，最后清理“脏”标识。在以下三种情况下，脏页会被写回磁盘:\n 空闲内存低于一个特定阈值 脏页在内存中驻留超过一个特定的阈值时 当用户进程调用 sync() 和 fsync() 系统调用时  可见，在正常情况下，即使不采用刷盘策略，数据最终也是会被同步到磁盘中去的:\n但是，即便有 flusher 线程来定时同步数据，如果此时机器断电的话，消息依然有可能丢失。RocketMQ 为了保证消息尽可能的不丢失，为了最大的高可靠性，做了同步和异步刷盘策略，来手动进行同步:\n八、消息刷盘过程 在介绍完上述消息刷盘背后的一些机制和理念后，我们再来分析刷盘整个过程。首先，无论同步刷盘还是异步刷盘，其线程都在一直周期性的尝试执行刷盘，在真正执行刷盘函数的调用之前，Broker 会检查文件的写位置是否大于 flush 位置，避免执行无意义的刷盘：\n其次，对于异步刷盘来讲，Broker 执行了更为严格的刷盘限制策略，当在某个时间点尝试执行刷盘之后，在接下来 10 秒内，如果想要继续刷盘，那么脏页面数量必须不小于 4 页，如下图所示:\n下面是执行刷盘前最后检查的刷盘条件：\npublic class MappedFile extends ReferenceResource { private boolean isAbleToFlush(final int flushLeastPages) { int flush = this.flushedPosition.get(); int write = getReadPosition(); if (this.isFull()) { return true; } if (flushLeastPages \u0026gt; 0) { // 计算当前脏页面算法  return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) \u0026gt;= flushLeastPages; } // wrotePosition \u0026gt; flushedPosition  return write \u0026gt; flush; } } 当刷盘完毕之后，首先会更新这个文件的 flush 位置，然后再更新 MappedFileQueue 的整体的 flush 位置:\n当刷盘完毕之后，便会将结果通知给客户端，告知发送消息成功。至此，整个存储过程完毕。\n扫描下面二维码，在手机端阅读：\n"});index.add({'id':46,'href':'/docs/it-zone/2020-06/rust-enter-top-20/','title':"Rust 语言首次进入 Tiobe 前 20 名",'content':"Rust 语言首次进入 Tiobe 前 20 名 日期：2020-06-02\n Rust 在 Tiobe 上的排名已大大提高，从去年的38位上升到今天的20位。Tiobe 的索引基于主要搜索引擎上对某种语言的搜索，因此这并不意味着更多的人正在使用 Rust，但是它表明更多的开发人员正在搜索有关该语言的信息。\n在 Stack Overflow 的2020年调查中， Rust 被开发人员连续第五年票选为最受欢迎的编程语言。今年，有86％的开发人员表示他们热衷于使用Rust，但只有5％的开发人员实际将其用于编程。\n另一方面，由于Microsoft已公开预览其针对Windows运行时（WinRT）的 Rust 库，因此它可能会得到更广泛的使用 ，这使开发人员可以更轻松地在Rust中编写Windows，跨平台应用程序和驱动程序。\nTiobe软件首席执行官Paul Jansen说，Rust的崛起是因为它是一种“正确的”系统编程语言。\n然而，Rust 项目的2020年开发人员调查发现，由于其陡峭的学习曲线以及很少有公司使用它，用户难以采用该语言。 Google 在新的Fuchsia OS 排除了 Rust 语言，因为很少有开发人员对此感到熟悉。\n"});index.add({'id':47,'href':'/docs/tutorial/eureka/demo/','title':"Spring Cloud Eureka 示例",'content':"Spring Cloud Eureka 示例 搭建服务注册中心 假设你使用的是 Maven，pom.xml 文件内容如下所示：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.2.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;service-registration-and-discovery-service\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;service-registration-and-discovery-service\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;spring-cloud.version\u0026gt;Hoxton.SR1\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;/project\u0026gt; 启动一个服务注册中心 @EnableEurekaServer @SpringBootApplication public class ServiceRegistrationAndDiscoveryServiceApplication { public static void main(String[] args) { SpringApplication.run(ServiceRegistrationAndDiscoveryServiceApplication.class, args); } } 默认会将自己作为客户端注册到注册中心，通过在 application.properties 中添加如下配置，禁止默认注册行为：\neureka.client.register-with-eureka=false\r服务提供者 用另外一个项目，引入 eureka-client 服务的依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后改造接口，引入 DiscoveryClient 类：\n@RestController class ServiceInstanceRestController { @Autowired private DiscoveryClient discoveryClient; @RequestMapping(\u0026#34;/service-instances/{applicationName}\u0026#34;) public List\u0026lt;ServiceInstance\u0026gt; serviceInstancesByApplicationName( @PathVariable String applicationName) { return this.discoveryClient.getInstances(applicationName); } } 然后启动类上加上 @EnableDiscoveryClient 注解：\n@EnableDiscoveryClient @SpringBootApplication public class ServiceRegistrationAndDiscoveryClientApplication { public static void main(String[] args) { SpringApplication.run(ServiceRegistrationAndDiscoveryClientApplication.class, args); } } 高可用部署 服务消费者 我们使用 PostMan 来验证即可。\n"});index.add({'id':48,'href':'/docs/tutorial/unix-command/','title':"UNIX 常用命令大全",'content':"UNIX 常用命令大全  grep find ls ssh top cat          如何知道是 Ubuntu 还是 Cent OS    "});index.add({'id':49,'href':'/docs/tutorial/unix-optimize/','title':"UNIX 性能优化",'content':"UNIX 性能优化 "});index.add({'id':50,'href':'/docs/tutorial/vue3/','title':"Vue.js 3.0 源码分析",'content':"Vue.js 3.0 源码分析 "});index.add({'id':51,'href':'/docs/books/history_of_quantum_physics/','title':"上帝掷骰子吗",'content':"上帝掷骰子吗-量子物理史话 1887年德国，赫兹在实验室证实了电磁波的存在，也证实了光其实是电磁波的一种，两者具有共同的波的特性，古老的光学终于可以被完全包容于新兴的电磁学里面。1901年，赫兹死后的第 7 年，无线电报已经可以穿越大西洋，实现两地的实时通讯了。\n赫兹铜环接收器的缺口之间不停地爆发着电火花，明白无误地昭示着电磁波的存在。但偶然间，赫兹又发现了一个奇怪的现象：当有光照射到这个缺口上的时候，似乎火花就出现得更容易一些。\n 量子就是能量的最小单位，就是能量里的一美分。一切能量的传输，都只能以这个量为单位来进行，它可以传输一个量子，两个量子，任意整数个量子，但却不能传输1 又1/2 个量子，那个状态是不允许的，就像你不能用现钱支付1 又1/2 美分一样。这个值，现在已经成为了自然科学中最为 重要的常数之一，以它的发现者命名，称为“普朗克常数”，用 h 来表示。\n在后来十几年的时间里，普朗克一直认为量子的假设并不是一个物理真实，而纯粹是一个为了方便而引入的假设而已。他不断地告诫人们，在引用普朗克常数 h 的时候，要尽量小心谨慎，不到万不得已千万不要胡思乱想。\n"});index.add({'id':52,'href':'/docs/tutorial/git/create-repository/','title':"创建 Git 仓库",'content':"创建 Git 仓库 （1）已有项目使用 Git 管理\n假设你的项目所在文件夹叫做：abc_project\ncd abc_project git init （2）新建项目直接使用 Git 管理\n假设新建的项目名为 xxx_project\ngit init xxx_project "});index.add({'id':53,'href':'/docs/cloud-plus-bbs/suikang-mini-program-design/','title':"穗康小程序口罩预约前后端架构及产品设计",'content':"穗康小程序口罩预约前后端架构及产品设计 在战“疫”期间，腾讯与广州市政府合作，在小程序“穗康”上，2天内上线了全国首款口罩预约功能，上线首日访问量1.7亿，累计参与口罩预约人次1400万+。那么，它是如何在2天内开发上线，扛住了超大并发量呢？其背后的前后端架构是怎样的？\n无损服务设计 整个流程下来需要 3 个实时接口：\n 药店当前口罩的库存情况 哪个时间段有名额 提交预约实时返回结果  有损服务设计 结果，口罩预约关注度远超预期：\n下面展示的 UI 的设计：\n为什么有 “损” 平衡的理论就是 CAP 理论、BASE 最终一致性：\n牺牲强一致换取高可用 两个机房需要同步，并发性差。以下是优化后的代码，引入计时器：\n降低了专线依赖。\n怎么 \u0026ldquo;损\u0026rdquo;  放弃绝对一致，追求高可用和快速响应 万有一失，用户重试 伸缩调度，降级服务  （1）穗康小程序 引入消息队列，最终一致：\n（2）QQ 相册负载高 选择扩容？带宽和存储成本高。\n（3）转账 用户重试极少量消息。再想一下微信的红色感叹号，点一下重新发送。\n（4）穗康的预约重试 （5）QQ 相册降级 "});index.add({'id':54,'href':'/docs/hire/','title':"👉招聘",'content':"👉招聘 收录全网最新互联网公司校园招聘、社会招聘！每日更新！更多招聘、咨询等事宜请联系 igozhaokun@163.com 👍👍👍\n私企 阿里巴巴  阿里巴巴2021届毕业生招聘正式启动！  腾讯  【2020.08.06 - 2020.09.16】腾讯2021校园招聘全球启动 微信事业群2021校园招聘正式启动！  百度  【2020.10.09 截止】百度2021校园招聘正式启动  华为  华为2021届应届生招聘启动  字节跳动  【2020.10.31 截止】字节跳动2021校园招聘正式启动！  美团点评  【2020.10.16 截止】美团2021届秋季校园招聘全面启动  滴滴  【2020.08.03 - 2020.09.20】滴滴2021届校招正式开启！快来加入旅程~  快手  快手2021校园招聘启动  京东  【09.15 截止】京东2021校园招聘燃力开启！  网易  【2020.08.05 - 2020.09.07】2021届网易互联网校招全面开启！  小米  【12月底截止】小米集团2021全球校园招聘全面开启！  拼多多  【08.30 截止】拼多多2021届校招提前批正式启动！  哔哩哔哩  【11月中旬截止】哔哩哔哩2021秋季校园招聘正式启动！  搜狐  【09.30 截止】搜狐集团2021校园招聘正式启动  搜狗  搜狗2021校园招聘网申全面开放！  新浪 \u0026amp; 微博  【09.09 截止】新浪\u0026amp;微博2021届校园招聘正式启动！  外企 亚马逊  亚马逊中国2021校园招聘正式启动！  微软  【08.12 - 09.16】微软2021校园招聘正式启动  FreeWheel  Software Engineer - 2021 Campus-北京-00763  🏦银行 中国农业银行研发中心  中国农业银行研发中心2021年校园招聘启事 中国农业银行研发中心2020年社会招聘启事  中国交通银行  交行总行金融科技板块2021校园招聘  更多公司 "});index.add({'id':55,'href':'/posts/consistency-problem-of-the-distrubuted-system/','title':"分布式系统一致性问题",'content':"描述解决分布式系统一致性问题的典型思路!\n一致性问题 思考下面几个分布式系统中可能存在的一致性问题:\n (1) 先下订单还是先扣库存?下订单成功扣库存失败则超卖;下订单失败扣库存成功则多卖。 (2) 系统 A 同步调用系统 B 服务超时后，这个时候系统 A 应该做什么? (3) 系统 B 异步回调系统 A 超时后，系统 A 迟迟没有收到回调结果怎么办? (4) 某个订单在系统 A 中能查询到，但是系统 B 中不存在。 (5) 系统间都存在请求，只是状态不一致。 (6) 交易系统依赖于数据库的 ACID，缓存和数据库之间如何保持一致性?强一致性还是弱一致性? (7) 多个节点上缓存的内容不一致怎么办?请求恰好在这个时间窗口进来了。 (8) 缓存数据结构不一致。某个数据由多个数据元素组成，如果其中某个子数据依赖于从其它服务中获取数据，假设这部分数据获取失败，那么就会导致数据不完整，可能会出现 NullPointerException 等。  酸碱平衡理论 ACID 具有 ACID 特性的数据库支持强一致性。这意味着每个事务都是原子的，或者成功或者失败，事物间是隔离的，互相完全不受影响，而且最终状态是持久落盘的。Oracle、MySQL、DB2 都能保证强一致性。一般而言，强一致性通常是通过多版本控制协议 (MVCC) 来实现的。\n交易系统只考虑: 关系型数据库 + 强悍硬件。 NoSQL完全不适合交易场景，一般用来作数据分析、ETL、报表、数据挖掘、推荐、日志处理、调用链分析等非核心交易场景。 ACID 是数据库事务完整性的理论。\nCAP 分布式系统设计理论  C (Consistency): A read is guaranteed to return the most recent write for a given client. 读操作保证能够返回最新的写操作结果。 A (Availability): A non-failing node will return a reasonable response within a reasonable amount of time (no error or no timeout). 合理时间返回合理响应。 P (Partition Tolerance): The system will continue to function when network partitions occur. 当出现网络分区后，系统能够继续“履行职责”。 虽然 CAP 理论定义是三个要素中只能取两个，但放到分布式环境下来思考，我们会发现必须选择 P（分区容忍）要素，因为网络本身无法做到 100% 可靠，有可能出故障，所以分区是一个必然的现象。如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），这又和 A 冲突了，因为 A 要求返回 no error 和 no timeout。因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。  1.CP - Consistency/Partition Tolerance\n如下图所示，为了保证一致性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 需要返回 Error，提示客户端 C“系统现在发生了错误”，这种处理方式违背了可用性（Availability）的要求，因此 CAP 三者只能满足 CP。\n2.AP - Availability/Partition Tolerance\n如下图所示，为了保证可用性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 将当前自己拥有的数据 x 返回给客户端 C 了，而实际上当前最新的数据已经是 y 了，这就不满足一致性（Consistency）的要求了，因此 CAP 三者只能满足 AP。注意：这里 N2 节点返回 x，虽然不是一个“正确”的结果，但是一个“合理”的结果，因为 x 是旧的数据，并不是一个错乱的值，只是不是最新的数据而已。\nBASE - CAP 理论中 AP 方案的延伸  BA (Basically Available): 出现故障时，允许损失部分可用性，保证核心可用，例如登录功能大于注册功能。 S (Soft State): 允许存在中间状态，中间状态不会影响系统整体可用性。 E (Eventual Consistency): 所有数据副本经过一段时间后，最终能够达到一致的状态。  牺牲强一致性而获得可用性，一般应用于服务化系统的应用层或者大数据处理系统中，通过达到最终一致性来尽量满足业务的绝大多数需求。由于不保证强一致性，因此系统在处理请求的过程中可以存在短暂的不一致，在这个时间窗口内，请求的每一步操作，都需要记录下来，以便在出现故障的时候可以从这些中间状态恢复过来。\n下面是解决一致性问题的三条实践经验:\n 单个数据库能够保证强一致性 将数据库进行水平伸缩和分片，相关数据分到数据库的同一个片上 记录每一步操作  CAP 理论实践 CAP 关注的粒度是数据，而不是整个系统 在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP。而如果我们做设计时，从整个系统的角度去选择 CP 还是 AP，就会发现顾此失彼，无论怎么做都是有问题的。\n以一个最简单的用户管理系统为例，用户管理系统包含用户账号数据（用户 ID、密码）、用户信息数据（昵称、兴趣、爱好、性别、自我介绍等）。通常情况下，用户账号数据会选择 CP，而用户信息数据会选择 AP，如果限定整个系统为 CP，则不符合用户信息数据的应用场景；如果限定整个系统为 AP，则又不符合用户账号数据的应用场景。\n所以在 CAP 理论落地实践时，我们需要将系统内的数据按照不同的应用场景和要求进行分类，每类数据选择不同的策略（CP 还是 AP），而不是直接限定整个系统所有数据都是同一策略。\nCAP 是忽略网络延迟的 这是一个非常隐含的假设，布鲁尔在定义一致性时，并没有将延迟考虑进去。也就是说，当事务提交时，数据能够瞬间复制到所有节点。但实际情况下，从节点 A 复制数据到节点 B，总是需要花费一定时间的。如果是相同机房，耗费时间可能是几毫秒；如果是跨地域的机房，例如北京机房同步到广州机房，耗费的时间就可能是几十毫秒。这就意味着，CAP 理论中的 C 在实践中是不可能完美实现的，在数据复制的过程中，节点 A 和节点 B 的数据并不一致。\n不要小看了这几毫秒或者几十毫秒的不一致，对于某些严苛的业务场景，例如和金钱相关的用户余额，或者和抢购相关的商品库存，技术上是无法做到分布式场景下完美的一致性的。而业务上必须要求一致性，因此单个用户的余额、单个商品的库存，理论上要求选择 CP 而实际上 CP 都做不到，只能选择 CA。也就是说，只能单点写入，其他节点做备份，无法做到分布式情况下多点写入。\n需要注意的是，这并不意味着这类系统无法应用分布式架构，只是说“单个用户余额、单个商品库存”无法做分布式，但系统整体还是可以应用分布式架构的。例如，下面的架构图是常见的将用户分区的分布式架构:\n我们可以将用户 id 为 0 ~ 100 的数据存储在 Node 1，将用户 id 为 101 ~ 200 的数据存储在 Node 2，Client 根据用户 id 来决定访问哪个 Node。对于单个用户来说，读写操作都只能在某个节点上进行；对所有用户来说，有一部分用户的读写操作在 Node 1 上，有一部分用户的读写操作在 Node 2 上。\n这样的设计有一个很明显的问题就是某个节点故障时，这个节点上的用户就无法进行读写操作了，但站在整体上来看，这种设计可以降低节点故障时受影响的用户的数量和范围，毕竟只影响 20% 的用户肯定要比影响所有用户要好。这也是为什么挖掘机挖断光缆后，支付宝只有一部分用户会出现业务异常，而不是所有用户业务异常的原因。\n正常运行情况下，不存在 CP 和 AP 的选择，可以同时满足 CA CAP 理论告诉我们分布式系统只能选择 CP 或者 AP，但其实这里的前提是系统发生了“分区”现象。如果系统没有发生分区现象，也就是说 P 不存在的时候（节点间的网络连接一切正常），我们没有必要放弃 C 或者 A，应该 C 和 A 都可以保证，这就要求架构设计的时候既要考虑分区发生时选择 CP 还是 AP，也要考虑分区没有发生时如何保证 CA。\n同样以用户管理系统为例，即使是实现 CA，不同的数据实现方式也可能不一样：用户账号数据可以采用“消息队列”的方式来实现 CA，因为消息队列可以比较好地控制实时性，但实现起来就复杂一些；而用户信息数据可以采用“数据库同步”的方式来实现 CA，因为数据库的方式虽然在某些场景下可能延迟较高，但使用起来简单。\n放弃并不等于什么都不做，需要为分区恢复后做准备 我们可以在分区期间进行一些操作，从而让分区故障解决后，系统能够重新达到 CA 的状态。\n最典型的就是在分区期间记录一些日志，当分区故障解决后，系统根据日志进行数据恢复，使得重新达到 CA 状态。同样以用户管理系统为例，对于用户账号数据，假设我们选择了 CP，则分区发生后，节点 1 可以继续注册新用户，节点 2 无法注册新用户（这里就是不符合 A 的原因，因为节点 2 收到注册请求后会返回 error），此时节点 1 可以将新注册但未同步到节点 2 的用户记录到日志中。当分区恢复后，节点 1 读取日志中的记录，同步给节点 2，当同步完成后，节点 1 和节点 2 就达到了同时满足 CA 的状态。\n而对于用户信息数据，假设我们选择了 AP，则分区发生后，节点 1 和节点 2 都可以修改用户信息，但两边可能修改不一样。例如，用户在节点 1 中将爱好改为“旅游、美食、跑步”，然后用户在节点 2 中将爱好改为“美食、游戏”，节点 1 和节点 2 都记录了未同步的爱好数据，当分区恢复后，系统按照某个规则来合并数据。例如，按照“最后修改优先规则”将用户爱好修改为“美食、游戏”，按照“字数最多优先规则”则将用户爱好修改为“旅游，美食、跑步”，也可以完全将数据冲突报告出来，由人工来选择具体应该采用哪一条。\n分布式一致性协议 两阶段提交协议 二阶段提交的算法思路可以概括为：协调者询问参与者是否准备好了提交，并根据所有参与者的反馈情况决定向所有参与者发送 commit 或者 rollback 指令（协调者向所有参与者发送相同的指令）。\n三阶段提交协议 三阶段提交协议是两阶段提交协议的改进版本。它通过超时机制解决了阻塞的问题，并且把两个阶段增加为三个阶段。\n不同点:\n 增加询问阶段:尽可能早地发现无法执行操作而需要中止的行为。 准备阶段以后，协调者和参与者执行任务中都增加了超时，一旦超时，则协调者和参与者都会继续提交事务，默认为成功，这也是根据概率统计超时后默认为成功的正确性最大。  存在问题:\n 在 doCommit 阶段，如果参与者没有及时接收到来自协调者的 doCommit 或者 rebort 请求时，会在等待超时之后，会继续进行事务的提交。所以，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。  TCC 协议 上述两个协议实现复杂，操作步骤多，性能也是一个很大问题，因此在互联网高并发系统中，鲜有使用两阶段提交和三阶段提交协议的场景。\nTCC 协议将一个任务拆分为 Try、Confirm、Cancel 三个步骤，没有单独的准备阶段， Try 操作兼备资源操作与准备能力，另外 Confirm 操作和 Cancel 操作要满足幂等性。虽然没有解决极端情况下不一致和脑裂的问题，然而 TCC 通过自动化补偿，将需要人工处理的不一致情况降低到最少，也是一种非常有用的解决方案。\nTCC 协议相比其它两个协议更简单且更容易实现。\n保证最终一致性的模式 一致性在现实系统实践中，仅仅需要达到最终一致性，并不需要专业的、复杂的一致性协议。实现最终一致性有一些有效、简单的模式如下:\n查询模式 通过查询模式，我们可以清楚地知道某个任务或者操作处于一个什么样的状态，是执行成功还是失败，还是正在执行，这样也方便其他系统依据当前返回的状态进行下一步操作。为了能够实现查询，每个服务操作都需要唯一的流水号标识，例如请求流水号、订单号等。\n补偿模式 我们修正系统以让其达到最终一致状态的过程，称之为补偿。而支持补偿模式，那么这个服务针对特定任务需要提供重试操作和取消操作:\n定期校对模式 在分布式系统中构建了唯一 ID、调用链等基础设施后，我们很容易对系统间的不一致进行核对，发现不一致，则利用补偿来修复即可。定期校对模式多用于金融系统中，涉及资金安全的，需要保证准确性。\n超时处理模式 超时补偿原则 超时补偿原则确定的是调用方和被调用方谁应该负责重试或补偿的问题。\n被调用方补偿:\n如果服务 2 告诉服务 1 消息已经接受，那么服务 1 任务就已经结束了，如果服务 2 处理失败，那么服务 2 应该负责重试或者补偿。\nvoid service2() { while (i \u0026lt; tryTimes) { // 任务没有执行成功，则自己补偿  if (!doTask()) { i++; continue; } break; } } 调用方补偿:\n如果服务 2 无明确接受响应，那么服务 1 应该持续进行重试，直到服务 2 明确表示已经接受消息:\nvoid service1() { while (true) { try { scheduleTaskToService2(); // 保证幂等性  } catch (TimeOutException e) { continue; // 无明确接受响应，调用方负责重试  } break; } } 解决 (1) 扣库存问题\n数据量小，可以利用关系数据库的强一致性解决，也就是把订单和库存表放到一个关系型数据库中。单机难以满足的话，就分片，尽量保证订单和库存放入同一个数据库分片中。\n(2) 超时无结果\n需要依据操作 ID 来主动查询任务的当前状态，以便决定下一步做什么。\n(3) 回调无结果\n需要依据操作 ID 来主动查询任务的当前状态，以便决定下一步做什么。\n(4) 订单不存在\n查询处理情况，定期校对，补偿修复。\n(5) 状态不一致\n查询处理情况，定期校对，补偿修复。\n(6) 缓存一致性\n为了提高性能，数据库与缓存只需要保持弱一致性，而不需要保持强一致性，否则违背了使用缓存的初衷。\n(7) 缓存时间窗口\n如果性能要求不是非常高，则尽量使用分布式缓存，而不要使用本地缓存。另外读的顺序是先读缓存，再读数据库；写的顺序是先写数据库，再写缓存。\n(8) 数据完整性\n写缓存时数据一定要完整，如果缓存数据的一部分有效，另一部分无效，则能可在需要时回源数据库，也不要把部分数据放入缓存中。\nboolean cacheData() { Object o1 = readFromDB1(); Object o2 = readFromDB2(); // 确保缓存数据完整性，不要缓存一部分数据  if (o1 != null \u0026amp;\u0026amp; o2 != null) { return cacheData(o1, o2); } return false; } 参考  《分布式服务架构：原理、设计与实战》 极客时间订阅:从0开始学架构 "});index.add({'id':56,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock-3/','title':"Best Time to Buy and Sell Stock Ⅲ",'content':"Best Time to Buy and Sell Stock Ⅲ 题目 LeetCode 地址：Best Time to Buy and Sell Stock Ⅲ\n有一个数组，第 i 个元素的值代表第 i 天的股票价格，如果你最多只能进行两次交易（某天买入一支股票，然后过几天卖掉），请问你能收获的最大利润是多少？\n分析 参考 Best Time to Buy and Sell Stock 思路上状态机，状态机应用两次即可。\n答案 // 最多两次交易 // 且不能同时持有，必须卖掉这个，然后持有另外一个 // https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/ // public class BestTimetoBuyandSellStockIII { // Buy Sell Buy Sell  // s0 ----\u0026gt; s1 -----\u0026gt; s2 -----\u0026gt; s3 ------\u0026gt; s4 (end)  // ↑___| ↑__| ↑____| ↑___|  //  public int maxProfit(int[] prices) { if (prices == null || prices.length \u0026lt;= 1) { return 0; } int s0 = 0; int s1 = -prices[0]; int s2 = 0; int s3 = -prices[0]; int s4 = 0; for (int i = 1; i \u0026lt; prices.length; i++) { s0 = s0; s1 = Math.max(s1, s0 - prices[i]); s2 = Math.max(s2, s1 + prices[i]); s3 = Math.max(s3, s2 - prices[i]); s4 = Math.max(s4, s3 + prices[i]); } return s4; } } 扫描下面二维码，在手机上阅读这篇文章：\n"});index.add({'id':57,'href':'/docs/tutorial/front-end-optimization-guide/css-optimization/','title':"CSS 优化",'content':"CSS 优化 本文讲述在实际工作中如何优化 CSS，提升页面加载的性能！\n避免使用 @import @import url(\u0026#34;base.css\u0026#34;); @import url(\u0026#34;layout.css\u0026#34;); @import url(\u0026#34;carousel.css\u0026#34;); 由于 @import 属性允许相互之间嵌套引入，因此浏览器必须串行的去下载每一个 @import 引入的文件，因此会增加下载 CSS 文件的时间，而使用 \u0026lt;link\u0026gt; 就可以并行下载 CSS 文件，可有效提升 CSS 加载的性能：\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;base.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;layout.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;carousel.css\u0026#34;\u0026gt; 简化 CSS 选择器 浏览器是从右向左逐步解析选择器表达式的，例如 #id .class \u0026gt; ul a 。首先找到页面上所有匹配 a 的节点，然后找到所有 ul 元素，并且将 a 元素恰好是 ul 元素子节点的元素过滤出来，直至解析到最左侧的选择器 #id 。\n如下是在网站上针对 50000 个元素使用不同 CSS 选择器选择元素的时间对比：\n   选择器 查询时间(ms)     div 4.8740   .box 3.625   .box \u0026gt; .title 4.4587   .box .title 4.5161   .box ~ .box 4.7082   .box + .box 4.6611   .box:last-of-type 3.944   .box:nth-of-type(2n - 1) 16.8491   .box:not(:last-of-type) 5.8947   .box:not(:empty):last-of-type .title 8.0202   .box:nth-last-child(n+6) ~ div 20.8710    最慢的选择器接近 20ms，而最快的仅需 3.5ms 左右，由此可见 CSS 选择器越短，解析速度越快！\n避免使用 CSS 表达式 IE5 引入了 CSS 表达式，或者称之为 \u0026ldquo;动态属性\u0026rdquo;，这样可以让开发人员以更为紧凑的方式在 CSS 中就可以完成高级样式处理。然而，CSS 表达式带来的性能损失是相当大的，因为每当触发任何事件（如窗口大小调整、鼠标移动等）时，浏览器都会重新运行每个表达式，这也是它在 IE8 被弃用的原因之一。如果在页面中使用了 CSS 表达式，则应尽一切努力删除它们，并使用其他方法来实现相同的功能。\n避免使用 expensive 的属性 有些属性生来渲染速度就慢于其它属性，下面这些属性在绘制之前可能会导致其它计算，因此尽量避免使用！\n border-radius box-shadow opacity transform filter position: fixed  精简 CSS 代码 精简 CSS 代码意味着对 CSS 源文件，采取移除无关的空白字符、换行符、注释、去除不必要的单位、去除不必要的零等方法。其可以有效压缩 CSS 文件的大小，减少浏览器下载和执行文件所需的时间。\n通过使用近几年来出现的新的页面布局方式，例如 Flexbox 和 Grid 布局，可以有效减少达到之前使用 float 属性进行的相同布局所需要的代码量，以前自己需要做的事情，现在浏览器在底层可以更快的帮你完成。\n在引入 CSS 库的时候，也要去仔细对比，在满足要求的条件下，应该尽量选择 size 比较小的库。\n优化 CSS 动画  同时进行多个 CSS 动画可能不会工作地很好，极有可能导致延迟出现，适当地给一些动画增加 transition-delay 属性以避免同时运行多个动画。 浏览器加载网页地时候非常忙，因此尽可能地将所有动画延迟到初始加载事件之后的几百毫秒，可以有效提升页面的整体性能。 SVG 非常适合动画，因为它们可以在不降低分辨率的情况下进行缩放。 只在最后才考虑使用 will-change 属性，毕竟它也消耗资源！  参考  20 Tips for Optimizing CSS Performance Tips for Improving CSS and JS Animation Performance PageSpeed: Avoid CSS expressions (deprecated)  扫描下面二维码，在手机端阅读本文：\n"});index.add({'id':58,'href':'/docs/tutorial/git/','title':"Git 教程",'content':"Git 教程 目录  Git 配置用户名和邮箱 创建 Git 仓库 Git 查看文件差异 Git 重置 Git checkout Git 保存当前进度 Git 多次提交合并成一次提交 Git 分支 Git 分支合并 Git 解决冲突 Git tag Git add 和 Git rm Git push 和 Git pull Git commit Git .ignore 文件  Git 的四个区 "});index.add({'id':59,'href':'/docs/tutorial/git/check-file-diff/','title':"Git 查看文件差异",'content':"Git 查看文件差异 Git 三个区 Git 有三个区：工作区、Stage 区（暂存区）、版本库。这意味着，一个文件可能在这三个区都有所不同。如下图所示，一个文件使用 git add 命令之后，这个文件就转移到了暂存区，继续使用 git commit 之后就转移到了版本库。git diff 使用不同的命令参数可以查看文件在这三个区域中的两两对比的差异。\n本地代码（工作区）与暂存区中的差异 git diff 示例结果如下所示：\ndiff 输出的格式介绍 下面解释上述 git diff 输出的格式：\n 第一行，展示了使用什么命令做的比较 第二行，100644 代表这是一个普通文件 --- 表示原始文件，即这个文件没有修改前的内容 +++ 表示新文件，即这个文件修改后的内容 -1,5 中的 - 表示原始文件，1,5 表示从第 1 行到第 4 行做了改动 +1,5 中的 + 表示新文件，1,5 表示从第 1 行到第 4 行做了改动 @@ -1,5 +1,5 @@ 表示这个文件的第 1 行到第 4 行，变更为了新文件的第 1 行到第 4 行  本地代码（工作区）和版本库的差异 git diff HEAD  HEAD 指：当前工作分支的版本库\n 如果你当前分支是 master，那么上述命令与下面这行命令是等价的：\ngit diff master 暂存区和版本库的差异 git diff --cached # 或 git diff --staged Git diff 命令与版本库关系图 参考  How to read the output from git diff? Git权威指南  扫描下面二维码，在手机端阅读：\n"});index.add({'id':60,'href':'/docs/tutorial/network/http2/','title':"HTTP2",'content':"HTTP2 二进制分帧层 HTTP/2 所有性能增强的核心在于新的二进制分帧层，它定义了如何封装 HTTP 消息并在客户端与服务器之间传输。\nHTTP/1.x 协议以换行符作为纯文本的分隔符，而 HTTP/2 将所有传输的信息分割为更小的消息和帧，并采用二进制格式对它们编码。\n数据流、消息和帧 新的二进制分帧机制改变了客户端与服务器之间交换数据的方式。 为了说明这个过程，我们需要了解 HTTP/2 的三个概念：\n 数据流：已建立的连接内的双向字节流，可以承载一条或多条消息。 消息：与逻辑请求或响应消息对应的完整的一系列帧。 帧：HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。  这些概念的关系总结如下：\n 所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。  简言之，HTTP/2 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都在一个 TCP 连接内复用。 这是 HTTP/2 协议所有其他功能和性能优化的基础。\n请求与响应复用 在 HTTP/1.x 中，如果客户端要想发起多个并行请求以提升性能，则必须使用多个 TCP 连接。 这是 HTTP/1.x 交付模型的直接结果，该模型可以保证每个连接每次只交付一个响应（响应排队）。 更糟糕的是，这种模型也会导致队首阻塞，从而造成底层 TCP 连接的效率低下。\nHTTP/2 中新的二进制分帧层突破了这些限制，实现了完整的请求和响应复用：客户端和服务器可以将 HTTP 消息分解为互不依赖的帧，然后交错发送，最后再在另一端把它们重新组装起来。\n快照捕捉了同一个连接内并行的多个数据流。 客户端正在向服务器传输一个 DATA 帧（数据流 5），与此同时，服务器正向客户端交错发送数据流 1 和数据流 3 的一系列帧。因此，一个连接上同时有三个并行数据流。\n将 HTTP 消息分解为独立的帧，交错发送，然后在另一端重新组装是 HTTP 2 最重要的一项增强。事实上，这个机制会在整个网络技术栈中引发一系列连锁反应，从而带来巨大的性能提升，让我们可以：\n 并行交错地发送多个请求，请求之间互不影响。 并行交错地发送多个响应，响应之间互不干扰。 使用一个连接并行发送多个请求和响应。 不必再为绕过 HTTP/1.x 限制而做很多工作（请参阅针对 HTTP/1.x 进行优化，例如级联文件、image sprites 和域名分片）。 消除不必要的延迟和提高现有网络容量的利用率，从而减少页面加载时间。 等等…  HTTP/2 中的新二进制分帧层解决了 HTTP/1.x 中存在的队首阻塞问题，也消除了并行处理和发送请求及响应时对多个连接的依赖。 结果，应用速度更快、开发更简单、部署成本更低。\n数据流优先级 将 HTTP 消息分解为很多独立的帧之后，我们就可以复用多个数据流中的帧，客户端和服务器交错发送和传输这些帧的顺序就成为关键的性能决定因素。 为了做到这一点，HTTP/2 标准允许每个数据流都有一个关联的权重和依赖关系：\n 可以向每个数据流分配一个介于 1 至 256 之间的整数。 每个数据流与其他数据流之间可以存在显式依赖关系。  数据流依赖关系和权重的组合让客户端可以构建和传递“优先级树”，表明它倾向于如何接收响应。 反过来，服务器可以使用此信息通过控制 CPU、内存和其他资源的分配设定数据流处理的优先级，在资源数据可用之后，带宽分配可以确保将高优先级响应以最优方式传输至客户端。\nHTTP/2 内的数据流依赖关系通过将另一个数据流的唯一标识符作为父项引用进行声明；如果忽略标识符，相应数据流将依赖于“根数据流”。 声明数据流依赖关系指出，应尽可能先向父数据流分配资源，然后再向其依赖项分配资源。 换句话说，“请先处理和传输响应 D，然后再处理和传输响应 C”。\n共享相同父项的数据流（即，同级数据流）应按其权重比例分配资源。 例如，如果数据流 A 的权重为 12，其同级数据流 B 的权重为 4，那么要确定每个数据流应接收的资源比例，请执行以下操作：\n 将所有权重求和：4 + 12 = 16 将每个数据流权重除以总权重：A = 12/16, B = 4/16  因此，数据流 A 应获得四分之三的可用资源，数据流 B 应获得四分之一的可用资源；数据流 B 获得的资源是数据流 A 所获资源的三分之一。\n我们来看一下上图中的其他几个操作示例。 从左到右依次为：\n 数据流 A 和数据流 B 都没有指定父依赖项，依赖于隐式“根数据流”；A 的权重为 12，B 的权重为 4。因此，根据比例权重：数据流 B 获得的资源是 A 所获资源的三分之一。 数据流 D 依赖于根数据流；C 依赖于 D。 因此，D 应先于 C 获得完整资源分配。 权重不重要，因为 C 的依赖关系拥有更高的优先级。 数据流 D 应先于 C 获得完整资源分配；C 应先于 A 和 B 获得完整资源分配；数据流 B 获得的资源是 A 所获资源的三分之一。 数据流 D 应先于 E 和 C 获得完整资源分配；E 和 C 应先于 A 和 B 获得相同的资源分配；A 和 B 应基于其权重获得比例分配。  如上面的示例所示，数据流依赖关系和权重的组合明确表达了资源优先级，这是一种用于提升浏览性能的关键功能，网络中拥有多种资源类型，它们的依赖关系和权重各不相同。 不仅如此，HTTP/2 协议还允许客户端随时更新这些优先级，进一步优化了浏览器性能。 换句话说，我们可以根据用户互动和其他信号更改依赖关系和重新分配权重。\n注：数据流依赖关系和权重表示传输优先级，而不是要求，因此不能保证特定的处理或传输顺序。 即，客户端无法强制服务器通过数据流优先级以特定顺序处理数据流。 尽管这看起来违反直觉，但却是一种必要行为。 我们不希望在优先级较高的资源受到阻止时，还阻止服务器处理优先级较低的资源。\n每个来源一个连接 有了新的分帧机制后，HTTP/2 不再依赖多个 TCP 连接去并行复用数据流；每个数据流都拆分成很多帧，而这些帧可以交错，还可以分别设定优先级。 因此，所有 HTTP/2 连接都是永久的，而且仅需要每个来源一个连接，随之带来诸多性能优势。\n SPDY 和 HTTP/2 的杀手级功能是，可以在一个拥塞受到良好控制的通道上任意进行复用。 这一功能的重要性和良好运行状况让我吃惊。 我喜欢的一个非常不错的指标是连接拆分，这些拆分仅承载一个 HTTP 事务（并因此让该事务承担所有开销）。 对于 HTTP/1，我们 74% 的活动连接仅承载一个事务 - 永久连接并不如我们所有人希望的那般有用。 但是在 HTTP/2 中，这一比例锐减至 25%。 这是在减少开销方面获得的巨大成效。\n 大多数 HTTP 传输都是短暂且急促的，而 TCP 则针对长时间的批量数据传输进行了优化。 通过重用相同的连接，HTTP/2 既可以更有效地利用每个 TCP 连接，也可以显著降低整体协议开销。 不仅如此，使用更少的连接还可以减少占用的内存和处理空间，也可以缩短完整连接路径（即，客户端、可信中介和源服务器之间的路径） 这降低了整体运行成本并提高了网络利用率和容量。 因此，迁移到 HTTP/2 不仅可以减少网络延迟，还有助于提高通量和降低运行成本。\n注：连接数量减少对提升 HTTPS 部署的性能来说是一项特别重要的功能：可以减少开销较大的 TLS 连接数、提升会话重用率，以及从整体上减少所需的客户端和服务器资源。\n流控制 流控制是一种阻止发送方向接收方发送大量数据的机制，以免超出后者的需求或处理能力：发送方可能非常繁忙、处于较高的负载之下，也可能仅仅希望为特定数据流分配固定量的资源。 例如，客户端可能请求了一个具有较高优先级的大型视频流，但是用户已经暂停视频，客户端现在希望暂停或限制从服务器的传输，以免提取和缓冲不必要的数据。 再比如，一个代理服务器可能具有较快的下游连接和较慢的上游连接，并且也希望调节下游连接传输数据的速度以匹配上游连接的速度来控制其资源利用率；等等。\n上述要求会让您想到 TCP 流控制吗？您应当想到这一点；因为问题基本相同。 不过，由于 HTTP/2 数据流在一个 TCP 连接内复用，TCP 流控制既不够精细，也无法提供必要的应用级 API 来调节各个数据流的传输。 为了解决这一问题，HTTP/2 提供了一组简单的构建块，这些构建块允许客户端和服务器实现其自己的数据流和连接级流控制：\n 流控制具有方向性。 每个接收方都可以根据自身需要选择为每个数据流和整个连接设置任意的窗口大小。 流控制基于信用。 每个接收方都可以公布其初始连接和数据流流控制窗口（以字节为单位），每当发送方发出 DATA 帧时都会减小，在接收方发出 WINDOW_UPDATE 帧时增大。 流控制无法停用。 建立 HTTP/2 连接后，客户端将与服务器交换 SETTINGS 帧，这会在两个方向上设置流控制窗口。 流控制窗口的默认值设为 65,535 字节，但是接收方可以设置一个较大的最大窗口大小（2^31-1 字节），并在接收到任意数据时通过发送 WINDOW_UPDATE 帧来维持这一大小。 流控制为逐跃点控制，而非端到端控制。 即，可信中介可以使用它来控制资源使用，以及基于自身条件和启发式算法实现资源分配机制。  HTTP/2 未指定任何特定算法来实现流控制。 不过，它提供了简单的构建块并推迟了客户端和服务器实现，可以实现自定义策略来调节资源使用和分配，以及实现新传输能力，同时提升网页应用的实际性能和感知性能。\n例如，应用层流控制允许浏览器仅提取一部分特定资源，通过将数据流流控制窗口减小为零来暂停提取，稍后再行恢复。 换句话说，它允许浏览器提取图像预览或首次扫描结果，进行显示并允许其他高优先级提取继续，然后在更关键的资源完成加载后恢复提取。\n服务器推送 HTTP/2 新增的另一个强大的新功能是，服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外资源，而无需客户端明确地请求。\n 注：HTTP/2 打破了严格的请求-响应语义，支持一对多和服务器发起的推送工作流，在浏览器内外开启了全新的互动可能性。 这是一项使能功能，对我们思考协议、协议用途和使用方式具有重要的长期影响。\n 为什么在浏览器中需要一种此类机制呢？一个典型的网络应用包含多种资源，客户端需要检查服务器提供的文档才能逐个找到它们。 那为什么不让服务器提前推送这些资源，从而减少额外的延迟时间呢？ 服务器已经知道客户端下一步要请求什么资源，这时候服务器推送即可派上用场。\n事实上，如果您在网页中内联过 CSS、JavaScript，或者通过数据 URI 内联过其他资产（请参阅资源内联），那么您就已经亲身体验过服务器推送了。 对于将资源手动内联到文档中的过程，我们实际上是在将资源推送给客户端，而不是等待客户端请求。 使用 HTTP/2，我们不仅可以实现相同结果，还会获得其他性能优势。 推送资源可以进行以下处理：\n 由客户端缓存 在不同页面之间重用 与其他资源一起复用 由服务器设定优先级 被客户端拒绝  PUSH_PROMISE 101 所有服务器推送数据流都由 PUSH_PROMISE 帧发起，表明了服务器向客户端推送所述资源的意图，并且需要先于请求推送资源的响应数据传输。 这种传输顺序非常重要：客户端需要了解服务器打算推送哪些资源，以免为这些资源创建重复请求。 满足此要求的最简单策略是先于父响应（即，DATA 帧）发送所有 PUSH_PROMISE 帧，其中包含所承诺资源的 HTTP 标头。\n在客户端接收到 PUSH_PROMISE 帧后，它可以根据自身情况选择拒绝数据流（通过 RST_STREAM 帧）。 （例如，如果资源已经位于缓存中，便可能会发生这种情况。） 这是一个相对于 HTTP/1.x 的重要提升。 相比之下，使用资源内联（一种受欢迎的 HTTP/1.x“优化”）等同于“强制推送”：客户端无法选择拒绝、取消或单独处理内联的资源。\n使用 HTTP/2，客户端仍然完全掌控服务器推送的使用方式。 客户端可以限制并行推送的数据流数量；调整初始的流控制窗口以控制在数据流首次打开时推送的数据量；或完全停用服务器推送。 这些优先级在 HTTP/2 连接开始时通过 SETTINGS 帧传输，可能随时更新。\n推送的每个资源都是一个数据流，与内嵌资源不同，客户端可以对推送的资源逐一复用、设定优先级和处理。 浏览器强制执行的唯一安全限制是，推送的资源必须符合原点相同这一政策：服务器对所提供内容必须具有权威性。\n标头压缩 每个 HTTP 传输都承载一组标头，这些标头说明了传输的资源及其属性。 在 HTTP/1.x 中，此元数据始终以纯文本形式，通常会给每个传输增加 500–800 字节的开销。如果使用 HTTP Cookie，增加的开销有时会达到上千字节。 为了减少此开销和提升性能，HTTP/2 使用 HPACK 压缩格式压缩请求和响应标头元数据，这种格式采用两种简单但是强大的技术：\n 这种格式支持通过静态霍夫曼代码对传输的标头字段进行编码，从而减小了各个传输的大小。 这种格式要求客户端和服务器同时维护和更新一个包含之前见过的标头字段的索引列表（换句话说，它可以建立一个共享的压缩上下文），此列表随后会用作参考，对之前传输的值进行有效编码。  利用霍夫曼编码，可以在传输时对各个值进行压缩，而利用之前传输值的索引列表，我们可以通过传输索引值的方式对重复值进行编码，索引值可用于有效查询和重构完整的标头键值对。\n作为一种进一步优化方式，HPACK 压缩上下文包含一个静态表和一个动态表：静态表在规范中定义，并提供了一个包含所有连接都可能使用的常用 HTTP 标头字段（例如，有效标头名称）的列表；动态表最初为空，将根据在特定连接内交换的值进行更新。 因此，为之前未见过的值采用静态 Huffman 编码，并替换每一侧静态表或动态表中已存在值的索引，可以减小每个请求的大小。\n注：在 HTTP/2 中，请求和响应标头字段的定义保持不变，仅有一些微小的差异：所有标头字段名称均为小写，请求行现在拆分成各个 :method、:scheme、:authority 和 :path 伪标头字段。\nHPACK 的安全性和性能 早期版本的 HTTP/2 和 SPDY 使用 zlib（带有一个自定义字典）压缩所有 HTTP 标头。 这种方式可以将所传输标头数据的大小减小 85% - 88%，显著减少了页面加载时间延迟：\n 在带宽较低的 DSL 链路中，上行链路速度仅有 375 Kbps，仅压缩请求标头就显著减少了特定网站（即，发出大量资源请求的网站）的页面加载时间。 我们发现，仅仅由于标头压缩，页面加载时间就减少了 45 - 1142 毫秒。\n 然而，2012 年夏天，出现了针对 TLS 和 SPDY 压缩算法的“犯罪”安全攻击，此攻击会导致会话被劫持。 于是，zlib 压缩算法被 HPACK 替代，后者经过专门设计，可以解决发现的安全问题、实现起来也更高效和简单，当然，可以对 HTTP 标头元数据进行良好压缩。\n参考  HTTP/2 简介  "});index.add({'id':61,'href':'/docs/it-zone/','title':"IT 圈",'content':"IT 圈 IT 圈：每日收录最新编程、IT、软件、互联网等热门焦点新闻，让您实时掌握 IT 届最新动态！\n 06-09 | 谷歌修改 Chromium 源码中的“黑白名单”术语 06-02 | Rust 语言首次进入 Tiobe 前 20 名 06-01 | fastjson 又现高危漏洞！  "});index.add({'id':62,'href':'/docs/tutorial/distributed-storage/leveldb-read/','title':"leveldb 源码分析与实现 - 读取",'content':"leveldb 源码分析与实现: 读取 读文件的过程 首先查看 memtable，其次查看 immutable memtable，最后尝试从 versions_-\u0026gt;current() 中获取。\n// First look in the memtable, then in the immutable memtable (if any). LookupKey lkey(key, snapshot); if (mem-\u0026gt;Get(lkey, value, \u0026amp;s)) { // Done } else if (imm != nullptr \u0026amp;\u0026amp; imm-\u0026gt;Get(lkey, value, \u0026amp;s)) { // Done } else { s = current-\u0026gt;Get(options, lkey, value, \u0026amp;stats); have_stat_update = true; } MemTable // memtable.cc bool MemTable::Get(const LookupKey\u0026amp; key, std::string* value, Status* s) { // 取出和这个 key 关联的 tag  const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); switch (static_cast\u0026lt;ValueType\u0026gt;(tag \u0026amp; 0xff)) { // 有值  case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u0026gt;assign(v.data(), v.size()); return true; } // key 已经被 delete 了  case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } Immutable MemTable imm 代表的是 Immutable MemTable，意味着只能从这个 MemTable 中读取值，而不能写入值。在写入值之前，leveldb 会在适当时机，将 mem_ 转为 imm_。\n// ... imm_ = mem_; has_imm_.store(true, std::memory_order_release); mem_ = new MemTable(internal_comparator_); // 从当前 Version 查找 重要结构体 FileMetaData 是一个结构体:\n// version_edit.h struct FileMetaData { FileMetaData() : refs(0), allowed_seeks(1 \u0026lt;\u0026lt; 30), file_size(0) {} int refs; int allowed_seeks; // Seeks allowed until compaction  uint64_t number; uint64_t file_size; // File size in bytes  InternalKey smallest; // Smallest internal key served by table  InternalKey largest; // Largest internal key served by table }; Rep 是定义在 table.cc 文件中的结构体:\nstruct Table::Rep { ~Rep() { delete filter; delete[] filter_data; delete index_block; } Options options; Status status; RandomAccessFile* file; uint64_t cache_id; FilterBlockReader* filter; const char* filter_data; BlockHandle metaindex_handle; // Handle to metaindex_block: saved from footer  Block* index_block; }; 查找的过程 // version_set.cc Status Version::Get(const ReadOptions\u0026amp; options, const LookupKey\u0026amp; k, std::string* value, GetStats* stats) { // ...  ForEachOverlapping(state.saver.user_key, state.ikey, \u0026amp;state, \u0026amp;State::Match); // ... } ForEachOverlapping 从 level-0 一直向更深的 level 进行搜索:\n// version_set.cc void Version::ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)) { // Search level-0 in order from newest to oldest.  for (uint32_t i = 0; i \u0026lt; files_[0].size(); i++) { // ...  } // Search other levels.  for (int level = 1; level \u0026lt; config::kNumLevels; level++) { // ...  } } 从此处可以得知，每一层的文件列表都存储在 files_ 数组中:\n// version_set.h // List of files per level std::vector\u0026lt;FileMetaData*\u0026gt; files_[config::kNumLevels]; InternalKey 比较器 // dbformat.h class InternalKeyComparator : public Comparator { private: const Comparator* user_comparator_; } // comparator.cc class BytewiseComparatorImpl : public Comparator { } // options.cc Options::Options() : comparator(BytewiseComparator()), env(Env::Default()) {} InternalKey 比较器默认采用的是 leveldb 提供的 BytewiseComparatorImpl 比较器，其比较算法如下:\n// comparator.cc int Compare(const Slice\u0026amp; a, const Slice\u0026amp; b) const override { return a.compare(b); } // slice.h inline int Slice::compare(const Slice\u0026amp; b) const { const size_t min_len = (size_ \u0026lt; b.size_) ? size_ : b.size_; int r = memcmp(data_, b.data_, min_len); if (r == 0) { if (size_ \u0026lt; b.size_) r = -1; else if (size_ \u0026gt; b.size_) r = +1; } return r; }  int memcmp ( const void * ptr1, const void * ptr2, size_t num ): Compare two blocks of memory. Compares the first num bytes of the block of memory pointed by ptr1 to the first num bytes pointed by ptr2, returning zero if they all match or a value different from zero representing which is greater if they do not.\n FileMetaData 的构建过程 我们在上述代码中已经看到，每一个文件是用 FileMetaData 来表示的，而这个数据结构中有几个最关键的字段 number、smallest、largest，如下就是它们各自的赋值过程:\n// db_impl.cc Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { FileMetaData meta; // ...  meta.number = versions_-\u0026gt;NewFileNumber(); Iterator* iter = mem-\u0026gt;NewIterator(); // ...  s = BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026amp;meta); } // builder.cc Status BuildTable(const std::string\u0026amp; dbname, Env* env, const Options\u0026amp; options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { iter-\u0026gt;SeekToFirst(); // ...  TableBuilder* builder = new TableBuilder(options, file); meta-\u0026gt;smallest.DecodeFrom(iter-\u0026gt;key()); Slice key; for (; iter-\u0026gt;Valid(); iter-\u0026gt;Next()) { key = iter-\u0026gt;key(); builder-\u0026gt;Add(key, iter-\u0026gt;value()); } if (!key.empty()) { meta-\u0026gt;largest.DecodeFrom(key); } } 其中 InternalKey 的 DecodeFrom 方法的定义如下所示:\nclass InternalKey { private: std::string rep_; public: bool DecodeFrom(const Slice\u0026amp; s) { rep_.assign(s.data(), s.size()); return !rep_.empty(); } 在此我们知道了一个文件的 smallest 指的是添加这个文件时候，位于内存中的 mem 跳表的最小 key，而 largest 则是 mem 跳表上的最大 key，number 则是版本集赋予的一个新的 FileNumber。\n跳表 mem 默认也是采用的 BytewiseComparator 作为元素的比较器的。\n"});index.add({'id':63,'href':'/docs/tutorial/unix-command/ls/','title':"ls",'content':"ls ls 命令教程，ls 命令的常见使用方法介绍。\n简介 ls 命令是一个命令行实用程序，用于列出通过标准输入提供给它的一个或多个目录的内容。它将结果写入标准输出。ls 命令支持显示关于文件的各种信息、对一系列选项进行排序和递归列表。\n示例 （1）显示目录中的文件\nls /home/zk （2）显示隐藏的文件和文件夹\nls -a /home/zk 结果：\nls -a /home/george . .goobook .tmux.conf .. .goobook_auth.json .urlview .asoundrc .inputrc .vim .asoundrc.asoundconf .install.sh .viminfo .asoundrc.asoundconf.bak .irbrc .viminfo.tmp ... （3）列出来的文件，标识上文件的类型\nls -F 显示结果如下所示：\nbin@ dotfiles/ file.txt irc/ src/ code/ Downloads/ go/ logs/ 不同文件类型显示的后缀不同：\n /：目录 @：symbolic link |：FIFO =：socket \u0026gt;：door 什么也不显示，代表正常文件  （4）显示更多信息\nls -l 显示结果：\n-rwxrw-r-- 10 root root 2048 Jan 13 07:11 afile.exe 每一列的含义：\n 文件权限 link 的数量 owner 名称 owner 组 文件大小 上次修改时间 文件/文件夹名称  （5）根据文件大小进行排序\nls -lS 排序结果如下（从大到小开始排序）：\nls -lS total 56 drwxr-xr-x 2 george users 32768 Oct 4 09:15 logs drwxr-xr-x 6 george users 4096 Oct 4 20:27 code drwxr-xr-x 10 george users 4096 Oct 4 09:13 dotfiles drwx------ 3 george users 4096 Oct 4 11:31 Downloads drwxr-xr-x 5 george users 4096 Sep 25 08:30 go （6）按照修改时间排序\nls -lt 排序结果如下：\nls -lt total 56 -rw-r--r-- 1 george users 0 Oct 4 20:42 file.txt drwxr-xr-x 6 george users 4096 Oct 4 20:27 code drwx------ 3 george users 4096 Oct 4 11:31 Downloads drwxr-xr-x 2 george users 32768 Oct 4 09:15 logs drwxr-xr-x 10 george users 4096 Oct 4 09:13 dotfiles （7）根据访问时间排序\nls -lu 排序结果如下：\nls -lu total 56 lrwxrwxrwx 1 george users 25 Oct 4 09:01 bin -\u0026gt; /home/george/dotfiles/bin drwxr-xr-x 6 george users 4096 Oct 4 20:23 code drwxr-xr-x 10 george users 4096 Oct 4 11:21 dotfiles drwx------ 3 george users 4096 Oct 4 11:24 Downloads （8）以人类可读的格式显示文件大小\nls -lh （9）递归显示文件\nls -R my_folder 参考  Linux and Unix ls command tutorial with examples  扫描下面二维码，在手机端阅读：\n"});index.add({'id':64,'href':'/docs/rocketmq/rocketmq-message-receive-flow/','title':"RocketMQ 消息接受流程",'content':"RocketMQ 消息接受流程 本篇讲述 RocketMQ 消息接受流程\n一、消费者注册 生产者负责往服务器 Broker 发送消息，消费者则从 Broker 获取消息。消费者获取消息采用的是订阅者模式，即消费者客户端可以任意订阅一个或者多个话题来消费消息:\npublic class Consumer { public static void main(String[] args) throws InterruptedException, MQClientException { /* * 订阅一个或者多个话题 */ consumer.subscribe(\u0026#34;TopicTest\u0026#34;, \u0026#34;*\u0026#34;); } } 当消费者客户端启动以后，其会每隔 30 秒从命名服务器查询一次用户订阅的所有话题路由信息:\npublic class MQClientInstance { private void startScheduledTask() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { // 从命名服务器拉取话题信息  MQClientInstance.this.updateTopicRouteInfoFromNameServer(); } }, 10, this.clientConfig.getPollNameServerInterval(), TimeUnit.MILLISECONDS); } } 我们由 RocketMQ 消息发送流程 这篇文章知道 RocketMQ 在发送消息的时候，每条消息会以轮循的方式均衡地分发的不同 Broker 的不同队列中去。由此，消费者客户端从服务器命名服务器获取下来的便是话题的所有消息队列:\n在获取话题路由信息的时候，客户端还会将话题路由信息中的所有 Broker 地址保存到本地:\npublic class MQClientInstance { public boolean updateTopicRouteInfoFromNameServer(final String topic, boolean isDefault, DefaultMQProducer defaultMQProducer) { // ...  if (changed) { TopicRouteData cloneTopicRouteData = topicRouteData.cloneTopicRouteData(); // 更新 Broker 地址列表  for (BrokerData bd : topicRouteData.getBrokerDatas()) { this.brokerAddrTable.put(bd.getBrokerName(), bd.getBrokerAddrs()); } return true; } // ...  } } 当消费者客户端获取到了 Broker 地址列表之后，其便会每隔 30 秒给服务器发送一条心跳数据包，告知所有 Broker 服务器这台消费者客户端的存在。在每次发送心跳包的同时，其数据包内还会捎带这个客户端消息订阅的一些组信息，比如用户订阅了哪几个话题等，与此相对应，每台 Broker 服务器会在内存中维护一份当前所有的消费者客户端列表信息:\npublic class ConsumerManager { private final ConcurrentMap\u0026lt;String/* Group */, ConsumerGroupInfo\u0026gt; consumerTable = new ConcurrentHashMap\u0026lt;String, ConsumerGroupInfo\u0026gt;(1024); } 消费者客户端与 Broker 服务器进行沟通的整体流程如下图所示：\n二、消息队列负载均衡 我们知道无论发送消息还是接受消息都需要指定消息的话题，然而实际上消息在 Broker 服务器上并不是以话题为单位进行存储的，而是采用了比话题更细粒度的队列来进行存储的。当你发送了 10 条相同话题的消息，这 10 条话题可能存储在了不同 Broker 服务器的不同队列中。由此，我们说 RocketMQ 管理消息的单位不是话题，而是队列。\n当我们讨论消息队列负载均衡的时候，就是在讨论服务器端的所有队列如何给所有消费者消费的问题。在 RocketMQ 中，客户端有两种消费模式，一种是广播模式，另外一种是集群模式。\n我们现在假设总共有两台 Broker 服务器，假设用户使用 Producer 已经发送了 8 条消息，这 8 条消息现在均衡的分布在两台 Broker 服务器的 8 个队列中，每个队列中有一个消息。现在有 3 台都订阅了 Test 话题的消费者实例，我们来看在不同消费模式下，不同的消费者会收到哪几条消息。\n(1) 广播模式 广播模式是指所有消息队列中的消息都会广播给所有的消费者客户端，如下图所示，每一个消费者都能收到这 8 条消息:\n(2) 集群模式 集群模式是指所有的消息队列会按照某种分配策略来分给不同的消费者客户端，比如消费者 A 消费前 3 个队列中的消息，消费者 B 消费中间 3 个队列中的消息等等。我们现在着重看 RocketMQ 为我们提供的三个比较重要的消息队列分配策略:\n1. 平均分配策略 平均分配策略下，三个消费者的消费情况如下所示：\n Consumer-1 消费前 3 个消息队列中的消息 Consumer-2 消费中间 3 个消息队列中的消息 Consumer-3 消费最后 2 个消息队列中的消息  2. 平均分配轮循策略 平均分配轮循策略下，三个消费者的消费情况如下所示：\n Consumer-1 消费 1、4、7消息队列中的消息 Consumer-2 消费 2、5、8消息队列中的消息 Consumer-3 消费 3、6消息队列中的消息  3. 一致性哈希策略 一致性哈希算法是根据这三台消费者各自的某个有代表性的属性(我们假设就是客户端ID)来计算出三个 Hash 值，此处为了减少由于 Hash 函数选取的不理想的情况， RocketMQ 算法对于每个消费者通过在客户端ID后面添加 1、2、3 索引来使每一个消费者多生成几个哈希值。那么现在我们需要哈希的就是九个字符串:\n Consumer-1-1 Consumer-1-2 Consumer-1-3 Consumer-2-1 Consumer-2-2 Consumer-2-3 Consumer-3-1 Consumer-3-2 Consumer-3-3  计算完这 9 个哈希值以后，我们按照从小到大的顺序来排列成一个环 (如图所示)。这个时候我们需要一一对这 8 个消息队列也要计算一下 Hash 值，当 Hash 值落在两个圈之间的时候，我们就选取沿着环的方向的那个节点作为这个消息队列的消费者。如下图所示 (注意: 图只是示例，并非真正的消费情况):\n在一致性哈希策略下，三个消费者的消费情况如下所示：\n Consumer-1 消费 1、2、3、4消息队列中的消息 Consumer-2 消费 5、8消息队列中的消息 Consumer-3 消费 6、7消息队列中的消息  消息队列的负载均衡是由一个不停运行的均衡服务来定时执行的:\npublic class RebalanceService extends ServiceThread { // 默认 20 秒一次  private static long waitInterval = Long.parseLong(System.getProperty(\u0026#34;rocketmq.client.rebalance.waitInterval\u0026#34;, \u0026#34;20000\u0026#34;)); @Override public void run() { while (!this.isStopped()) { this.waitForRunning(waitInterval); // 重新执行消息队列的负载均衡  this.mqClientFactory.doRebalance(); } } } 接着往下看，会知道在广播模式下，当前这台消费者消费和话题相关的所有消息队列，而集群模式会先按照某种分配策略来进行消息队列的分配，得到的结果就是当前这台消费者需要消费的消息队列:\npublic abstract class RebalanceImpl { private void rebalanceByTopic(final String topic, final boolean isOrder) { switch (messageModel) { // 广播模式  case BROADCASTING: { // 消费这个话题的所有消息队列  Set\u0026lt;MessageQueue\u0026gt; mqSet = this.topicSubscribeInfoTable.get(topic); if (mqSet != null) { // ...  } break; } // 集群模式  case CLUSTERING: { // ...  // 按照某种负载均衡策略进行消息队列和消费客户端之间的分配  // allocateResult 就是当前这台消费者被分配到的消息队列  allocateResult = strategy.allocate( this.consumerGroup, this.mQClientFactory.getClientId(), mqAll, cidAll); // ...  } break; } } } 三、Broker 消费队列文件 现在我们再来看 Broker 服务器端。首先我们应该知道，消息往 Broker 存储就是在向 CommitLog 消息文件中写入数据的一个过程。在 Broker 启动过程中，其会启动一个叫做 ReputMessageService 的服务，这个服务每隔 1 秒会检查一下这个 CommitLog 是否有新的数据写入。ReputMessageService 自身维护了一个偏移量 reputFromOffset，用以对比和 CommitLog 文件中的消息总偏移量的差距。当这两个偏移量不同的时候，就代表有新的消息到来了:\nclass ReputMessageService extends ServiceThread { private volatile long reputFromOffset = 0; private boolean isCommitLogAvailable() { // 看当前有没有新的消息到来  return this.reputFromOffset \u0026lt; DefaultMessageStore.this.commitLog.getMaxOffset(); } @Override public void run() { while (!this.isStopped()) { try { Thread.sleep(1); this.doReput(); } catch (Exception e) { DefaultMessageStore.log.warn(this.getServiceName() + \u0026#34; service has exception. \u0026#34;, e); } } } } 在有新的消息到来之后，doReput() 函数会取出新到来的所有消息，每一条消息都会封装为一个 DispatchRequest 请求，进而将这条请求分发给不同的请求消费者，我们在这篇文章中只会关注利用消息创建消费队列的服务 CommitLogDispatcherBuildConsumeQueue:\nclass ReputMessageService extends ServiceThread { // ... 部分代码有删减  private void doReput() { SelectMappedBufferResult result = DefaultMessageStore.this.commitLog.getData(reputFromOffset); if (result != null) { this.reputFromOffset = result.getStartOffset(); for (int readSize = 0; readSize \u0026lt; result.getSize() \u0026amp;\u0026amp; doNext; ) { // 读取一条消息，然后封装为 DispatchRequest  DispatchRequest dispatchRequest = DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false); int size = dispatchRequest.getMsgSize(); if (dispatchRequest.isSuccess()) { // 分发这个 DispatchRequest 请求  DefaultMessageStore.this.doDispatch(dispatchRequest); this.reputFromOffset += size; readSize += size; } // ...  } } } } CommitLogDispatcherBuildConsumeQueue 服务会根据这条请求按照不同的队列 ID 创建不同的消费队列文件，并在内存中维护一份消费队列列表。然后将 DispatchRequest 请求中这条消息的消息偏移量、消息大小以及消息在发送时候附带的标签的 Hash 值写入到相应的消费队列文件中去。\n消费队列文件的创建与消息存储 CommitLog 文件的创建过程是一致的，只是路径不同，这里不再赘述。\n寻找消费队列的代码如下:\npublic class DefaultMessageStore implements MessageStore { private final ConcurrentMap\u0026lt;String/* topic */, ConcurrentMap\u0026lt;Integer/* queueId */, ConsumeQueue\u0026gt;\u0026gt; consumeQueueTable; public void putMessagePositionInfo(DispatchRequest dispatchRequest) { ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); cq.putMessagePositionInfoWrapper(dispatchRequest); } } 向消费队列文件中存储数据的代码如下:\npublic class ConsumeQueue { private boolean putMessagePositionInfo(final long offset, final int size, final long tagsCode, final long cqOffset) { // 存储偏移量、大小、标签码  this.byteBufferIndex.flip(); this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE); this.byteBufferIndex.putLong(offset); this.byteBufferIndex.putInt(size); this.byteBufferIndex.putLong(tagsCode); // 获取消费队列文件  final long expectLogicOffset = cqOffset * CQ_STORE_UNIT_SIZE; MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset); if (mappedFile != null) { // ...  return mappedFile.appendMessage(this.byteBufferIndex.array()); } return false; } } 以上阐述了消费队列创建并存储消息的一个过程，但是消费队列文件中的消息是需要持久化到磁盘中去的。持久化的过程是通过后台服务 FlushConsumeQueueService 来定时持久化的:\nclass FlushConsumeQueueService extends ServiceThread { private void doFlush(int retryTimes) { // ...  ConcurrentMap\u0026lt;String, ConcurrentMap\u0026lt;Integer, ConsumeQueue\u0026gt;\u0026gt; tables = DefaultMessageStore.this.consumeQueueTable; for (ConcurrentMap\u0026lt;Integer, ConsumeQueue\u0026gt; maps : tables.values()) { for (ConsumeQueue cq : maps.values()) { boolean result = false; for (int i = 0; i \u0026lt; retryTimes \u0026amp;\u0026amp; !result; i++) { // 刷新到磁盘  result = cq.flush(flushConsumeQueueLeastPages); } } } // ...  } } 上述过程体现在磁盘文件的变化如下图所示，commitLog 文件夹下面存放的是完整的消息，来一条消息，向文件中追加一条消息。同时，根据这一条消息属于 TopicTest 话题下的哪一个队列，又会往相应的 consumequeue 文件下的相应消费队列文件中追加消息的偏移量、消息大小和标签码:\n总流程图如下所示:\n四、消息队列偏移量 Broker 服务器存储了各个消费队列，客户端需要消费每个消费队列中的消息。消费模式的不同，每个客户端所消费的消息队列也不同。那么客户端如何记录自己所消费的队列消费到哪里了呢？答案就是消费队列偏移量。\n针对同一话题，在集群模式下，由于每个客户端所消费的消息队列不同，所以每个消息队列已经消费到哪里的消费偏移量是记录在 Broker 服务器端的。而在广播模式下，由于每个客户端分配消费这个话题的所有消息队列，所以每个消息队列已经消费到哪里的消费偏移量是记录在客户端本地的。\n下面分别讲述两种模式下偏移量是如何获取和更新的:\n(1) 集群模式 在集群模式下，消费者客户端在内存中维护了一个 offsetTable 表:\npublic class RemoteBrokerOffsetStore implements OffsetStore { private ConcurrentMap\u0026lt;MessageQueue, AtomicLong\u0026gt; offsetTable = new ConcurrentHashMap\u0026lt;MessageQueue, AtomicLong\u0026gt;(); } 同样在 Broker 服务器端也维护了一个偏移量表:\npublic class ConsumerOffsetManager extends ConfigManager { private ConcurrentMap\u0026lt;String/* topic@group */, ConcurrentMap\u0026lt;Integer, Long\u0026gt;\u0026gt; offsetTable = new ConcurrentHashMap\u0026lt;String, ConcurrentMap\u0026lt;Integer, Long\u0026gt;\u0026gt;(512); } 在消费者客户端，RebalanceService 服务会定时地 (默认 20 秒) 从 Broker 服务器获取当前客户端所需要消费的消息队列，并与当前消费者客户端的消费队列进行对比，看是否有变化。对于每个消费队列，会从 Broker 服务器查询这个队列当前的消费偏移量。然后根据这几个消费队列，创建对应的拉取请求 PullRequest 准备从 Broker 服务器拉取消息，如下图所示:\n当从 Broker 服务器拉取下来消息以后，只有当用户成功消费的时候，才会更新本地的偏移量表。本地的偏移量表再通过定时服务每隔 5 秒同步到 Broker 服务器端:\npublic class MQClientInstance { private void startScheduledTask() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { MQClientInstance.this.persistAllConsumerOffset(); } }, 1000 * 10, this.clientConfig.getPersistConsumerOffsetInterval(), TimeUnit.MILLISECONDS); } } 而维护在 Broker 服务器端的偏移量表也会每隔 5 秒钟序列化到磁盘中:\npublic class BrokerController { public boolean initialize() throws CloneNotSupportedException { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { BrokerController.this.consumerOffsetManager.persist(); } }, 1000 * 10, this.brokerConfig.getFlushConsumerOffsetInterval(), TimeUnit.MILLISECONDS); } } 保存的格式如下所示：\n上述整体流程如下所示，红框框住的是这个话题下面的队列的 ID，箭头指向的分别是每个队列的消费偏移量：\n(2) 广播模式 对于广播模式而言，每个消费队列的偏移量肯定不能存储在 Broker 服务器端，因为多个消费者对于同一个队列的消费可能不一致，偏移量会互相覆盖掉。因此，在广播模式下，每个客户端的消费偏移量是存储在本地的，然后每隔 5 秒将内存中的 offsetTable 持久化到磁盘中。当首次从服务器获取可消费队列的时候，偏移量不像集群模式下是从 Broker 服务器读取的，而是直接从本地文件中读取的:\npublic class LocalFileOffsetStore implements OffsetStore { @Override public long readOffset(final MessageQueue mq, final ReadOffsetType type) { if (mq != null) { switch (type) { case READ_FROM_STORE: { // 本地读取  offsetSerializeWrapper = this.readLocalOffset(); // ...  } } } // ...  } } 当消息消费成功后，偏移量的更新也是持久化到本地，而非更新到 Broker 服务器中。这里提一下，在广播模式下，消息队列的偏移量默认放在用户目录下的 .rocketmq_offsets 目录下:\npublic class LocalFileOffsetStore implements OffsetStore { @Override public void persistAll(Set\u0026lt;MessageQueue\u0026gt; mqs) { // ...  String jsonString = offsetSerializeWrapper.toJson(true); MixAll.string2File(jsonString, this.storePath); // ...  } } 存储格式如下：\n简要流程图如下：\n五、拉取消息 在客户端运行着一个专门用来拉取消息的后台服务 PullMessageService，其接受每个队列创建 PullRequest 拉取消息请求，然后拉取消息:\npublic class PullMessageService extends ServiceThread { @Override public void run() { while (!this.isStopped()) { PullRequest pullRequest = this.pullRequestQueue.take(); if (pullRequest != null) { this.pullMessage(pullRequest); } } } } 每一个 PullRequest 都关联着一个 MessageQueue 和一个 ProcessQueue，在 ProcessQueue 的内部还维护了一个用来等待用户消费的消息树，如下代码所示:\npublic class PullRequest { private MessageQueue messageQueue; private ProcessQueue processQueue; } public class ProcessQueue { private final TreeMap\u0026lt;Long, MessageExt\u0026gt; msgTreeMap = new TreeMap\u0026lt;Long, MessageExt\u0026gt;(); } 当真正尝试拉取消息之前，其会检查当前请求的内部缓存的消息数量、消息大小、消息阈值跨度是否超过了某个阈值，如果超过某个阈值，则推迟 50 毫秒重新执行这个请求:\npublic class DefaultMQPushConsumerImpl implements MQConsumerInner { public void pullMessage(final PullRequest pullRequest) { // ...  final ProcessQueue processQueue = pullRequest.getProcessQueue(); long cachedMessageCount = processQueue.getMsgCount().get(); long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024); // 缓存消息数量阈值，默认为 1000  if (cachedMessageCount \u0026gt; this.defaultMQPushConsumer.getPullThresholdForQueue()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } // 缓存消息大小阈值，默认为 100 MB  if (cachedMessageSizeInMiB \u0026gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } if (!this.consumeOrderly) { // 最小偏移量和最大偏移量的阈值跨度，默认为 2000 偏移量，消费速度不能太慢  if (processQueue.getMaxSpan() \u0026gt; this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } } // ...  } } 当执行完一些必要的检查之后，客户端会将用户指定的过滤信息以及一些其它必要消费字段封装到请求信息体中，然后才开始从 Broker 服务器拉取这个请求从当前偏移量开始的消息，默认一次性最多拉取 32 条，服务器返回的响应会告诉客户端这个队列下次开始拉取时的偏移量。客户端每次都会注册一个 PullCallback 回调，用以接受服务器返回的响应信息，根据响应信息的不同状态信息，然后修正这个请求的偏移量，并进行下次请求:\npublic void pullMessage(final PullRequest pullRequest) { PullCallback pullCallback = new PullCallback() { @Override public void onSuccess(PullResult pullResult) { if (pullResult != null) { // ...  switch (pullResult.getPullStatus()) { case FOUND: // ...  break; case NO_NEW_MSG: // ...  break; case NO_MATCHED_MSG: // ...  break; case OFFSET_ILLEGAL: // ...  break; default: break; } } } @Override public void onException(Throwable e) { // ...  } }; } 上述是客户端拉取消息时的一些机制，现在再说一下 Broker 服务器端与此相对应的逻辑。\n服务器在收到客户端的请求之后，会根据话题和队列 ID 定位到对应的消费队列。然后根据这条请求传入的 offset 消费队列偏移量，定位到对应的消费队列文件。偏移量指定的是消费队列文件的消费下限，而最大上限是由如下算法来进行约束的:\nfinal int maxFilterMessageCount = Math.max(16000, maxMsgNums * ConsumeQueue.CQ_STORE_UNIT_SIZE); 有了上限和下限，客户端便会开始从消费队列文件中取出每个消息的偏移量和消息大小，然后再根据这两个值去 CommitLog 文件中寻找相应的完整的消息，并添加到最后的消息队列中，精简过的代码如下所示：\npublic class DefaultMessageStore implements MessageStore { public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset, final int maxMsgNums, final MessageFilter messageFilter) { // ...  ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId); if (consumeQueue != null) { // 首先根据消费队列的偏移量定位消费队列  SelectMappedBufferResult bufferConsumeQueue = consumeQueue.getIndexBuffer(offset); if (bufferConsumeQueue != null) { try { status = GetMessageStatus.NO_MATCHED_MESSAGE; // 最大消息长度  final int maxFilterMessageCount = Math.max(16000, maxMsgNums * ConsumeQueue.CQ_STORE_UNIT_SIZE); // 取消息  for (; i \u0026lt; bufferConsumeQueue.getSize() \u0026amp;\u0026amp; i \u0026lt; maxFilterMessageCount; i += ConsumeQueue.CQ_STORE_UNIT_SIZE) { long offsetPy = bufferConsumeQueue.getByteBuffer().getLong(); int sizePy = bufferConsumeQueue.getByteBuffer().getInt(); // 根据消息的偏移量和消息的大小从 CommitLog 文件中取出一条消息  SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy); getResult.addMessage(selectResult); status = GetMessageStatus.FOUND; } // 增加下次开始的偏移量  nextBeginOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE); } finally { bufferConsumeQueue.release(); } } } // ...  } } 客户端和 Broker 服务器端完整拉取消息的流程图如下所示：\n六、消费消息 依赖于用户指定的消息回调函数的不同，消息的消费分为两种: 并发消费和有序消费。\n并发消费没有考虑消息发送的顺序，客户端从服务器获取到消息就会直接回调给用户。而有序消费会考虑每个队列消息发送的顺序，注意此处并不是每个话题消息发送的顺序，一定要记住 RocketMQ 控制消息的最细粒度是消息队列。当我们讲有序消费的时候，就是在说对于某个话题的某个队列，发往这个队列的消息，客户端接受消息的顺序与发送的顺序完全一致。\n下面我们分别看这两种消费模式是如何实现的。\n(1) 并发消费 当用户注册消息回调类的时候，如果注册的是 MessageListenerConcurrently 回调类，那么就认为用户不关心消息的顺序问题。我们在上文提到过每个 PullRequest 都关联了一个处理队列 ProcessQueue，而每个处理队列又都关联了一颗消息树 msgTreeMap。当客户端拉取到新的消息以后，其先将消息放入到这个请求所关联的处理队列的消息树中，然后提交一个消息消费请求，用以回调用户端的代码消费消息:\npublic class DefaultMQPushConsumerImpl implements MQConsumerInner { public void pullMessage(final PullRequest pullRequest) { PullCallback pullCallback = new PullCallback() { @Override public void onSuccess(PullResult pullResult) { if (pullResult != null) { switch (pullResult.getPullStatus()) { case FOUND: // 消息放入处理队列的消息树中  boolean dispathToConsume = processQueue .putMessage(pullResult.getMsgFoundList()); // 提交一个消息消费请求  DefaultMQPushConsumerImpl.this .consumeMessageService .submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispathToConsume); break; } } } }; } } 当提交一个消息消费请求后，对于并发消费，其实现如下:\npublic class ConsumeMessageConcurrentlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { // ...  status = listener.consumeMessage(Collections.unmodifiableList(msgs), context); // ...  } } } 我们可以看到 msgs 是直接从服务器端拿到的最新消息，直接喂给了客户端进行消费，并未做任何有序处理。当消费成功后，会从消息树中将这些消息再给删除掉:\npublic class ConsumeMessageConcurrentlyService implements ConsumeMessageService { public void processConsumeResult(final ConsumeConcurrentlyStatus status, /** 其它参数 **/) { // 从消息树中删除消息  long offset = consumeRequest.getProcessQueue().removeMessage(consumeRequest.getMsgs()); if (offset \u0026gt;= 0 \u0026amp;\u0026amp; !consumeRequest.getProcessQueue().isDropped()) { this.defaultMQPushConsumerImpl.getOffsetStore() .updateOffset(consumeRequest.getMessageQueue(), offset, true); } } } (2) 有序消费 RocketMQ 的有序消费主要依靠两把锁，一把是维护在 Broker 端，一把维护在消费者客户端。Broker 端有一个 RebalanceLockManager 服务，其内部维护了一个 mqLockTable 消息队列锁表:\npublic class RebalanceLockManager { private final ConcurrentMap\u0026lt;String/* group */, ConcurrentHashMap\u0026lt;MessageQueue, LockEntry\u0026gt;\u0026gt; mqLockTable = new ConcurrentHashMap\u0026lt;String, ConcurrentHashMap\u0026lt;MessageQueue, LockEntry\u0026gt;\u0026gt;(1024); } 在有序消费的时候，Broker 需要确保任何一个队列在任何时候都只有一个客户端在消费它，都在被一个客户端所锁定。当客户端在本地根据消息队列构建 PullRequest 之前，会与 Broker 沟通尝试锁定这个队列，另外当进行有序消费的时候，客户端也会周期性地 (默认是 20 秒) 锁定所有当前需要消费的消息队列:\npublic class ConsumeMessageOrderlyService implements ConsumeMessageService { public void start() { if (MessageModel.CLUSTERING.equals(ConsumeMessageOrderlyService.this.defaultMQPushConsumerImpl.messageModel())) { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { ConsumeMessageOrderlyService.this.lockMQPeriodically(); } }, 1000 * 1, ProcessQueue.REBALANCE_LOCK_INTERVAL, TimeUnit.MILLISECONDS); } } } 由上述这段代码也能看出，只在集群模式下才会周期性地锁定 Broker 端的消息队列，因此在广播模式下是不支持进行有序消费的。\n而在 Broker 这端，每个客户端所锁定的消息队列对应的锁项 LogEntry 有一个上次锁定时的时间戳，当超过锁的超时时间 (默认是 60 秒) 后，也会判定这个客户端已经不再持有这把锁，以让其他客户端能够有序消费这个队列。\n在前面我们说到过 RebalanceService 均衡服务会定时地依据不同消费者数量分配消费队列。我们假设 Consumer-1 消费者客户端一开始需要消费 3 个消费队列，这个时候又加入了 Consumer-2 消费者客户端，并且分配到了 MessageQueue-2 消费队列。当 Consumer-1 内部的均衡服务检测到当前消费队列需要移除 MessageQueue-2 队列，这个时候，会首先解除 Broker 端的锁，确保新加入的 Consumer-2 消费者客户端能够成功锁住这个队列，以进行有序消费。\npublic abstract class RebalanceImpl { private boolean updateProcessQueueTableInRebalance(final String topic, final Set\u0026lt;MessageQueue\u0026gt; mqSet, final boolean isOrder) { while (it.hasNext()) { // ...  if (mq.getTopic().equals(topic)) { // 当前客户端不需要处理这个消息队列了  if (!mqSet.contains(mq)) { pq.setDropped(true); // 解锁  if (this.removeUnnecessaryMessageQueue(mq, pq)) { // ...  } } // ...  } } } } 消费者客户端每一次拉取消息请求，如果有发现新的消息，那么都会将这些消息封装为 ConsumeRequest 来喂给消费线程池，以待消费。如果消息特别多，这样一个队列可能有多个消费请求正在等待客户端消费，用户可能会先消费偏移量大的消息，后消费偏移量小的消息。所以消费同一队列的时候，需要一把锁以消费请求顺序化:\npublic class ConsumeMessageOrderlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { final Object objLock = messageQueueLock.fetchLockObject(this.messageQueue); synchronized (objLock) { // ...  } } } } RocketMQ 的消息树是用 TreeMap 实现的，其内部基于消息偏移量维护了消息的有序性。每次消费请求都会从消息树中拿取偏移量最小的几条消息 (默认为 1 条)给用户，以此来达到有序消费的目的:\npublic class ConsumeMessageOrderlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { // ...  final int consumeBatchSize = ConsumeMessageOrderlyService.this .defaultMQPushConsumer .getConsumeMessageBatchMaxSize(); List\u0026lt;MessageExt\u0026gt; msgs = this.processQueue.takeMessags(consumeBatchSize); } } } 扫描下面二维码，在手机端阅读：\n"});index.add({'id':65,'href':'/docs/books/clean_code/','title':"代码整洁之道",'content':"代码整洁之道 勒布朗法则：Later equals never.\n随着混乱的增加，团队生产力也持续下降，趋近于零。生产力下降的时候，管理层只能增加更多的人手，期望提高生产力。\n什么是整洁代码  我喜欢优雅和高效的代码。代码逻辑应当直截了当，叫缺陷难以隐藏；尽量减少依赖关系，使之便于维护；依据某种分层战略完善错误处理代码；性能调至最优，省得引诱别人做没规矩的优化，搞出一堆混乱来。整洁的代码只做好一件事。\u0026mdash; Bjarne Stroustrup，C++ 语言发明者\n  整洁的代码应可由作者之外的开发者阅读和增补。它应有单元测试和验收测试。它使用有意义的命名。它只提供一种而非多种做一件事的途径。它只有尽量少的依赖关系，且要明确地定义和提供清晰、尽量少的 API。代码应通过其表面表达含义，因为不同的语言导致并非所有必需信息均可通过代码自身清晰表达。\u0026mdash; Dave Thomas, OTI 公司创始人\n  整洁的代码总是看起来像是某位特别在意它的人写的。几乎没有改进的余地，代码作者什么都想到了。\u0026mdash; 《修改代码的艺术》作者\n 有意义的命名 对于变量，如果其需要注释来补充，那就不算是名副其实。比如你需要定义一个变量，这个变量存储的是消逝的时间，其单位是天，那么下面是一些比较好的命名：\nint elapsedTimeInDays; int daysSinceCreation; int daysSinceModification; int fileAgeInDays; 别用 accountList 来指一组账号，除非它真的是 List 类型，List 一词对于程序员有特殊意义，所以用 accountGroup 或 bunchOfAcounts，甚至用 accounts 都会好一些。\n别说废话，废话都是冗余。假如你有一个 Product 类，如果还有一个 ProductInfo 或 ProductData 类，它们虽然名称不同，意思却无区别。Info 和 Data 就像 a、an 和 the 一样，是意义含混的废话。下面三个函数的命名，我们怎么知道应该调用哪个呢？\ngetActiveAccount(); getActiveAccounts(); getActiveAccountInfo(); 使用常量，WORK_DAYS_PER_WEEK 比数字 5 要好找的多。\n 对于类名，其应该是名词或名词短语，如 Customer、WikiPage、Account 和 AddressParser，避免使用 Manager、Processor、Data 或 Info 这样的类名。类名不应当是动词。\n 对于方法名，其应当是动词或动词短语，如 postPayment、deletePage 或 save。\n 为每一个抽象概念选一个词，并且一以贯之。例如使用 fetch、retrieve 和 get 来给在多个类中的同种方法命名，你怎么记得住哪个类是哪个方法呢？在一堆代码中，有 controller，又有 manager，还有 driver，就会令人困惑。\n多数变量都依赖一个类、一个函数来给读者提供语境，但如果做不到的话，你可能就需要加上前缀。例如 addrFirstName 比 firstName 更能说明，你想表达的是地址的一部分，当然更好的方案是创建一个名为 Address 的类。当然也没必要添加不必要的语境，只要短名称足够清楚，就比长名称好。\n语境不明确的变量  有语境的变量   如何写好函数 函数的第一个规则是短小。第二条规则还是要短小。\n函数应该做一件事，做好这件事，只做这一件事。如何判断函数做了是否不止一件事，看是否能再拆出一个函数。要确保函数只做一件事，函数中的语句都要在同一抽象层级上。getHtml() 位于较高抽象层级，PathParser.render(pagePath) 位于中间抽象层，.append(\u0026quot;\\n\u0026quot;) 位于相当低的抽象层。函数中混杂了不同的抽象层级，往往容易让人迷惑，读者无法判断出某个表达式是基础概念还是细节。\n像如下带有 switch 函数的代码，有几个问题。太长、违反单一原则、违反开放闭合原则（添加新类型，必须修改）等，该问题的解决方案是将 switch 语句埋到抽象工厂底下，不让任何人看到。\nSwitch 语句  用多态封装 Switch 语句   好名称的价值怎么好评都不为过，别害怕长名称，长而具有描述性的名称，要比短而令人费解的名称好，要比描述性的长注释好。别害怕花时间取名字。\n关于函数参数，除非你有足够特殊的理由，才能用三个以上的参数。对于有一个参数的函数，如果要对这个参数进行某种转换操作，那么应该使用返回值来返回转换后的值：StringBuffer transform(StringBuffer in) 要比 void transform(StringBuffer out) 强。\n如果函数看来需要两个、三个或三个以上的参数，说明其中一些参数就需要封装为类了：\nCircle makeCircle(double x, double y, double radius); Circle makeCircle(Point center, double radius); 给函数起一个好名字，能够解释函数意图、参数顺序的名字。writeField(name) 要比 write(name) 强，assertExpectedEqualsActual(expected, actual) 要比 assertEqual 强，这大大减轻了记忆参数的负担。\n确保函数无副作用，函数承诺做这件事，不要在其内部偷偷地做其它事情。\ntry/catch 代码块丑陋不堪，最好把 try 和 catch 代码块的主题部分抽离出来，另外形成函数。错误处理本身就是一件事，这意味着在 try 应该是函数的第一个单词，catch/finally 是这个函数的最后的内容。\n注释 代码在变动，在演化，但注释不能总是随之变动，注释会撒谎。注释不能美化糟糕的代码。\n直接把代码注释掉是讨厌的做法，其他人不敢删除注释掉的代码，他们会想代码依然放在那儿，一定有其原因。\n格式 代码每行展现一个表达式或一个子句，每组代码行展示一条完整的思路。这些思路用空白行区隔开来。每个空白行都是一条线索，标识出新的独立概念。往下读代码时，你的目光总会停留于空白行之后的那一行。\n若某个函数调用了另外一个，就应该把他们放到一起，而且调用者应该尽可能放在被调用者上面，这样，程序有一个自然的顺序。\n对象和数据结构 乱加 set 和 get 时最坏的选择，不要暴露数据细节，而要以抽象形态表述数据。\n暴露了数据细节的车辆  百分比抽象   过程式代码便于在不改动现有数据结构的前提下添加新的函数，面向对象代码便于在不改动现有函数的前提下添加新的类。\nThe Law of Demeter 认为模块不应了解它所操作对象的内部情形，对象应该隐藏数据，暴露操作。下面代码违反了：\nfinal String outputDir = ctxt.getOptions().getScratchDir().getAbsolutePath(); 最为精炼的数据结构，是一个只有公共变量、没有函数的类，这种数据结构就是 DTO（Data Transfer Objects），这种数据结构在与数据库通信、解析套接字传递的消息之类场景中，非常有用。\n错误处理 使用 Checked Exception 的依赖成本要高于收益，每个调用该函数的函数都要捕获它，或者添加合适的 throw 语句，最终得到的时一个从软件最底端贯穿到最高端的修改链，封装被打破，抛出路径上的每个函数都要去了解下一层的异常细节。\n将第三方 API 打包是个良好的实践手段，降低了对它的依赖，未来可以不太痛苦地改用其它代码库，你也可以不必绑死在某个特定厂商的 API 设计上。\n返回 null 的时候，考虑是否可以直接抛出异常，或者返回一个特定的对象，尽量不要返回 null，它在给调用者添乱。返回 null 是糟糕的做法，那么传递 null 值给其它方法就是更糟糕的了。\n单元测试 测试带来一切好处。\n类 系统应该由许多短小的类而不是少量巨大的类组成。\n对类加以组织，可以降低修改的风险。\n一个必须打开修改的类  一组封闭类   "});index.add({'id':66,'href':'/docs/tutorial/sentinel/spi/','title':"可扩展性",'content':"可扩展性 SpiLoader public final class SpiLoader { public static \u0026lt;T\u0026gt; T loadFirstInstanceOrDefault(Class\u0026lt;T\u0026gt; clazz, Class\u0026lt;? extends T\u0026gt; defaultClass) { AssertUtil.notNull(clazz, \u0026#34;SPI class cannot be null\u0026#34;); AssertUtil.notNull(defaultClass, \u0026#34;default SPI class cannot be null\u0026#34;); try { String key = clazz.getName(); // Not thread-safe, as it\u0026#39;s expected to be resolved in a thread-safe context.  ServiceLoader\u0026lt;T\u0026gt; serviceLoader = SERVICE_LOADER_MAP.get(key); if (serviceLoader == null) { serviceLoader = ServiceLoaderUtil.getServiceLoader(clazz); SERVICE_LOADER_MAP.put(key, serviceLoader); } for (T instance : serviceLoader) { if (instance.getClass() != defaultClass) { return instance; } } return defaultClass.newInstance(); } catch (Throwable t) { RecordLog.error(\u0026#34;[SpiLoader] ERROR: loadFirstInstanceOrDefault failed\u0026#34;, t); t.printStackTrace(); return null; } } } "});index.add({'id':67,'href':'/docs/it-zone/2020-06/chrome-change-blacklist-to-blocklist/','title':"谷歌修改 Chromium 源码中的“黑白名单”术语",'content':"谷歌修改 Chromium 源码中的“黑白名单”术语 日期：2020-06-09\n 【为了种族中立，谷歌修改 Chromium 源码中的“黑白名单”术语】\n国外正在进行的 Black Lives Matter 运动，谷歌已表态支持。\n据外媒报道，Google 在修改 Chromium 源码中的有种族歧视色彩的术语，来消除微妙的种族主义形式。\n blacklist 改成 blocklist， whitelist 改成 allowlist；\n 2019 年 10 月，Chromium 开源项目的官方代码风格指南中，新增了如何编写种族中立代码的内容。其中明确指出，Chrome 和 Chromium 开发人员应避免使用“黑名单”和“白名单”一词，而应使用中性术语“阻止名单”和“允许名单”。\n其实早在 2018 年 5 月，Google 已开始着手删除普通用户在 Chrome 浏览器中能看到的“黑名单”和“白名单”。\n但普通用户看不到的 Chrome / Chromium 源码中，还有很多很多，据统计约 2000 多处。\n"});index.add({'id':68,'href':'/posts/java-lock/','title':"Java 并发 - 锁",'content':"Java 世界中都有哪些锁？锁的分类？如何减少锁的竞争等问题。\n线程安全的三种实现方式 互斥同步 (Blocking Synchronization)，属于悲观并发策略:\n非阻塞同步 (Non-Blocking Synchronization)，属于乐观并发策略:\n无同步 - 线程本地存储 (Thread Local Storage):\nJava 主内存与工作内存交互 从主内存读取变量到工作内存:\n将工作内存的变量写入到主内存:\n内置锁 Synchronized Java 提供了一种内置锁 (Intrinsic Lock)机制来支持原子性: 同步代码块 (Synchronized Block)。每个 Java 对象都可以用做一个实现同步的锁，这些锁被称为内置锁 (Instrinsic Lock) 或监视器锁 (Monitor Lock)。Java 的内置锁相当于一种互斥体(或互斥锁)，这意味着最多只有一个线程能持有这种锁。但是，加锁的含义不仅仅局限于互斥行为，还包括内存可见性，为了确保所有线程都能看到共享变量的最新值，所有执行读操作或者写操作的线程都必须在同一个锁上同步。\nJava synchronized 语句 是基于 monitorenter/monitorexit 机制来实现的。当你写下面这段代码的时候:\nstatic void Sort(int [] array) { // synchronize this operation so that some other thread can\u0026#39;t  // manipulate the array while we are sorting it. This assumes that other  // threads also synchronize their accesses to the array.  synchronized(array) { // now sort elements in array  } } 实际上 JVM 可能会生成下面的代码:\n.method static Sort([I)V aload_0 monitorenter ; lock object in local variable 0 ; now sort elements in array aload_0 monitorexit ; finished with object in local variable 0 return .end method monitorenter 在对象的引用上获取了一个 exclusive lock (独占锁)\n 内置锁 synchronized 是可重入的，某个线程试图获取一个已经由它自己持有的锁，那么这个请求就会成功。\n死锁 在数据库系统的设计中考虑了监测死锁以及从死锁中恢复。在执行一个事务 (Transaction) 时可能需要获取多个锁，并一直持有这些锁直到事务提交。当数据库服务器监测到一组事务发生了死锁时 (通过在表示等待关系的有向图中搜索循环)，将 选择一个牺牲者并放弃这个事务。作为牺牲者的事务会释放它所持有的资源，从而让其它事务继续进行。应用程序可以重新执行被强行中止的事务，而这个事务现在也可以成功完成。\n死锁的四大必要条件 (必须全部满足):\n 互斥 持有并等待资源 不可抢占 循环等待  如果所有的线程以固定的顺序来获得锁，那么在程序中就不会出现锁顺序死锁问题:\n// 不要这么做 public class LeftRightDeadlock { private final Object left = new Object(); private final Object right = new Object(); public void leftRight() { synchronized (left) { synchronized (right) { doSomething(); } } } public void rightLeft() { synchronized (right) { synchronized (left) { doSomethingElse(); } } } } 有时候，你并不能清除地知道是否在锁顺序上有足够的控制权来避免死锁的发生:\n// 动态的锁顺序 // Warning: deadlock-prone! public void transferMoney(Account fromAccount, Account toAccount, DollarAmount amount) throws InsufficientFundsException { synchronized (fromAccount) { synchronized (toAccount) { if (fromAccount.getBalance().compareTo(amount) \u0026lt; 0) throw new InsufficientFundsException(); else { fromAccount.debit(amount); toAccount.credit(amount); } } } } 在这里锁的顺序取决于参数顺序，而这些参数顺序又取决于外部输入，考虑下面代码就有可能发生死锁:\nA: transferMoney(myAccount, yourAccount, 10); B: transferMoney(yourAccount, myAccount, 20); 使用 System.identityHashCode 来定义锁的顺序:\nprivate static final Object tieLock = new Object(); public void transferMoney(final Account fromAcct, final Account toAcct, final DollarAmount amount) throws InsufficientFundsException { class Helper { public void transfer() throws InsufficientFundsException { if (fromAcct.getBalance().compareTo(amount) \u0026lt; 0) throw new InsufficientFundsException(); else { fromAcct.debit(amount); toAcct.credit(amount); } } } // 如果 Account 中包含一个唯一的、不可变的，并且具备可比性的键值，例如  // 账号，那么制定锁的顺序就更加容易了。  int fromHash = System.identityHashCode(fromAcct); int toHash = System.identityHashCode(toAcct); if (fromHash \u0026lt; toHash) { synchronized (fromAcct) { synchronized (toAcct) { new Helper().transfer(); } } } else if (fromHash \u0026gt; toHash) { synchronized (toAcct) { synchronized (fromAcct) { new Helper().transfer(); } } } else { // 在极少数情况下，两个对象可能拥有相同的散列值，  // 此时可以通过某种任意的方法来决定锁的顺序，  // 而这有可能重新引入死锁。为了避免这种情况，可以使用  // “加时赛”锁  synchronized (tieLock) { synchronized (fromAcct) { synchronized (toAcct) { new Helper().transfer(); } } } } } 某些获取多个锁的操作并不像 LeftRightDeadLock 或 transferMoney 中那么明显，这两个锁并不一定必须在同一个方法中被获取:\nclass Taxi { @GuardedBy(\u0026#34;this\u0026#34;) private Point location, destination; private final Dispatcher dispatcher; public Taxi(Dispatcher dispatcher) { this.dispatcher = dispatcher; } public synchronized Point getLocation() { return location; } // 先获取 Taxi 锁  // 再获取 Dispatcher 锁  public synchronized void setLocation(Point location) { this.location = location; if (location.equals(destination)) dispatcher.notifyAvailable(this); } } class Dispatcher { @GuardedBy(\u0026#34;this\u0026#34;) private final Set\u0026lt;Taxi\u0026gt; taxis; @GuardedBy(\u0026#34;this\u0026#34;) private final Set\u0026lt;Taxi\u0026gt; availableTaxis; public Dispatcher() { taxis = new HashSet\u0026lt;Taxi\u0026gt;(); availableTaxis = new HashSet\u0026lt;Taxi\u0026gt;(); } public synchronized void notifyAvailable(Taxi taxi) { availableTaxis.add(taxi); } // 先获取 Dispatcher 锁  // 再获取每一个 Taxi 锁  public synchronized Image getImage() { Image image = new Image(); for (Taxi t : taxis) image.drawMarker(t.getLocation()); return image; } } 通过将上述代码修改为开放调用 (调用某个方法时不需要使用锁)，从而消除发生死锁的风险:\n@ThreadSafe class Taxi { @GuardedBy(\u0026#34;this\u0026#34;) private Point location, destination; private final Dispatcher dispatcher; ... public synchronized Point getLocation() { return location; } public synchronized void setLocation(Point location) { boolean reachedDestination; synchronized (this) { this.location = location; reachedDestination = location.equals(destination); } if (reachedDestination) dispatcher.notifyAvailable(this); } } @ThreadSafe class Dispatcher { @GuardedBy(\u0026#34;this\u0026#34;) private final Set\u0026lt;Taxi\u0026gt; taxis; @GuardedBy(\u0026#34;this\u0026#34;) private final Set\u0026lt;Taxi\u0026gt; availableTaxis; ... public synchronized void notifyAvailable(Taxi taxi) { availableTaxis.add(taxi); } public Image getImage() { Set\u0026lt;Taxi\u0026gt; copy; synchronized (this) { copy = new HashSet\u0026lt;Taxi\u0026gt;(taxis); } Image image = new Image(); for (Taxi t : copy) image.drawMarker(t.getLocation()); return image; } } 在程序中应该尽量使用开放调用。与那些在持有锁时调用外部方法的程序相比，更易于对依赖于开放调用的程序进行死锁分析。通过使用定时锁能够有效地应对死锁问题，通过 Thread Dump 能够帮助你识别死锁的发生。\n减少锁的竞争 有三种方式可以降低锁的竞争程度:\n 减少锁的持有时间 降低锁的请求频率 使用带有协调机制的独占锁，这些机制允许更高的并发性  重入锁 ReentrantLock ReentrantLock 的 tryLock 方法为你提供了轮询锁与定时锁的锁获取模式，与无条件的锁获取模式相比，它具有更完善的错误恢复机制。方法 lockInterruptibly 方法能够在获得锁的同时保持对中断的响应。ReentrantLock 的构造函数中提供了两种公平性选择: 创建一个非公平的锁 (默认) 或者一个公平的锁。在公平的锁上，线程将按照它们发出请求的顺序来获得锁，但在非公平的锁上，则允许“插队”。在大多数情况下，非公平锁的性能要高于公平锁的性能。\npublic class ReentrantLock implements Lock, java.io.Serializable { /** Synchronizer providing all implementation mechanics */ private final Sync sync; abstract static class Sync extends AbstractQueuedSynchronizer { } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } public boolean tryLock() { return sync.nonfairTryAcquire(1); } public void unlock() { sync.release(1); } } 读写锁 ReadWriteLock public class ReentrantReadWriteLock implements ReadWriteLock, java.io.Serializable { public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } } 记录锁 Record Locking Record Locking 更好的叫法应该被称为: byte-range locking，目的是为了防止两个进程同时修改一个文件的某块区域。函数原型如下:\n#include \u0026lt;fcntl.h\u0026gt;// 出错返回 -1 int fcntl(int fd, int cmd, ... /* struct flock *flockptr */ ); 其中 flock 结构体定义如下:\nstruct flock { short l_type; /* F_RDLCK, F_WRLCK, or F_UNLCK */ short l_whence; /*SEEK_SET, SEEK_CUR, or SEEK_END */ off_t l_start; /*offset in bytes, relative to l_whence */ off_t l_len; /*length, in bytes; 0 means lock to EOF */ pid_t l_pid; /*returned with F_GETLK */ };  F_RDLCK: 共享读锁 F_WRLCK: 排斥写锁 F_UNLCK: 取消某个区域的锁  锁优化 自旋锁 SpinLock 互斥同步对性能最大的影响就是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来了很大的压力。同时，虚拟机的开发团队也注意到在许多应用上，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得。为了能让线程稍微等一会，我们只需让线程执行一个忙循环 (自旋)，这项技术就是所谓的自旋锁。\n现在我们假设硬件上有一种能够保证原子性的 TestAndSet 指令实现函数:\nint TestAndSet(int *x){ register int temp = *x; *x = 1; return temp; } TestAndSet 是一种常用的用于支持并发的原子操作指令。另外一种经常使用的指令是原子 Exchange 操作:\nvoid Exchange(int *a, int *b) { int temp = *a; *a = *b; *b = temp; } 这些所有的原子性操作中最重要的是 CompareAndSwap (CAS) 操作，它经常被用于 lock-free and wait-free algorithms 算法中。\nboolean CAS(int *a, int old, int new) { int temp = *a; if (temp == old) { *a = new; return true; } else return false; } 使用 CAS 来实现 temp++:\nint temp = x; while (!CAS(\u0026amp;x, temp, temp+1)) { temp = x; } 使用 CAS 来实现更链表头插法:\nwhile (1) { Node *q = *head; p-\u0026gt;next = q; if (CAS(head, q, p)) break; } 一般而言，SpinLock 是一种抽象的数据类型，其通常提供三种操作:\n InitLock Lock UnLock  Lock(mutex); Si; UnLock(mutex); 实现 SpinLock 的伪代码如下:\ntypedef int SpinLock; void InitLock(SpinLock *L) { *L = 0; } void Lock(SpinLock *L) { while (TestAndSet(L)) ; } void UnLock(SpinLock *L) { *L = 0; } 一种使用 Exchange 操作的可能实现:\ntypedef int SpinLock; void InitLock(SpinLock *s) { *s = 0; } void Lock (SpinLock *s) { int L = 1; do { Exchange(\u0026amp;L, s); } while (L == 1); } void UnLock (SpinLock *s) { *s = 0; } 另外一种使用 CompareAndSwap 指令的实现:\ntypedef int SpinLock; void InitLock(SpinLock *s) { *s = 0; } void Lock (SpinLock *s) { do { } until (CompareAndSwap(s, 0, 1)); } void UnLock (SpinLock *s) { *s = 0; } 自旋锁最大的问题就是可能会占用比较高的 memory bus 带宽，另外它也不保证公平性，即无法保证先后进入临界区的两个进程 P 和 Q 按照 FIFO 顺序来服务。\n锁消除 Lock Elimination 虚拟机 JIT 在运行时，对一些代码要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。主要判定依据来自于逃逸分析的数据支持。\n锁粗化 Lock Coarsening 轻量级锁 Lightweight Locking 偏向锁 Biased Locking 偏向锁的\u0026quot;偏\u0026rdquo;，是偏心的\u0026quot;偏\u0026rdquo;，它的意思就是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。\nJDK 1.6 默认开启 -XX:+UseBiasedLocking，使用 -XX:-UseBiasedLocking 来关闭。\n偏向锁转为轻量级锁的流程图:\n锁升级 Lock Escalation 所谓的锁升级（lock escalation），是数据库的一种作用机制，该机制普遍见于各大数据库产品。 为了节约内存的开销，其会将为数众多并占用大量资源的细粒度的锁转化为数量较少的且占用相对较少资源的粗粒度的锁，多数情况下主要指将为数众多的行锁升级为一个表锁。当然，DB2 支持很多粒度的锁，如**表空间（table space），表（table），行（row）以及索引（index）**等。MySQL 的 InnoDB 存储引擎支持事务，默认是行锁。得益于这些特性，数据库支持高并发。\n锁升级与两种事情有关:\n 事务的隔离级别 索引  常用的索引有三类：主键、唯一索引、普通索引。主键 不由分说，自带最高效的索引属性；唯一索引 指的是该属性值重复率为0，一般可作为业务主键，例如学号；普通索引 与前者不同的是，属性值的重复率大于0，不能作为唯一指定条件，例如学生姓名。当“值重复率”低时，甚至接近主键或者唯一索引的效果，“普通索引”依然是行锁；当“值重复率”高时，MySQL 不会把这个“普通索引”当做索引，即造成了一个没有索引的 SQL，此时引发表锁。索引不是越多越好，索引存在一个和这个表相关的文件里，占用硬盘空间，宁缺勿滥，每个表都有主键（id），操作能使用主键尽量使用主键。同 JVM 自动优化 java 代码一样，MySQL 也具有自动优化 SQL 的功能。低效的索引将被忽略，这也就倒逼开发者使用正确且高效的索引。\n参考  《Java 并发编程实战》 Deadlock 《Advanced Programming in the UNIX》 《深入理解 Java 虚拟机》 CIS 4307: Spinlocks and Semaphores enter synchronized region of code 关于 DB2 锁升级 (lock escalation) 相关问题的探讨 MySQL 避免行锁升级为表锁——使用高效的索引 Innodb中的事务隔离级别和锁的关系 MySQL数据库事务各隔离级别加锁情况\u0026ndash;read uncommitted篇 Thread Synchronization "});index.add({'id':69,'href':'/posts/jvm-optimization/','title':"JVM 性能调优",'content':"JVM 如何进行性能调优？\nJava 虚拟机内存模型 JVM 虚拟机将内存数据分为如下这几部分：\npc register  pc register (program counter)： 一个包含当前时刻指令的地址的寄存器  程序寄存器区域是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域\nstack 栈会抛出两种异常：StackOverflowError 和 OutOfMemoryError，在 HotSpot 虚拟机栈中，可以使用参数 -Xss1M 来设置栈的大小为 1MB。随着调用函数参数的增加和局部变量的增加，单次函数调用对栈空间的需求也会增加，因此栈的最大递归次数不是一成不变的。函数嵌套调用的次数由栈的大小决定：栈越大，函数嵌套调用次数越多；对一个函数而言，它的参数越多，内部局部变量越多，它的栈帧就越大，其嵌套调用次数就会越少。\n Xss1M: 设置栈的大小  native method stack 与 stack 一样，同样抛出两种异常：StackOverflowError 和 OutOfMemoryError。在 sun 的 HOT SPOT 虚拟机中，不区分本地方法栈和虚拟机栈\nHEAP  -Xmx: 设置堆的最大值 -Xms: 设置堆的最小值，即 JVM 启动时，所占据的操作系统内存大小。JVM 会试图将系统内存尽可能地限制在 -Xms 中，因此当内存使用量触及 -Xms 指定的大小时，会触发 Full GC。因此把 -Xms 值设置为 -Xmx 时，可以在系统运行初期减少 GC 的次数和耗时。 Xmn: 设置新生代大小。等于把 -XX:NewSize 和 -XX:MaxNewSize 设置成了相同的大小。这两个如果设置成不同的值，会导致内存震荡，产生不必要的开销。  -XX:NewSize: 设置新生代的初始大小 -XX:MaxNewSize: 设置新生代的最大值    错误的把 Xmx 参数设置为了 Xmn 参数以后:\n获取当前内存/最大可用内存/最大可用堆:\nRuntime.getRuntime().freeMemory() / 1024 / 1024 Runtime.getRuntime().totalMemory() / 1024 / 1024 Runtime.getRuntime().maxMemory() / 1000 / 1000 逃逸分析 Java 7 开始支持对象的栈分配和逃逸分析机制，这样的机制能够将堆分配对象变成栈分配对象:\nvoid myMethod() { V v = new V(); // use v  v = null; }  -server: server 模式下，才可以启用逃逸分析 -XX:DoEscapeAnalysis: 启用逃逸分析  method area 方法区主要保存的是类的元数据：类型、常量池、字段、方法。在 Hot Spot 虚拟机中，方法区也称为永久区，同样也可以被 GC 回收。持久代的大小直接决定了系统可以支持多少个类定义和多少常量。对于使用 CGLIB 或者 Javassist 等动态字节码生成工具的应用程序而言，设置合理的持久代有利于维持系统稳定。\n方法区的大小直接决定了系统可以保存多少个类，如果系统使用了一些动态代理，那么有可能会在运行时生成大量的类，如果这样，就需要设置一个合理的永久区大小，确保不发生永久区内存溢出。\n -XX:MaxPermSize=4M: 设置持久代的最大值 -XX:PermSize=4M: 设置持久代的初始大小  在 JDK 1.8 中，永久区已经被彻底移除，取而代之的是元数据区 (Metaspace)，元数据区是一块堆外的直接内存，如果不指定元数据区大小的话，默认情况下，虚拟机会耗尽所有的可用系统内存。\n -XX:MaxMetaspaceSize: 指定元数据区大小  直接内存 使用 NIO 之后，直接内存的使用变得非常普遍，直接内存跳过了 Java 堆，可以直接访问原生堆空间。直接内存适合申请次数少、访问较为频繁的场合。如果需要频繁申请，则并不适合使用直接内存。\n -XX:MaxDirectMemorySize: 最大可用直接内存，默认为 -Xmx  区域比例  -XX:SurvivorRatio=8: 设置新生代中 eden 空间 和 S0 空间 的比例关系 -XX:NewRatio=2: 设置老生代和新生代的比例  垃圾回收算法  引用计数法: 无法解决循环引用问题 标记-清除算法 (Mark-Sweep):  标记从根节点开始的可达对象 清除所有未被标记的对象 最大缺点: 回收后的空间是不连续的   复制算法 (新生代):  内存空间分为两块，每次只用一块 存活对象复制到未使用的内存块中 清除正在使用的内存块中的所有对象 交换两个内存的角色 适合于新生代: 垃圾对象通常多于存活对象   标记-压缩算法:  标记从根节点开始的可达对象 将所有存活对象 (未标记的对象) 压缩到内存的一端 清理边界外 (标记和未标记对象的边界) 的对象     分代 (Generational Collecting):  根据每块内存空间特点的不同，使用不同的回收算法。如新生代 (存活对象少，垃圾对象多) 使用复制算法，老年代 (大部分对象是存活对象) 使用标记-压缩算法    为了支持高频率的新生代回收，虚拟机可能使用一种叫做卡表 (Card Table) 的数据结构。卡表为一个比特位集合，每一个比特位可以用来表示老年代的某一区域中的所有对象是否持有新生代对象的引用。这样在新生代 GC 时，只需先扫描卡表，就能快速知道用不用扫描特定的老年代对象，而卡表为 0 的所在区域一定不含有新生代对象的引用。\n谁才是真正的垃圾  可触及性: 根节点可到达 可复活: finalize() 中复活 不可触及: finalize() 中未复活  finalize() 方法只会被调用一次\n@Override protected void finalize() throws Throwable { super.finalize(); obj = this; }  StringBuffer str = new StringBuffer(\u0026#34;Hello world\u0026#34;); 假设以上代码是在函数体内运行的，那么:\n 软引用: java.lang.ref.SoftReference 可被回收的引用\n 弱引用: 发现即回收。由于垃圾回收器的线程通常优先级很大，因此并不一定很快地发现持有弱引用的对象。\n 虚引用: 跟踪垃圾回收过程\n垃圾回收器 串行回收器  新生代垃圾串行收集器，使用 -XX:+UseSerialGC 来指定新生代和老年代都是用串行收集器。这个收集器虽然古老，但却久经考验。使用单线程进行垃圾回收。虚拟机在 Client 模式下运行，它是默认的垃圾收集器。独占式回收。   老年代串行收集器，使用的是标记-压缩算法。  -XX:+UseSerialGC: 新生代、老生代都使用串行回收器 -XX:+UseParNewGC -XX:+UseParallelGC     并行回收器 新生代 ParNew 回收器:\n -XX:+UseParNewGC -XX:+UseConcMarkSweepGC  回收器工作时的线程数量可以使用 -XX:ParallelGCThreads 参数指。一般最好与 CPU 数量相当，避免过多的线程数。默认算法\nint getGCThreadsCount() { if ( countOfCPU \u0026lt; 8 ) return countOfCPU; else return 3 + ( ( 5 * countOfCPU ) / 8 ); }  新生代 ParallelGC 回收器: 关注系统吞吐量\n -XX:+UseParallelGC -XX:+UseParallelOldGC  两个重要参数控制系统吞吐量:\n -XX:MaxGCPauseMillis: 设置最大垃圾收集停顿时间 -XX:GCTimeRatio: 设置吞吐量大小 -XX:+UseAdaptiveSizePolicy: 打开自适应 GC 策略   老年代 ParallelOldGC: 标记压缩算法\n  并行收集器，将串行回收器多线程化。并行回收器工作时的线程数量可以使用 -XX:ParallelGCThreads 参数指定，一般最好与 CPU 数量相当，避免过多的线程数，影响垃圾收集性能。  -XX:+UseParNewGC: 新生代使用并行回收收集器 (ParNew)，老年代使用串行收集器 -XX:+UseConcMarkSweepGC: 新生代使用并行收集器 (ParNew)，老年代使用 CMS   新生代并行回收收集器，使用复制算法  -XX:+UseParallelGC: 新生代使用并行回收收集器 (ParallelGC)，老年代使用串行收集器   老年代并行回收收集器，使用标记-压缩算法  使用 -XX:+UseParallelOldGC: 新生代使用 ParallelGC ，老年代使用 ParallelOldGC     CMS (Concurrent Mark Sweep): 关注系统停顿时间，非独占式\n -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction: 当老年代的空间使用率达到 68% (默认) 时进行一次 CMS 垃圾回收 -XX:+UseCMSCompactAtFullCollection: 在垃圾收集完成之后，进行一次内存碎片整理 CMS 收集器，这是一个关注停顿的垃圾收集器    G1 收集器: JDK 1.7 正式启用  新生代串行收集器和老年代串行收集器都是串行的、独占式的垃圾收集器。不要求整个 eden 区、年轻代或者老年代都连续\n 使用 -XX:+UseSerialGC 打印出的 GC 信息:\n[GC (Allocation Failure) [DefNew: 18954K-\u0026gt;897K(28864K), 0.0020543 secs] 18954K-\u0026gt;897K(93056K), 0.0020917 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 使用 -XX:+UseParNewGC 打印出的 GC 信息:\n[GC (Allocation Failure) [ParNew: 19468K-\u0026gt;880K(28864K), 0.0033698 secs] 19468K-\u0026gt;880K(93056K), 0.0034037 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 使用 -XX:+UseParallelOldGC (默认) 打印出的 GC 信息:\n[GC (Allocation Failure) [PSYoungGen: 24485K-\u0026gt;448K(28160K)] 368549K-\u0026gt;344520K(379904K), 0.0039329 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] G1 (Garbage-First) 垃圾收集器 以前的垃圾收集器 (serial, parallel, CMS) 将堆分为固定大小的三个区域: 年轻代、老年代和永久代:\n但是，G1 采取了一种不同的方法:\n堆被分成了一系列相同大小的区域，并且相同角色的区域的大小不再是固定的，这样在内存使用上能够提供更大的灵活性。当垃圾收集开始的时候，G1 和 CMS 执行的操作其实是一样的:\n 并发全局扫描标记检查存活的对象 哪些区域垃圾对象最多，G1 就先收集哪些区域，这也是它为什么称为 Garbage-First 的原因   其他垃圾收集器使用 jvm 内置线程回收，而 G1 采用应用线程承担回收工作。\nG1 垃圾收集器 VS CMS 垃圾收集器 G1 就是计划取代 Concurrent Mark-Sweep Collector (CMS). 与 CMS 相比，G1:\n G1 是一个 compacting collector. G1 compacts sufficiently to completely avoid the use of fine-grained free lists for allocation, and instead relies on regions. This considerably simplifies parts of the collector, and mostly eliminates potential fragmentation issues. G1 提供了更多的可预测的垃圾收集停顿，允许用户指定停顿时间  实用 JVM 参数  获取堆快照。  发生 OutOfMemoryError 时，可以使用 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=C:\\m.hprof 来保存当前的堆快照到文件中。也可以加上参数 -XX:OnOutOfMemoryError=c:\\reset.bat 来运行一段脚本。\n当发生 OutOfMemoryError (在一个 Windows 32 系统上就发生过) 的时候，应该尝试使用增大可用堆：\njava -Xmn1024M -jar xxx.jar TODO: 思考: 如果知晓程序究竟需要多大内存？\n 获取 GC 信息  使用参数 -verbose:gc 或者 -XX:+PrintGC 来获取简要的 GC 信息，也可以使用 -XX:+PrintGCDetails 来获取更加详细的信息。如果需要在 GC 发生的时刻打印 GC 发生的时间，则可以追加 -XX:+PrintGCTimeStamps 选项以查看相对时间或者 -XX:+PrintGCDateStamps 以查看绝对时间。如果许雅查看新生对象晋升到老年代的实际阈值，可以使用参数 -XX:+PrintTenuringDistribution -XX:MaxTenuringThreshold=18 来运行程序。如果需要在 GC 时，打印详细的堆信息，则可以打开 -XX:+PrintHeapAtGC 开关。\n 控制 GC  -XX:+PrintExplicitGC 选项用于禁止显式的 GC 操作，即禁止在程序中使用 System.gc() 触发的 Full GC。另一个有用的 GC 控制参数是 -Xincgc，一旦启用这个参数，系统便会进行增量式的 GC。\nJVM 调优的主要过程有: 确定堆内存大小 (-Xmx、-Xms)、合理分配新生代和老年代 (-XX:NewRatio、-Xmn、-XX:SurvivorRatio)、确定永久区大小 (-XX:Permsize、-XX:MaxPermSize)、选择垃圾收集器、对垃圾收集器进行合理的设置。除此之外，禁用显式 GC (-XX:+DisableExplicitGC)、禁用类元数据回收 (+Xnoclassgc)、禁用类验证 (-Xverify:none) 等设置，对提升系统性能也有一定的帮助。\n GC 日志示例  使用 -XX:+PrintGC 获取的 GC 日志:\n[GC (Allocation Failure) GC前堆使用量20M-\u0026gt;GC后堆使用量(当前可用堆大小90M), 本次GC花费 0.0028389 秒] [GC (Allocation Failure) 20409K-\u0026gt;432K(92672K), 0.0028389 secs] 同样的代码使用 -X:+PrintGCDetails 获取的 GC 日志:\n[GC (Allocation Failure) [新生代: 从20M-\u0026gt;降为0.4M(可用28M)] 整个堆从20M-\u0026gt;将为0.4M(可用90M), 0.0151333 secs] [Times: 用户态时间耗时，系统态时间耗时，GC 实际经历的时间] 新生代 总大小 28M, 已用 13M [下界，当前上界，上界] [GC (Allocation Failure) [PSYoungGen: 20409K-\u0026gt;448K(28160K)] 20409K-\u0026gt;456K(92672K), 0.0151333 secs] [Times: user=0.00 sys=0.00, real=0.02 secs] Heap PSYoungGen total 28160K, used 13461K [0x00000000e1380000, 0x00000000e4a80000, 0x0000000100000000) eden space 24576K, 52% used [0x00000000e1380000,0x00000000e20356d0,0x00000000e2b80000) from space 3584K, 12% used [0x00000000e2b80000,0x00000000e2bf0020,0x00000000e2f00000) to space 3584K, 0% used [0x00000000e4700000,0x00000000e4700000,0x00000000e4a80000) ParOldGen total 64512K, used 8K [0x00000000a3a00000, 0x00000000a7900000, 0x00000000e1380000) object space 64512K, 0% used [0x00000000a3a00000,0x00000000a3a02000,0x00000000a7900000) Metaspace used 3264K, capacity 4494K, committed 4864K, reserved 1056768K class space used 363K, capacity 386K, committed 512K, reserved 1048576K 如果需要更为全面的堆信息，还可以使用参数 -XX:+PrintHeapAtGC，它会在每次 GC 前后分别打印堆的信息\n{Heap before GC invocations=1 (full 0): ... Heap after GC invocations=1 (full 0): ... } 如果需要分析 GC 发生的时间，还可以使用 -XX:+PrintGCTimeStamps 参数，该输出时间为虚拟机启动后的时间偏移量:\n0.174: [GC (Allocation Failure) 20409K-\u0026gt;504K(92672K), 0.0016586 secs] 0.179: [GC (Allocation Failure) 19415K-\u0026gt;464K(92672K), 0.0031200 secs] 0.186: [GC (Allocation Failure) 19812K-\u0026gt;432K(92672K), 0.0009531 secs] 由于 GC 还会引起应用程序停顿，使用参数 -XX:+PrintGCApplicationConcurrentTime 可以打印应用程序的执行时间，使用参数 -XX:+PrintGCApplicationStoppedTime 可以打印应用程序由于 GC 而产生的停顿时间:\nApplication time: 0.0084849 seconds [GC (Allocation Failure) 20409K-\u0026gt;520K(92672K), 0.0044274 secs] Total time for which application threads were stopped: 0.0045452 seconds, Stopping threads took: 0.0000210 seconds Application time: 0.0033066 seconds [GC (Allocation Failure) 19431K-\u0026gt;440K(117248K), 0.0020202 secs] Total time for which application threads were stopped: 0.0021438 seconds, Stopping threads took: 0.0000258 seconds Application time: 0.0082455 seconds 如果想跟踪系统内的软引用、弱引用、虚引用和 Finalize 队列，则可以使用打开 -XX:+PrintReferenceGC 开关. 使用参数 -Xloggc:log/gc.log 启动虚拟机，将 GC 日志输出到 gc.log 文件中\nJava HotSpot(TM) 64-Bit Server VM (25.111-b14) for linux-amd64 JRE (1.8.0_111-b14), built on Sep 22 2016 16:14:03 by \u0026quot;java_re\u0026quot; with gcc 4.3.0 20080428 (Red Hat 4.3.0-8) Memory: 4k page, physical 6052560k(316636k free), swap 6233084k(4248464k free) CommandLine flags: -XX:InitialHeapSize=96840960 -XX:MaxHeapSize=1549455360 -XX:+PrintGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC 0.183: Application time: 0.0107645 seconds 0.183: [GC (Allocation Failure) 20409K-\u0026gt;432K(92672K), 0.0033748 secs] 0.187: Total time for which application threads were stopped: 0.0035825 seconds, Stopping threads took: 0.0000191 seconds 0.192: Application time: 0.0054269 seconds 0.193: [GC (Allocation Failure) 19343K-\u0026gt;496K(117248K), 0.0108382 secs] 0.204: Total time for which application threads were stopped: 0.0116746 seconds, Stopping threads took: 0.0000766 seconds 0.212: Application time: 0.0084699 seconds 系统参数查看:\n -XX:+PrintVMOptions: 打印虚拟机接受的命令行显示参数 -XX:+PrintCommandLineFlags: 打印虚拟机的显示和隐式参数 -XX:+PrintFlagsFinal: 打印所有的系统参数的值  # 打印出系统的堆大小 java -XX:+PrintFlagsFinal -version | grep -iE \u0026#39;HeapSize|PermSize|ThreadStackSize\u0026#39; Minor GC、Major GC 和 Full GC  Minor GC: 从年轻代回收垃圾，当 JVM 无法分配新对象的时候会触发 Minor GC，也就是说 Eden 区域已经满了 Major GC: 清除 Tenured 区域 Full GC: 清除整个堆，包括 Yound 和 Tenured 区域  Java 各版本默认垃圾收集器 参考 1 说:\nOn server-class machines running the server VM, the garbage collector (GC) has changed from the previous serial collector [\u0026hellip;] to a parallel collector\nReference 2 says:\nStarting with J2SE 5.0, when an application starts up, the launcher can attempt to detect whether the application is running on a \u0026ldquo;server-class\u0026rdquo; machine and, if so, use the Java HotSpot Server Virtual Machine (server VM) instead of the Java HotSpot Client Virtual Machine (client VM).\nAlso, reference 2 says:\n注意: 对于 Java SE 6, the definition of a server-class machine is one with at least 2 CPUs and at least 2GB of physical memory.\nJava 7 和 Java 8 使用的都是 Parallel GC，Java 9 使用的是 G1 垃圾收集器\nJVM 的工作模式  java -version: 查看 Server VM java -client -version: 查看 Client VM  Client 和 Server 模式下的各种参数可能会有很大不同\nHeap Memory 最佳实践  是否分配了过多实例: 使用 jcmd 8998 GC.class_histogram 来查看各实例有多少个，也可以使用 jmap -histo 8998 来获得相同的结果 分析堆快照: 使用 jhat、jvisualvm、mat 等工具来分析 hprof 文件  jcmd 8998 GC.heap_dump /path/to/heap_dump.hprof jmap -dump:live,file=/path/to/heap_dump.hprof 8998: 引入 live 强制 full GC    Java Monitoring 常用工具 jstack Jstack: Dumps the stacks of a Java 进程\njstack $PID \u0026gt; $DATE_DIR/jstack-$PID.dump 2\u0026gt;\u0026amp;1 jinfo Jinfo: Provides visibility into the system properties of the JVM, and allows some system properties to be set dynamically.\nroot@zk-pc:~# jinfo 18772 Attaching to process ID 18772, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.144-b01 Java System Properties: com.sun.management.jmxremote.authenticate = false java.runtime.name = Java(TM) SE Runtime Environment java.vm.version = 25.144-b01 ...(省略好多) VM Flags: Non-default VM flags: -XX:CICompilerCount=3 -XX:InitialHeapSize=98566144 -XX:+ManagementServer -XX:MaxHeapSize=1549795328 -XX:MaxNewSize=516423680 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=32505856 -XX:OldSize=66060288 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseFastUnorderedTimeStamps -XX:+UseParallelGC Command line: -Dcom.sun.management.jmxremote.port=5780 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/usr/lib/intellij_idea/idea-IC-172.3968.16/lib/idea_rt.jar=35487:/usr/lib/intellij_idea/idea-IC-172.3968.16/bin -Dfile.encoding=UTF-8 jstat jstat: 提供有关 GC 和类加载活动的相关信息\n显示可用的九个 options:\njstat -options One useful option is -gcutil, which displays the time spent in GC as well as the percentage of each GC area that is currently filled. Other options to jstat will display the GC sizes in terms of KB.\nRemember that jstat takes an optional argument—the number of milliseconds to repeat the command—so it can monitor over time the effect of GC in an application.\njstat -gcutil process_id 1000 打印出的是:\nroot@zk-pc:~# jstat -gcutil 18772 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 71.53 97.93 34.02 96.70 93.37 29 0.133 1 0.040 0.172  gccapacity 可以显示 VM 内存中三代（young,old,perm）对象的使用和占用大小\njstat -gccapacity process_id 打印出的是:\nroot@zk-pc:~# jstat -gccapacity 18772 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 31744.0 504320.0 30720.0 4608.0 4608.0 21504.0 64512.0 1009152.0 44032.0 44032.0 0.0 1069056.0 22272.0 0.0 1048576.0 2560.0 32 1 jmap (Memory Map) jmap: Provides heap dumps and other information about JVM memory usage.\njmap $PID 打印的是一堆这种东西:\nroot@zk-pc:~# jmap 18772 Attaching to process ID 18772, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.144-b01 0x0000000000400000\t7K\t/usr/lib/jvm/oracle_jdk8/jdk1.8.0_144/bin/java 0x00007f7072978000\t98K\t/lib/x86_64-linux-gnu/libresolv-2.23.so 0x00007f7072b93000\t26K\t/lib/x86_64-linux-gnu/libnss_dns-2.23.so 0x00007f7072d9a000\t10K\t/lib/x86_64-linux-gnu/libnss_mdns4_minimal.so.2 0x00007f70737a1000\t87K\t/lib/x86_64-linux-gnu/libgcc_s.so.1 0x00007f70739b7000\t251K\t/usr/lib/jvm/oracle_jdk8/jdk1.8.0_144/jre/lib/amd64/libsunec.so ...(省略好多)  Print histogram(直方图；柱状图) of java object heap; if the \u0026ldquo;live\u0026rdquo; suboption is specified, only count live objects:\njmap -histo $PID jmap -histo:live $PID root@zk-pc:~# jmap -F -histo 18772 Object Histogram: num #instances\t#bytes\tClass description -------------------------------------------------------------------------- 1:\t65711\t10183976\tchar[] 2:\t13523\t8919400\tbyte[] 3:\t54732\t2159368\tjava.lang.Object[] 4:\t7341\t1451792\tint[] 5:\t56423\t1354152\tjava.lang.String 6:\t15476\t619040\tjava.util.TreeMap$Entry 7:\t16562\t529984\tjava.io.ObjectStreamClass$WeakClassKey 8:\t11915\t476600\tjava.util.LinkedHashMap$Entry 9:\t9716\t466368\tjava.util.HashMap 10:\t3993\t453312\tjava.lang.Class 11:\t11568\t370176\tjava.util.concurrent.ConcurrentHashMap$Node 12:\t6160\t306952\tjava.util.HashMap$Node[] 13:\t4210\t279856\tjava.util.Hashtable$Entry[] 14:\t8320\t266240\tjava.util.Vector 15:\t8070\t258240\tjava.util.HashMap$Node 16:\t10495\t251880\torg.jsoup.nodes.Attribute 17:\t4181\t200688\tjava.util.Hashtable ...(省略好多)  Print java heap summary:\njmap -heap $PID 打印出的是一堆这种东西:\nroot@zk-pc:~# jmap -heap 18772 Attaching to process ID 18772, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.144-b01 using thread-local object allocation. Parallel GC with 4 thread(s) Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 1549795328 (1478.0MB) NewSize = 32505856 (31.0MB) MaxNewSize = 516423680 (492.5MB) OldSize = 66060288 (63.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: PS Young Generation Eden Space: capacity = 23068672 (22.0MB) used = 11772712 (11.227333068847656MB) free = 11295960 (10.772666931152344MB) 51.03333213112571% used From Space: capacity = 11010048 (10.5MB) used = 2035424 (1.941131591796875MB) free = 8974624 (8.558868408203125MB) 18.48696754092262% used To Space: capacity = 11534336 (11.0MB) used = 0 (0.0MB) free = 11534336 (11.0MB) 0.0% used PS Old Generation capacity = 45088768 (43.0MB) used = 13718432 (13.082916259765625MB) free = 31370336 (29.917083740234375MB) 30.42538665061773% used 8999 interned Strings occupying 836656 bytes. 堆内存使用最佳实践 堆分析 (1) 查看直方图\n// jcmd 命令默认就会进行 full GC jcmd 6808 GC.class_histogram jmap -histo 6808 // 如果指明 live: 选项，将会强制进行一个 full GC jmap -histo:live 6808  num #instances #bytes class name ---------------------------------------------- 1: 12227 1303424 [C 2: 1003 627856 [B 3: 1917 461864 [I 4: 3828 421768 java.lang.Class 5: 11665 279960 java.lang.String 6: 6065 194080 java.util.concurrent.ConcurrentHashMap$Node 7: 2794 173144 [Ljava.lang.Object; 8: 3072 122880 org.apache.lucene.index.FreqProxTermsWriter$PostingList 9: 2760 110400 java.util.LinkedHashMap$Entry 10: 1097 101144 [Ljava.util.HashMap$Node; 11: 5440 87040 java.lang.Object 12: 2680 85760 java.util.HashMap$Node 13: 520 45760 java.lang.reflect.Method 14: 44 44064 [Ljava.util.concurrent.ConcurrentHashMap$Node; 15: 781 43736 java.util.LinkedHashMap 16: 96 41088 [Lorg.apache.lucene.index.RawPostingList; ... (2) Dump 堆\n// 指明 live，强制进行 full GC jmap -dump:live,file=/tmp/heap_dump.hprof 6808 // 或者 jmap -F -dump:format=b,file=filename.hprof 20961 // 或者简单点 jmap -F -dump:file=filename.hprof 20961  注意: 路径一定要显示指明，否则不知道默认保存到哪里去了\n 通常有三种工具能够分析 .hprof 文件：\n jhat jvisualvm mat  (3) 内存溢出\n内存溢出通常发生在:\n Native 内存用光了 permgen(Java 7) 或者 metaspace(Java 8) 内存用光了 Java 堆内存用光了 JVM 进行 GC 的时间太长了  使用更少的内存 (1) 减少对象大小\n(2) 延迟初始化 (3) 不可变对象 (4) String Interning\n对象生命周期管理 JIT (1) 编译还是解释 Languages like C++ and Fortran are called compiled languages because their programs are delivered as binary (compiled) code: the program is written, and then a static compiler produces a binary. The assembly code in that binary is targeted to a particular CPU. Complementary CPUs can execute the same binary: for example, AMD and Intel CPUs share a basic, common set of assembly language instructions, and later versions of CPUs almost always can execute the same set of instructions as previous versions of that CPU.\nLanguages like PHP and Perl, on the other hand, are interpreted. The same program source code can be run on any CPU as long as the machine has the correct interpreter (that is, the program called php or perl). The interpreter translates each line of the program into binary code as that line is executed.\nJava attempts to find a middle ground here. Java applications are compiled—but instead of being compiled into a specific binary for a specific CPU, they are compiled into an idealized assembly language. This assembly language (know as Java bytecodes) is then run by the java binary (in the same way that an interpreted PHP script is run by the php binary). This gives Java the platform independence of an interpreted language. Because it is executing an idealized binary code, the java program is able to compile the code into the platform binary as the code executes. This compilation occurs as the program is executed: it happens “just in time.\n(2) HotSpot 名字的含义 In a typical program, only a small subset of code is executed frequently, and the performance of an application depends primarily on how fast those sections of code are executed. These critical sections are known as the hot spots of the application; the more the section of code is executed, the hotter that section is said to be.\nHence, when the JVM executes code, it does not begin compiling the code immediately. There are two basic reasons for this. First, if the code is going to be executed only once, then compiling it is essentially a wasted effort; it will be faster to interpret the Java bytecodes than to compile them and execute (only once) the compiled code.\nthe more times that the JVM executes a particular method or loop, the more information it has about that code. This allows the JVM to make a number of optimizations when it compiles the code.\n(3) 寄存器和内存 If the value of sum were to be retrieved from (and stored back to) main memory on every iteration of this loop, performance would be dismal. Instead, the compiler will load a register with the initial value of sum, perform the loop using that value in the register, and then (at an indeterminate point in time) store the final result from the register back to main memory.\nRegister usage is a general optimization of the compiler, and when escape analysis is enabled (see the end of this chapter), register use is quite aggressive.\n(4) 选择 Java 编译器  A 32-bit client version (-client) A 32-bit server version (-server) A 64-bit server version (-d64)  For the sake of compatibility, the argument specifying which compiler to use is not rigorously followed. If you have a 64-bit JVM and specify -client, the application will use the 64-bit server compiler anyway. If you have a 32 bit JVM and you specify -d64, you will get an error that the given instance does not support a 64-bit JVM.\nThe client compiler begins compiling sooner than the server compiler does. code produced by the server compiler will be faster than that produced by the client compiler. couldn’t the JVM start with the client compiler, and then use the server compiler as code gets hotter? That technique is known as tiered compilation. With tiered compilation, code is first compiled by the client compiler; as it becomes hot, it is recompiled by the server compiler.\n# Java 7 需要打开, Java 8 默认开启 -server -XX:+TieredCompilation  For GUI programs, uses the client compiler by default. Performance is often all about perception: if the initial startup seems faster, and everything else seems fine, users will tend to view the program that has started faster as being faster overall. For long-running applications, always choose the server compiler, preferably in conjunction with tiered compilation.  查看默认编译器:\njava -version (5) 更多考虑因素 Code Cache: When the JVM compiles code, it holds the set of assembly-language instructions in the code cache. Code Cache 有固定大小, and once it has filled up, the JVM is not able to compile any additional code.\n 编译阈值: The major factor involved here is 多频繁 the code is executed; once it is executed a certain number of times, its compilation threshold is reached, and the compiler deems that it has enough information to compile the code.\nCompilation is based on two counters in the JVM: 方法调用次数, and 方法内循环的实际次数. When the JVM executes a Java method, it checks the sum of those two counters and decides whether or not the method is eligible for compilation. This kind of compilation has no official name but is often called standard compilation (标准编译).\nBut what if the method has a really long loop—or one that never exits and provides all the logic of the program? In that case, the JVM needs to compile the loop without waiting for a method invocation. So every time the loop completes an execution, the branching counter is incremented and inspected. If the branching counter has exceeded its individual threshold, then the loop (and not the entire method) becomes eligible for compilation.\nThis kind of compilation is called on-stack replacement (OSR), because even if the loop is compiled, that isn’t sufficient: the JVM has to have the ability to start executing the compiled version of the loop while the loop is still running. When the code for the has finished compiling, the JVM replaces the code (on-stack), and the next iteration of the loop will execute the much-faster compiled version of the code (下一次循环就是编译版本了).\nStandard compilation is triggered by the value of the -XX:CompileThreshold=N flag. The default value of N for the client compiler is 1,500; for the server compiler it is 10,000.\n 查看编译过程: -XX:+PrintCompilation.\njstat has two options to provide information about the compiler. The -compiler option supplies summary information about 多少方法被编译了 (here 5003 is the process ID of the program to be inspected):\njstat -compiler 5003 lternately, you can use the -printcompilation option to get information about the 最后一个方法 that is compiled. In this example, jstat repeats the information for process ID 5003 every second (1,000 ms):\njstat -printcompilation 5003 1000  编译线程个数:\n 内联:\nOne of the most important optimizations the compiler makes is to inline methods.\npublic class Point { private int x, y; public void getX() { return x; } public void setX(int i) { x = i; } } 当你写这样代码的时候:\nPoint p = getPoint(); p.setX(p.getX() * 2); 编译后的代码执行的将会是:\nPoint p = getPoint(); p.x = p.x * 2; The basic decision about whether to inline a method depends on 多频繁 and 大小. The JVM determines if a method is hot (i.e., called frequently) based on an internal calculation; it is not directly subject to any tunable parameters. If a method is eligible for inlining because it is called frequently, then it will be inlined only if its 字节码大小小于 325 字节 (or whatever is specified as the -XX:MaxFreqInlineSize=N flag). Otherwise, it is eligible for inlining only if it is small: 小于 35 字节 (or whatever is specified as the -XX:MaxInlineSize=N flag)\n 逃逸分析:\nThe server compiler performs some very aggressive optimizations if escape analysis is enabled (-XX:+DoEscapeAnalysis, 默认开启).\npublic class Factorial { private BigInteger factorial; private int n; public Factorial(int n) { this.n = n; } public synchronized BigInteger getFactorial() { if (factorial == null) factorial = ...; return factorial; } } The factorial object is referenced only inside that loop; no other code can ever access that object. Hence, the JVM is free to perform a number of optimizations on that object:\n It needn’t get a synchronization lock when calling the getFactorial() method. It needn’t store the field n in memory; it can keep that value in a register. Similarly it can store the factorial object reference in a register. In fact, it needn’t allocate an actual factorial object at all; it can just keep track of the individual fields of the object.  (6) Deoptimization Deoptimization means that the compiler 不得不撤销一些优化; the effect is that the performance of the application will be reduced—at least until the compiler can recompile the code in question. There are two cases of deoptimization: when code is “made not entrant,” and when code is “made zombie”.\n Not Entrant Code:\nThere are two things that cause code to be made not entrant. One is due to the way classes and interfaces work, and one is an implementation detail of tiered compilation\nStockPriceHistory sph; String log = request.getParameter(\u0026#34;log\u0026#34;); if (log != null \u0026amp;\u0026amp; log.equals(\u0026#34;true\u0026#34;)) { sph = new StockPriceHistoryLogger(...); } else { sph = new StockPriceHistoryImpl(...); } // Then the JSP makes calls to: sph.getHighPrice(); sph.getStdDev(); // and so on If a bunch of calls are made to http://localhost:8080/StockServlet (that is, without the log parameter), the compiler will see that the actual type of the sph object is StockPriceHistoryImpl. It will then inline code and perform other optimizations based on that knowledge. Later, say a call is made to http://localhost:8080/StockServlet?log=true. Now the assumption the compiler made regarding the type of the sph object is false; the previous optimizations are no longer valid. This generates a deoptimization trap, and the previous optimizations are discarded. If a lot of additional calls are made with logging enabled, the JVM will quickly end up compiling that code and making new optimizations.\nIn tiered compilation, code is compiled by the client compiler, and then later compiled by the server compiler (and actually it’s a little more complicated than that, as discussed in the next section). When the code compiled by the server compiler is ready, the JVM must replace the code compiled by the client compiler. It does this by 将旧代码标记为 Not Entrant and using the same mechanism to substitute the newly compiled (and more efficient) code.\n Deoptimizing Zombie Code:\nRecall that the compiled code is held in a fixedsize code cache; when zombie methods are identified, it means that the code in question can be removed from the code cache, making room for other classes to be compiled (or limiting the amount of memory the JVM will need to allocate later).\nThe possible downside here is that if the code for the class is made zombie and then later reloaded and heavily used again, the JVM will need to recompile and reoptimize the code.\nTODO  https://docs.oracle.com/javase/6/docs/technotes/guides/management/jconsole.html https://stackoverflow.com/questions/1058991/how-to-monitor-java-memory-usage  远程 JVisualVM 远程机器上输入 jstatd:\nCould not create remote object access denied (\u0026quot;java.util.PropertyPermission\u0026quot; \u0026quot;java.rmi.server.ignoreSubClasses\u0026quot; \u0026quot;write\u0026quot;) java.security.AccessControlException: access denied (\u0026quot;java.util.PropertyPermission\u0026quot; \u0026quot;java.rmi.server.ignoreSubClasses\u0026quot; \u0026quot;write\u0026quot;) at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.security.AccessController.checkPermission(AccessController.java:884) at java.lang.SecurityManager.checkPermission(SecurityManager.java:549) at java.lang.System.setProperty(System.java:792) at sun.tools.jstatd.Jstatd.main(Jstatd.java:139) 你需要创建一个安全策略文件: jstatd.all.policy，里面写上这句话:\ngrant codebase \u0026quot;file:/opt/java/jdk1.7.0_21/lib/tools.jar\u0026quot; { permission java.security.AllPermission; }; 然后使用如下命令重新启动:\njstatd -J -Djava.security.policy=/home/user/jstatd.all.policy 在本机测试，是否能够 telnet 到 jstatd 服务:\ntelnet 10.108.112.218 1099 有些时候，jstatd 可能绑定的并不是正确的网卡:\n-J-Djava.rmi.server.hostname=10.1.1.123 强制使用 IPV4:\n-J-Djava.net.preferIPv4Stack=true 查看一些日志输出:\n-J-Djava.rmi.server.logCalls=true 最后的命令:\njstatd -J-Djava.security.policy=./jstatd.all.policy -J-Djava.rmi.server.hostname=10.108.112.218 -J-Djava.rmi.server.logCalls=true GC 日志分析工具  GCeasy  DUMP 什么 以下是 dubbo - dump.sh 备份的内容:\nDUMP_DATE=`date +%Y%m%d%H%M%S` DATE_DIR=$DUMP_DIR/$DUMP_DATE echo -e \u0026#34;Dumping the $SERVER_NAME...\\c\u0026#34; for PID in $PIDS ; do jstack $PID \u0026gt; $DATE_DIR/jstack-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jinfo $PID \u0026gt; $DATE_DIR/jinfo-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jstat -gcutil $PID \u0026gt; $DATE_DIR/jstat-gcutil-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jstat -gccapacity $PID \u0026gt; $DATE_DIR/jstat-gccapacity-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jmap $PID \u0026gt; $DATE_DIR/jmap-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jmap -heap $PID \u0026gt; $DATE_DIR/jmap-heap-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; jmap -histo $PID \u0026gt; $DATE_DIR/jmap-histo-$PID.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; if [ -r /usr/sbin/lsof ]; then /usr/sbin/lsof -p $PID \u0026gt; $DATE_DIR/lsof-$PID.dump echo -e \u0026#34;.\\c\u0026#34; fi done if [ -r /bin/netstat ]; then /bin/netstat -an \u0026gt; $DATE_DIR/netstat.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/iostat ]; then /usr/bin/iostat \u0026gt; $DATE_DIR/iostat.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/mpstat ]; then /usr/bin/mpstat \u0026gt; $DATE_DIR/mpstat.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/vmstat ]; then /usr/bin/vmstat \u0026gt; $DATE_DIR/vmstat.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/free ]; then /usr/bin/free -t \u0026gt; $DATE_DIR/free.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/sar ]; then /usr/bin/sar \u0026gt; $DATE_DIR/sar.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi if [ -r /usr/bin/uptime ]; then /usr/bin/uptime \u0026gt; $DATE_DIR/uptime.dump 2\u0026gt;\u0026amp;1 echo -e \u0026#34;.\\c\u0026#34; fi 从上可知一般统计的都有如下几项:\n jstack: 线程信息 jinfo: 配置信息. The configuration information includes Java system properties and Java Virtual Machine (JVM) command-line flags. jstat -gcutil: 垃圾收集统计 jstat -gccapacity: Displays statistics about the capacities of the generations and their corresponding spaces. jmap: Prints 共享对象内存 maps or 堆内存 details for a process, core file, or remote debug server. jmap -heap: Prints a heap summary of the garbage collection used, the head configuration, and generation-wise heap usage. In addition, the number and size of interned Strings are printed. jmap -histo: Prints a histogram of the heap lsof -p netstat -an iostat: Report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS). mpstat: Report 处理器 related statistics. vmstat: vmstat (virtual memory statistics) is a computer system monitoring tool that collects and displays summary information about operating system memory, processes, interrupts, paging and block I/O. free -t: Display amount of 可用/已用内存 in the system. -t: Display a line showing the column totals. sar: In computing, sar (System Activity Report) is a Unix System V-derived system monitor command used to report on various system loads, including CPU 活动, memory/paging, 设备负载, 网络. Linux distributions provide sar through the sysstat package. uptime: uptime gives a one line display of the following information. The 当前时间, 多长时间 the system has been running, 多少用户 are currently logged on, and the 系统平均负载 averages for the past 1, 5, and 15 minutes.  实际运用中如何清晰明了地观察 JVM 的运行过程?  图形工具: JProfiler, JConsole, Java VisualVM 命令: jps, jstack, jmap, jhat, jstat  JVM 如何进阶 问:JVM如何进阶，目前周志明的《深入理解JVM》第2版看了两遍，能够根据目录口述书中大部分内容，还需要了解哪些知识？\n答：周志明的书只能算是 JVM 的入门书籍。接下来你应该去读一读**《Java虚拟机规范》**，周志明的书很多内容是从里面来的，但是规范本身比较详细，注意读英文原版。其次去读一下Oralce的文档：**《Hotspot Memory Management white paper》, 《Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide》**。现在你需要进一步修炼关于**内存管理**的部分，阅读比如**《垃圾回收算法与实现》**，如果这本读完还不满足，那么阅读**《自动内存管理艺术——垃圾回收算法手册》**。到了这一步，理论你已经掌握得很好了，是时候把 Hotspot 源码 download 下来编译好之后断点调试玩玩了，这个时候我要推荐你今年阿里人刚出的**《揭秘Java虚拟机》**，不过阅读这本书之前你要是愿意先读完**《深入理解计算机系统》**效果更好。到了这一步，剩下的，自己探索了，我也在探索。\n 问: 线上CPU很高、内存占用很少，有能快速查找到原因的方法吗？\n答: 给一个代码，在 Linux 下保存成 .sh 文件直接执行即可。\n#!/bin/sh ts=$(date +\u0026#34;%s\u0026#34;) jvmPid=$1 defaultLines=100 defaultTop=20 threadStackLines=${2:-$defaultLines} topThreads=${3:-$defaultTop} jvmCapture=$(top -b -n1 | grep java ) threadsTopCapture=$(top -b -n1 -H | grep java ) jstackOutput=$(echo \u0026#34;$(jstack $jvmPid )\u0026#34; ) topOutput=$(echo \u0026#34;$(echo \u0026#34;$threadsTopCapture\u0026#34; | head -n $topThreads | perl -pe \u0026#39;s/\\e\\[?.*?[\\@-~] ?//g\u0026#39; | awk \u0026#39;{gsub(/^ +/,\u0026#34;\u0026#34;);print}\u0026#39; | awk \u0026#39;{gsub(/ +|[+-]/,\u0026#34; \u0026#34;);print}\u0026#39; | cut -d \u0026#34; \u0026#34; -f 1,9 )\\n \u0026#34;) echo \u0026#34;*************************************************************************************************************\u0026#34; uptime echo \u0026#34;Analyzing top $topThreadsthreads\u0026#34; echo \u0026#34;*************************************************************************************************************\u0026#34; printf %s \u0026#34;$topOutput\u0026#34; | while IFS= read line do pid=$(echo $line | cut -d \u0026#34; \u0026#34; -f 1) hexapid=$(printf \u0026#34;%x\u0026#34; $pid) cpu=$(echo $line | cut -d \u0026#34; \u0026#34; -f 2) echo -n $cpu\u0026#34;% [$pid] \u0026#34; echo \u0026#34;$jstackOutput\u0026#34; | grep \u0026#34;tid.*0x$hexapid\u0026#34; -A $threadStackLines | sed -n -e \u0026#39;/0x\u0026#39;$hexapid\u0026#39;/,/tid/ p\u0026#39; | head -n -1 echo \u0026#34;\\n\u0026#34; done echo \u0026#34;\\n\u0026#34; 代码的意思，打印出 JVM 的所有线程以及按照 CPU 占比排序。\n 问: 您好，想问一个 JVM 比较基础的知识，现在的垃圾收集都是分代回收，那么在回收新生代的时候是要同时扫描老年代吗？是全表还是有一种策略，比如 G1 的 Remembered set，这个 set 只是记录了一种引用关系；那其它的分代回收，比如 CMS 和 ParNew 组合时只能是回收新生代的时候扫描老年代吗？那这样效率不就是降低了不少吗？\n答：对于老年代指向新生代的引用，JVM提供了一种叫 card table 的数据结构，所以每次并不需要全量遍历老年代，只需要遍历 card table 就行了。\n 问: 线上定位内存 JVM 内存溢出，除了打印堆栈拿出来分析，还有没有其它的方式？\n答：导出 JVM dump 文件，在本地使用 Eclipse 插件 MAT 分析，可视化的分析最方便、直观、有效。\n垃圾回收器怎么选择  最小化地使用内存和并行开销，请选择 Serial GC 最大化应用程序的吞吐量，请选择Parallel GC 最小化 GC 的中断或者停顿时间，请选择 CMS GC   并发和并行都可以表示两个或者多个任务一起执行，但是偏重点不同。并发偏重于多个任务交替执行，而多个任务之间有可能还是串行的。而并行是真正意义上的“同时执行”。\n内存泄漏代码示例 while (true) { for (int i=0; i\u0026lt;10000; i++) { if (!m.contains(new Key(i))) { m.put(new Key(i), \u0026#34;Number:\u0026#34; + i); } } } Interned Strings String 类型的常量池比较特殊。主要使用方法有两种:\n 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是双引号声明的 String 对象，可以使用 String 提供的 intern 方法。intern 会先判断是否存在常量池中，如果不存在，则会将当前字符串放入常量池中。  JDK 6的常量池放在 Perm 区中，默认大小只有 4 MB。JDK 7开始，放在堆中。\nMAT 1) The Dominator Tree:\nThe key to understanding your retained heap, is looking at the dominator tree. The dominator tree is a tree produced by the complex object graph in your system. The dominator tree allows you to identify the largest memory graphs. An Object X is said to dominate an Object Y if every path from the Root to Y must pass through X.\nhttps://javaeesupportpatterns.blogspot.jp/2013/03/openjpa-memory-leak-case-study.html\nJVM 诊断示例 1) 健康的 JVM:\n2) 启动内存暴涨:\n3) 激增:\n4) 内存泄露\nJVisualVM 需要安装一个 Visual GC 插件:\n才能显示具体的 GC 过程:\nTODO https://plumbr.eu/handbook/garbage-collection-algorithms-implementations#serial-minor-gc\n如何在生产环境使用 Btrace 进行调试 大多数问题的解决方式都是在本地打断点进行调试，或者在测试环境利用输出日志进行调试，这种方式简单粗暴，但过程比较繁琐，需要各种重新发布，重启应用，还不能保证一次就找到问题的根源。\nBTrace 是 sun 公司推出的一款 Java 动态、安全追踪（监控）工具，可以在不用重启的情况下监控系统运行情况，方便的获取程序运行时的数据信息，如方法参数、返回值、全局变量和堆栈信息等，并且做到最少的侵入，占用最少的系统资源。\n由于 Btrace 会把脚本逻辑直接侵入到运行的代码中，所以在使用上做很多限制：\n 不能创建对象 不能使用数组 不能抛出或捕获异常 不能使用循环 不能使用 synchronized 关键字 属性和方法必须使用 static 修饰  根据官方声明，不恰当的使用 BTrace 可能导致 JVM 崩溃，如在 BTrace 脚本使用错误的 class 文件，所以在上生产环境之前，务必在本地充分的验证脚本的正确性。\nBtrace 可以做什么？  接口性能变慢，分析每个方法的耗时情况； 当在 Map 中插入大量数据，分析其扩容情况； 分析哪个方法调用了 System.gc() 执行某个方法抛出异常时，分析运行时参数； \u0026hellip;  假设服务器端运行的是如下代码:\npublic class BtraceCase { public static Random random = new Random(); public int size; public static void main(String[] args) throws Exception { new BtraceCase().run(); } public void run() throws Exception { while (true) { add(random.nextInt(10), random.nextInt(10)); } } public int add(int a, int b) throws Exception { Thread.sleep(random.nextInt(10) * 100); return a + b; } } 我们想要对 add 方法的传入参数、返回值和执行耗时进行分析:\n通过 jps 获取服务器端的进程ID: 8454，执行命令\nbtrace 8454 Debug.java 实现对运行代码的监控:\n可以发现，Btrace 可以获取每次执行 add 方法时的数据，当然 Btrace 能做的远远不止这些，比如获取当前 jvm 堆使用情况、当前线程的执行栈等等。\n参数说明 // clazz: 需要监控的类 // method: 需要监控的方法 // clazz 和 method 可以使用正则、接口、注解等来指定 // location: 拦截位置 // Kind.ENTRY: 进入方法的时候，调用脚本 // Kind.RETURN: 执行完的时候，调用脚本 // 只有定义为 RETURN，才能获取方法的返回结果 @Return 和 @Duration @OnMethod(clazz=\u0026#34;com.metty.rpc.common.BtraceCase\u0026#34;, method=\u0026#34;add\u0026#34;, location=@Location(Kind.RETURN)) 如何使用 Btrace 定位问题  找出所有耗时超过 1ms 的过滤器 Filter  由于 @Dutation 返回的时间是纳秒级别，需要进行转换。\n 哪个方法调用了 System.gc()，调用栈如何？   统计方法的调用次数，且每隔 1 分钟打印调用次数  Btrace 的 @OnTimer 注解可以实现定时执行脚本中的一个方法\n 方法执行时，查看对象的实例属性值  通过反射机制，可以很方便的得到当前实例的属性值。\n总结 Btrace 能做的事情太多，但使用之前切记检查脚本的可行性，一旦 Btrace 脚本侵入到系统中，只有通过重启才能恢复。\n参考  《Java 程序性能优化》 Java (JVM) Memory Model – Memory Management in Java find which type of garbage collector is running Default garbage collector for Java 8 Getting Started with the G1 Garbage Collector cms Minor GC vs Major GC vs Full GC 《Java Performance: The Definitive Guide》 《大话 Java 性能调优》 《深入理解 JVM \u0026amp; G1 GC》 "});index.add({'id':70,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock-4/','title':"Best Time to Buy and Sell Stock Ⅳ",'content':"Best Time to Buy and Sell Stock Ⅳ 题目 LeetCode 地址：Best Time to Buy and Sell Stock Ⅳ\n有一个数组，第 i 个元素的值代表第 i 天的股票价格，如果你最多只能进行K次交易（某天买入一支股票，然后过几天卖掉），请问你能收获的最大利润是多少？\n分析 参考 Best Time to Buy and Sell Stock 思路上状态机，状态机应用K次即可。\n答案 // 最多交易 k 次 // https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/ // public class BestTimetoBuyandSellStockIV { public int maxProfit(int k, int[] prices) { if (prices == null || prices.length \u0026lt;= 1 || k \u0026lt;= 0) { return 0; } if (k \u0026gt;= prices.length / 2) { // 这就相当于怎样，可交易任意多次  // 问题转为 BestTimetoBuyandSellStockII  return maxProfitQuestion2(prices); } int s0 = 0; int[] sArray = new int[k * 2]; initStateArray(prices, sArray); for (int i = 1; i \u0026lt; prices.length; i++) { s0 = s0; sArray[0] = Math.max(sArray[0], s0 - prices[i]); for (int j = 1; j \u0026lt; sArray.length; j++) { if (j % 2 == 1) { sArray[j] = Math.max(sArray[j - 1] - prices[i], sArray[j]); } else { sArray[j] = Math.max(sArray[j - 1] + prices[i], sArray[j]); } } } return sArray[sArray.length - 1]; } private void initStateArray(int[] prices, int[] sArray) { for (int i = 0; i \u0026lt; sArray.length; i += 2) { sArray[i] = -prices[0]; } } public int maxProfitQuestion2(int[] prices) { int max = 0; for (int i = 1; i \u0026lt; prices.length; i++) { int diff = prices[i] - prices[i - 1]; if (diff \u0026gt; 0) { max += diff; } } return max; } } 扫描下面二维码，在手机上阅读这篇文章：\n"});index.add({'id':71,'href':'/docs/tutorial/git/git-reset/','title':"Git 重置",'content':"Git 重置 git reset 命令是 Git 最危险最容易误用的命令之一！一定要慎用，要清除地知道你自己在做什么！\nGit reset 命令格式 git reset [--soft | --mixed | --hard] [\u0026lt;commit\u0026gt;] Git 提交历史记录 cat .git/refs/heads/master 显示的就是当前版本库的最新的 commitid\nGit 重置与版本变化关系图 上述图，\n 1 代表更新引用指向，即引用指向新的 commit 2 代表暂存区的内容与版本库保持一致 3 代表工作区的内容与暂存区保持一致  使用不同的参数，执行的操作不一样：\n --hard 参数，上图 1、2、3 这三步全部执行 --soft 参数，上图 1 执行 --mixed 参数，上图 1、2 执行 不使用参数，等同于使用了 --mixed 参数  根据上述解释，我们来看几个例子：\n彻底回退到上一次提交 git reset --hard HEAD^  HEAD^ 指：HEAD 的父提交，即上一次提交。注意 --hard 选项会将本地工作区的内容也恢复为上一次提交，且不可恢复，所以此命令慎用！！！\n 彻底回退到某一次 commit 根据 commit id 回退到某一次的提交：\ngit reset --hard 9e8a761  9e8a761 代表完整 commit id 的前 6 位，一般前 6 位就可以定位出一次 commit 了。\n 将文件改动撤出暂存区 git reset # 或 git reset HEAD # 或 git reset -- fileName # 或 git reset HEAD fileName 上述命令，相当于对 git add 命令的反向操作，其可以撤出所有 add 进暂存区的文件，也可以单独针对某个 fileName 文件做出撤出。\n撤出后，你对这个文件的改动依然保留在本地，只不过这个文件没有 add 进暂存区而已，没有什么损失。\n软回退到上一次提交 git reset --soft HEAD^ 工作区和暂存区保持不变，但是引用向前回退一次。当对最新提交的修改、或者提交的修改说明不满意的时候，可以使用这个命令撤销最新的提交，以便重新提交。\n工作中常用的 git commit --amend 命令（用于对最新的提交修改或提交说明进行修改，以便重新进行提交）就是相当于执行了下面两条命令：\ngit reset --soft HEAD^ git commit -e -F .git/COMMIT_EDITMSG 如何恢复 reset 在执行 reset 之前，一定要使用 git log 记录好最近的几次 commitId 记录，当出现问题的时候，还可以恢复到指定的 commitId 提交上。\n不过如果之前忘记记录的话，也可以通过查看 cat .git/logs/HEAD 日志文件内容，来找到有关 commit id 的记录。\nGit 本身也提供了 git reflog 命令，来查看日志信息：\ngit reflog show master 参考  Git权威指南  扫描下面二维码，在手机端阅读：\n"});index.add({'id':72,'href':'/docs/tutorial/network/http3/','title':"HTTP3",'content':"HTTP3 HTTP/1.1 和 HTTP/2 使用 TCP 传输协议，HTTP/3 使用 QUIC (Quick UDP Internet Connections) 传输层协议，其是由 Google 开发的一种基于 UDP 的传输协议。之所以用 QUIC 协议的最主要原因，是为了彻底解决队头阻塞问题。\nQUIC TCP 队头阻塞 HTTP/2 解决了 HTTP 的队头拥塞（head of line blocking）问题，客户端无须等待一个请求完成才能发送下一个请求，但它解决的只是 HTTP 的队头阻塞问题。\n如果 HTTP/2 连接双方的网络中有一个数据包丢失，或者任何一方的网络出现中断，整个TCP连接就会暂停，丢失的数据包需要被重新传输。因为TCP是一个按序传输的链条，因此如果其中一个点丢失了，链路上之后的内容就都需要等待。\n这种单个数据包造成的阻塞，就是 TCP 上的队头阻塞（head of line blocking）。\n安全性 QUIC 始终保证安全性。QUIC 协议没有明文的版本，所以想要建立一个 QUIC 连接，就必须通过 TLS 1.3 来安全地建立一个加密连接。QUIC 只在加密协议协商时会发送几个明文传送的初始握手报文。\n减少延迟 TCP 需要 3 次握手，QUIC 提供了 0-RTT 和 1-RTT 握手，减少了协商和建立连接所需要的时间。\nHTTP3 协议特点 协议栈 可靠性 虽然 UDP 不提供可靠的传输，但 QUIC 在基于 UDP 之时增加了一层带来可靠性的层。它提供了数据包重传、拥塞控制、调整传输节奏（pacing）以及其他一些TCP中存在的特性。\n只要连接没有中断，从QUIC一端传输的数据迟早会出现在另一端。\n数据流 QUIC在同一物理连接上可以有多个独立的逻辑数据流。这些数据流并行在同一个连接上传输，不影响其他流。在已建立的连接上，双方均可以建立传输给对方的数据流。单一数据流的传输是可靠、有序的，但不同的数据流间可能无序传送。QUIC可对连接和数据流分别进行流量控制（flow control）。\n快速握手 QUIC提供0-RTT和1-RTT的连接建立，这意味着QUIC在最佳情况下不需要任何的额外往返时间便可建立新连接。其中更快的0-RTT仅在两个主机之间建立过连接且缓存了该连接的“秘密”（secret）时可以使用。\nTLS 1.3 握手所花费的往返次数更低，从而能降低协议的延迟。\nHTTP/2 vs HTTP/3  HTTP/3面向QUIC设计，QUIC是一个自己处理数据流的传输层协议。 HTTP/2面向TCP设计，因此数据流在HTTP层处理。  相似之处 这两个协议为客户端提供了几乎相同的功能集。\n 两者都提供数据流 两者都提供服务器推送 两者都有头部压缩，QPACK与HPACK的设计非常类似 两者都通过单一连接上的数据流提供复用 两者都提供数据流的优先度设置  不同之处  两个协议的主要不同点在于细节，不同之处主要由HTTP/3使用的QUIC带来。 得益于QUIC的0-RTT握手，HTTP/3可以提供更好的早期数据支持，而TCP快速打开和TLS通常只能传输更少的数据，且经常存在问题。 得益于QUIC，HTTP/3的握手速度比TCP+TLS快得多。 HTTP/3不存在明文的不安全版本。尽管在互联网上很少见，HTTP/2还是可以不配合HTTPS来实现和使用。 通过ALPN拓展，HTTP/2可以直接在TLS握手时进行协商。HTTP/3基于QUIC，所以需要凭借响应中的 Alt-Svc: 头部来向客户端宣告。  参考  HTTP/3 HTTP/3 explained  "});index.add({'id':73,'href':'/docs/tutorial/front-end-optimization-guide/js-optimization/','title':"JS 优化",'content':"JS 优化 本文介绍常见的优化 JS、提升 JS 加载性能的优化方法！\n提升加载性能 script 放入到 body 中 \u0026lt;script\u0026gt; 标签经常以下面的这种方式引入：\n\u0026lt;script src=\u0026#34;script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 当 HTML 解析器看到这一行代码时，就会请求获取脚本，并执行脚本。一旦这个过程完成，解析就可以继续，剩下的 HTML 也可以被分析。所以你可以想象，这个操作会对页面的加载时间产生多么大的影响。如果脚本加载的时间比预期的稍长，例如，如果网络有点慢，或者如果您在移动设备上，并且网速特别慢，则在加载和执行脚本之前，访问者可能会看到一个空白页。\n所以推荐将 script 标签从 \u0026lt;head\u0026gt; 位置挪到 \u0026lt;/body\u0026gt; 标签前。如果你这样做了，脚本在所有页面都被解析和加载之后才会加载和执行，这是对 \u0026lt;head\u0026gt; 替代方案的巨大改进。\n\u0026lt;script defer src=\u0026#34;script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Async 和 Defer 如果不考虑兼容旧浏览器，那么 async 和 defer 这两个布尔属性值，会是提升页面加载速度的更好选择：\n\u0026lt;script async src=\u0026#34;script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 这两个属性都可以达到异步加载和执行 script 标签的目的，如果同时指定了两个，那么 async 优先级高一点，老一点的浏览器不支持 async 会降级到 defer。这些属性只有在页面的 \u0026lt;head\u0026gt; 部分使用 \u0026lt;script\u0026gt; 时才有意义，如果像我们上面看到的那样将脚本放在 \u0026lt;body\u0026gt; 中，则这些属性是无用的。\n使用 async 会阻塞 HTML 的解析：\n使用 defer 并不会阻塞 HTML 的解析：\n因此在使用 script 时，要加快页面加载速度，最好的方法是将它们放在 \u0026lt;head\u0026gt; 中，并在 \u0026lt;script\u0026gt; 中添加一个 defer 属性：\n\u0026lt;script defer src=\u0026#34;script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; JS 变量和函数优化 尽量使用 ID 选择器 使用 ID 选择器来选择元素永远都是最快的！\n不要使用 eval eval 函数的三宗罪：\n 不正确使用 eval 会让代码变得更容易注入攻击 调试可能更具挑战性（没有行号等） eval 代码执行速度较慢（没有机会编译/缓存 eval 代码）  JS 事件节流 假设您有一个滚动事件处理程序，当用户在页面上向下移动时，您想在其中向用户显示新内容。如果我们在用户每次滚动单个像素时就执行回调，那么如果他们快速滚动事件，我们的页面将会变得巨卡无比，因为它将快速连续发送数百或数千个事件。相反，我们对其进行限制，比如仅检查每100毫秒滚动一次的数量，这样每秒仅获得10个回调。用户仍然可以立即感觉到响应，并且计算效率更高。可以看到，通过限制这些回调或者比如频繁的 API 请求等，可以防止应用程序卡住或对服务器不必要的请求。\n节流函数示例：\n// Pass in the callback that we want to throttle and the delay between throttled events const throttle = (callback, delay) =\u0026gt; { // Create a closure around these variables.  // They will be shared among all events handled by the throttle.  let throttleTimeout = null; let storedEvent = null; // This is the function that will handle events and throttle callbacks when the throttle is active.  const throttledEventHandler = event =\u0026gt; { // Update the stored event every iteration  storedEvent = event; // We execute the callback with our event if our throttle is not active  const shouldHandleEvent = !throttleTimeout; // If there isn\u0026#39;t a throttle active, we execute the callback and create a new throttle.  if (shouldHandleEvent) { // Handle our event  callback(storedEvent); // Since we have used our stored event, we null it out.  storedEvent = null; // Create a new throttle by setting a timeout to prevent handling events during the delay.  // Once the timeout finishes, we execute our throttle if we have a stored event.  throttleTimeout = setTimeout(() =\u0026gt; { // We immediately null out the throttleTimeout since the throttle time has expired.  throttleTimeout = null; // If we have a stored event, recursively call this function.  // The recursion is what allows us to run continusously while events are present.  // If events stop coming in, our throttle will end. It will then execute immediately if a new event ever comes.  if (storedEvent) { // Since our timeout finishes:  // 1. This recursive call will execute `callback` immediately since throttleTimeout is now null  // 2. It will restart the throttle timer, allowing us to repeat the throttle process  throttledEventHandler(storedEvent); } }, delay); } }; // Return our throttled event handler as a closure  return throttledEventHandler; }; 如何使用这个节流器呢？\nvar returnedFunction = throttle(function() { // Do all the taxing stuff and API requests }, 500); window.addEventListener(\u0026#39;scroll\u0026#39;, returnedFunction); 事件委托 如下图所示，“+ Add Character” 按钮可以动态地给 DOM 增加复选框，复选框本身又可以进行点击，那么新增的复选框点击的事件如何绑定上？针对此问题，事件委托的解决方案是指将拦截点击的事件监听器绑定在父元素 \u0026lt;ul\u0026gt; 上，而非单个 input 上。\n最好把事件委托看作是负责任的父母和疏忽大意的孩子。父母基本上是神，孩子们必须听父母的话。美妙的是，如果我们增加更多的孩子（更多的输入），父母会保持不变-他们从一开始就在那里（ul 元素在页面加载的时候就存在）。\n事件委托的代码：\n\u0026lt;ul class=”characters”\u0026gt; // PARENT -- 监听器绑定在这个元素上 ! \u0026lt;li\u0026gt; \u0026lt;input type=”checkbox” data-index=”0\u0026#34; id=”char0\u0026#34;\u0026gt; //CHILD 1 \u0026lt;label for=”char0\u0026#34;\u0026gt;Mickey\u0026lt;/label\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;input type=”checkbox” data-index=”1\u0026#34; id=”char1\u0026#34;\u0026gt; //CHILD 2 \u0026lt;label for=”char1\u0026#34;\u0026gt;Minnie\u0026lt;/label\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;input type=”checkbox” data-index=”2\u0026#34; id=”char2\u0026#34;\u0026gt; //CHILD 3 \u0026lt;label for=”char2\u0026#34;\u0026gt;Goofy\u0026lt;/label\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;script\u0026gt; //Event Delegation function toggleDone (event) { if (!event.target.matches(‘input’)) return console.log(event.target) //We now have the correct input - we can manipulate the node here } const characterList = document.querySelector(\u0026#39;.characters\u0026#39;); characterList.addEventListener(\u0026#39;click\u0026#39;, toggleDone); \u0026lt;/script\u0026gt; 上述，当点击任意一个 input 的时候，event.target 的值指向的是哪个元素发生了点击：\n而 event.currentTarget 代表的是事件监听器绑定在哪个元素上：\n更进一步，当点击 input 元素的时候，这个事件会沿着自己的父链向上传递，所以任何一个父节点都能感受到这个事件，这称之为事件冒泡。\n如果没有事件委托，则必须将单击事件监听器重新绑定到加载到页面的每个新输入，这是复杂和繁重的编码任务。首先，它会大幅增加页面上事件监听器的数量，更多的事件监听器会增加页面的总内存占用。内存占用量越大，性能越差……这是一件坏事。其次，可能存在与绑定和解除绑定事件监听器以及从 DOM 中删除元素相关联的内存泄漏问题。\nJS 动画优化 尽量使用 CSS 动画 如果 CSS 动画能够满足要求，那么尽量使用 CSS 动画，因为其可以充分利用浏览器提供的优化、甚至可以使用 GPU 来提高性能。而 JS 动画涉及到导入库、JS 执行、学习 JS 动画库提供的 API 等，成本高。\n使用 requestAnimationFrame  浏览器可以优化它，所以动画看起来更流畅 当页面不可见的时候，动画自动停止运行，让 CPU 歇会儿 比较省电  JavaScript 函数应该保持简洁 将大量 JS 动画添加到页面中，一定要想好。如果你的页面开始变卡，代码看起来不太灵活，那么可以使用 Web Workers 来尝试将 JS 动画放到另外一个线程来执行。\n参考  Efficiently load JavaScript with defer and async Why is using the JavaScript eval function a bad idea? Throttle in JavaScript: Improve Your Application’s Performance Part 4: What is Event Delegation in JavaScript? Tips for Improving CSS and JS Animation Performance  扫描下面二维码，在手机端阅读：\n"});index.add({'id':74,'href':'/docs/rocketmq/rocketmq-message-filter-flow/','title':"RocketMQ 消息过滤流程",'content':"RocketMQ 消息过滤流程 讲述 RocketMQ 消息过滤流程\n一、消息过滤类型 Producer 在发送消息的时候可以指定消息的标签类型，还可以为每一个消息添加一个或者多个额外的属性:\n// 指定标签 Message msg = new Message(\u0026#34;TopicTest\u0026#34;, \u0026#34;TagA\u0026#34;, (\u0026#34;Hello RocketMQ\u0026#34;).getBytes(RemotingHelper.DEFAULT_CHARSET)); // 添加属性 a msg.putUserProperty(\u0026#34;a\u0026#34;, 5); 根据标签和属性的不同，RocketMQ 客户端在消费消息的时候有三种消息过滤类型:\n(1) 标签匹配 consumer.subscribe(\u0026#34;TopicTest\u0026#34;, \u0026#34;TagA | TagB | TagC\u0026#34;); (2) SQL 匹配 consumer.subscribe(\u0026#34;TopicTest\u0026#34;, MessageSelector.bySql( \u0026#34;(TAGS is not null and TAGS in (\u0026#39;TagA\u0026#39;, \u0026#39;TagB\u0026#39;))\u0026#34; + \u0026#34;and (a is not null and a between 0 3)\u0026#34;)); (3) 自定义匹配 客户端实现 MessageFilter 类，自定义过滤逻辑:\nClassLoader classLoader = Thread.currentThread().getContextClassLoader(); File classFile = new File(classLoader.getResource(\u0026#34;MessageFilterImpl.java\u0026#34;).getFile()); String filterCode = MixAll.file2String(classFile); consumer.subscribe(\u0026#34;TopicTest\u0026#34;, \u0026#34;org.apache.rocketmq.example.filter.MessageFilterImpl\u0026#34;,filterCode); 对于 MessageFilter 类实现 match 方法即可:\npublic class MessageFilterImpl implements MessageFilter { @Override public boolean match(MessageExt msg, FilterContext context) { String property = msg.getProperty(\u0026#34;SequenceId\u0026#34;); if (property != null) { int id = Integer.parseInt(property); if (((id % 10) == 0) \u0026amp;\u0026amp; (id \u0026gt; 100)) { return true; } } return false; } } 下面我们一一讲解各自背后的机制与实现原理。\n二、标签匹配 当为消息指定消息标签类型的时候，实际上所指定的标签例如 TagA 是作为一个属性放入到了这条消息中的:\npublic class Message implements Serializable { public void setTags(String tags) { this.putProperty(MessageConst.PROPERTY_TAGS, tags); } } 当这条消息到达 Broker 服务器端后，用户设置的标签会计算为标签码，默认的计算方式采用的标签字符串的 hashCode() 作为计算结果的:\npublic class CommitLog { public DispatchRequest checkMessageAndReturnSize(java.nio.ByteBuffer byteBuffer, final boolean checkCRC, final boolean readBody) { // ...  String tags = propertiesMap.get(MessageConst.PROPERTY_TAGS); if (tags != null \u0026amp;\u0026amp; tags.length() \u0026gt; 0) { tagsCode = MessageExtBrokerInner .tagsString2tagsCode(MessageExt.parseTopicFilterType(sysFlag), tags); } // ...  } } 当计算出来标签码之后，这条消息的标签码会被存放至消费队列文件中，用来与消费者客户端消费队列的标签码进行匹配。消费者客户端订阅消费话题的时候，会指定想要匹配的标签类型:\nconsumer.subscribe(\u0026#34;TopicTest\u0026#34;, \u0026#34;TagA | TagB | TagC\u0026#34;); 这段代码在内部实现中利用 FilterAPI 构建了一个 SubscriptionData 对象:\npublic class DefaultMQPushConsumerImpl implements MQConsumerInner { public void subscribe(String topic, String subExpression) throws MQClientException { SubscriptionData subscriptionData = FilterAPI .buildSubscriptionData(this.defaultMQPushConsumer.getConsumerGroup(), topic, subExpression); // ...  } } 当用户未指定标签或者指定为星号标签的时候，则代表用户接受所有标签的消息。如果用户指定了一个或者多个标签，那么会将每一个标签取其 hashCode() 放入到 codeSet 中。SubscriptionData 还有一个 expressionType 字段，在使用标签匹配的时候，其不会设置这个这个字段的值，因此其保留为 null。在这些信息设置好以后，当客户端发送心跳包的时候，会将这些话题的注册信息一并上传至 Broker 服务器端，方便在 Broker 端进行匹配。\npublic class SubscriptionData implements Comparable\u0026lt;SubscriptionData\u0026gt; { public final static String SUB_ALL = \u0026#34;*\u0026#34;; private Set\u0026lt;String\u0026gt; tagsSet = new HashSet\u0026lt;String\u0026gt;(); private Set\u0026lt;Integer\u0026gt; codeSet = new HashSet\u0026lt;Integer\u0026gt;(); private String expressionType; } 当 Broker 端服务器在取消息的时候，每取出来一条消息，都会执行两道过滤机制:\n ConsumeQueue 文件匹配 CommitLog 文件匹配  任一检查没有通过后，绝不会放行这条消息给客户端:\npublic class DefaultMessageStore implements MessageStore { public GetMessageResult getMessage(final String group, /** 其他参数 **/) { for (; i \u0026lt; bufferConsumeQueue.getSize() \u0026amp;\u0026amp; i \u0026lt; maxFilterMessageCount; i += ConsumeQueue.CQ_STORE_UNIT_SIZE) { // ConsumeQueue 文件匹配  if (messageFilter != null \u0026amp;\u0026amp; !messageFilter.isMatchedByConsumeQueue(isTagsCodeLegal ? tagsCode : null, extRet ? cqExtUnit : null)) { if (getResult.getBufferTotalSize() == 0) { status = GetMessageStatus.NO_MATCHED_MESSAGE; } continue; } // CommitLog 文件匹配  if (messageFilter != null \u0026amp;\u0026amp; !messageFilter.isMatchedByCommitLog(selectResult.getByteBuffer().slice(), null)) { if (getResult.getBufferTotalSize() == 0) { status = GetMessageStatus.NO_MATCHED_MESSAGE; } // release...  selectResult.release(); continue; } } } } 消息过滤器的默认实现是 ExpressionMessageFilter ，消息过滤的默认实现策略就是看这个话题的标签码集合中是否包括当前这条消息的标签码:\npublic class ExpressionMessageFilter implements MessageFilter { @Override public boolean isMatchedByConsumeQueue(Long tagsCode, ConsumeQueueExt.CqExtUnit cqExtUnit) { // ...  if (ExpressionType.isTagType(subscriptionData.getExpressionType())) { if (tagsCode == null) { return true; } if (subscriptionData.getSubString().equals(SubscriptionData.SUB_ALL)) { return true; } return subscriptionData.getCodeSet().contains(tagsCode.intValue()); } // ...  return true; } @Override public boolean isMatchedByCommitLog(ByteBuffer msgBuffer, Map\u0026lt;String, String\u0026gt; properties) { if (ExpressionType.isTagType(subscriptionData.getExpressionType())) { return true; } // ...  } } 下图是一幅标签匹配的简要流程图:\n三、SQL 匹配 在发送消息的时候，可以为每一条消息附带一个或者多个属性值，SQL 匹配指的就是依据这些属性值和 TAG 标签 是否满足一定的 SQL 语句条件，来过滤消息。用户如果想要开启 SQL 匹配，那么需要在 Broker 启动的时候，启用如下几个配置信息:\nbrokerConfig.setEnablePropertyFilter(true); brokerConfig.setEnableCalcFilterBitMap(true); messageStoreConfig.setEnableConsumeQueueExt(true); (1) 注册过滤信息 我们在消费者如何接受消息一文中提到过，消费者启动之后，会通过心跳包定时给 Broker 服务器汇报自己的信息。而 Broker 服务器在收到消费者的心跳包之后，会产生一个注册事件，如下所示:\npublic class ConsumerManager { public boolean registerConsumer(final String group, /** 其他参数 **/) { // ...  this.consumerIdsChangeListener.handle(ConsumerGroupEvent.REGISTER, group, subList); // ...  } } DefaultConsumerIdsChangeListener 是默认的消费者列表注册事件通知器的实现类，其在收到注册事件以后，会将用户在消费者端订阅的话题信息注册到 ConsumerFilterManager 中:\npublic class DefaultConsumerIdsChangeListener implements ConsumerIdsChangeListener { @Override public void handle(ConsumerGroupEvent event, String group, Object... args) { switch (event) { case REGISTER: Collection\u0026lt;SubscriptionData\u0026gt; subscriptionDataList = (Collection\u0026lt;SubscriptionData\u0026gt;) args[0]; this.brokerController.getConsumerFilterManager().register(group, subscriptionDataList); break; // ...  } } } ConsumerFilterData 中包含了消费者客户端注册的 SQL 表达式，由上图我们可以看到对于每一个话题所对应的 FilterDataMapByTopic ，可以注册多个 SQL 表达式。但是这里需要注意的是，这多个 SQL 表达式是按照组来做区分的，就是说一个组只能有一个 SQL 表达式，客户端如果在一个组中注册了多个不同的 SQL 表达式，那么后注册的会覆盖掉前注册的。因此，如果想要对同一个组使用不同的 SQL 语句来过滤自己想要的信息，这些不同的 SQL 语句必须划分到不同的组里面才可行。\n(2) 生成 BloomFilterData 布隆过滤器 (BloomFilter) 是一种空间效率很高的数据结构，其可以用来判断某个元素是否可能存在于某个集合中。当判断结果返回 true 的时候，表示可能存在，当返回 false 的时候，表示这个元素一定不存在于这个集合中。\n它的原理是当一个元素被加入集合时，通过 k 个 Hash 函数将这个元素映射成一个长度为 m 位数组（Bit array）中的 k 个点，把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了：\n 如果这些点有任何一个 0，则被检索元素一定不在。 如果都是 1， 则被检索元素很可能在。  如下是一个采用位数组长度为 m=18 以及哈希函数个数为 k=3 实现的布隆过滤器，”x,y,z” 每一个字母都需要经过 3 次哈希函数的计算，然后映射到 3 个不同的槽中。由于字母 “w” 在经过 3 次哈希函数计算后，其中一次产生的哈希值并未命中已有的槽，因此可以确定的是 “w” 肯定不存在于这个集合中。\n在 RocketMQ 的实现中，其有四个最关键的值:\npublic class BloomFilter { // 最大错误率  private int f; // 可能插入 n 个元素  private int n; // k 个哈希函数  private int k; // 数组总共 m 位  private int m; } RocketMQ 实现的布隆过滤器是根据错误率 f 和可能插入的元素数量 n 计算出来的 k 和 m，在默认配置情况下，即如下 n = 32 和 f = 20，计算出来需要 k = 3 个哈希函数和 m = 112 位的数组。\npublic class BrokerConfig { // Expect num of consumers will use filter.  private int expectConsumerNumUseFilter = 32; // Error rate of bloom filter, 1~100.  private int maxErrorRateOfBloomFilter = 20; } 我们这里大致了解以下布隆过滤器的一个基本想法即可，具体算法比较复杂，也不在讨论范畴以内。当客户端注册过滤信息的时候，其会根据 “组#话题” 这个字符串计算出相应的位映射数据，也即这个字符串经过布隆过滤器中的若干个哈希函数得到的几个不同的哈希值:\npublic class ConsumerFilterManager extends ConfigManager { public boolean register(final String topic, /** 其它参数 **/) { // ...  BloomFilterData bloomFilterData = bloomFilter.generate(consumerGroup + \u0026#34;#\u0026#34; + topic); // ...  } } ConsumerFilterManager 中的话题过滤信息数据，每隔 10 秒进行一次磁盘持久化:\npublic class BrokerController { public boolean initialize() throws CloneNotSupportedException { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { BrokerController.this.consumerFilterManager.persist(); } }, 1000 * 10, 1000 * 10, TimeUnit.MILLISECONDS); } } 磁盘文件 consumerFilter.json 中保存的数据信息如下示例:\n上述大致流程图如下所示：\n(3) 编译 SQL 语句 JavaCC (Java Compiler Compiler) 是一个能生成语法和词法分析器的生成程序，它通过阅读一个自定义的语法标准文件 (通常以 jj 为后缀名) ，然后就能生成能够解析该语法的扫描器和解析器的代码。\n通过执行 javacc SelectorParser.jj 命令以后，其会生成如下七个 Java 文件，用以解析 SQL 语法:\n过滤器工厂 FilterFactory 在初次使用的时候，会注册一个 SqlFilter 类，这个类能够将消费者端指定的 SQL 语句编译解析为 Expression 表达式对象，方便后续消息的快速匹配与过滤。\npublic class SqlFilter implements FilterSpi { @Override public Expression compile(final String expr) throws MQFilterException { return SelectorParser.parse(expr); } } (4) 计算位映射 当 Broker 服务器接收到新的消息到来之后，一直在后台运行的 ReputMessageService 会负责将这条消息封装为一个 DispatchRequest 分发请求，这个请求会传递给提前构建好的分发请求链。在 DefaultMessageStore 的构造函数中，我们看到依次添加了构建消费队列和构建索引的分发请求服务:\npublic class DefaultMessageStore implements MessageStore { public DefaultMessageStore(final MessageStoreConfig messageStoreConfig, /** 其它参数 **/) throws IOException { this.dispatcherList = new LinkedList\u0026lt;\u0026gt;(); this.dispatcherList.addLast(new CommitLogDispatcherBuildConsumeQueue()); this.dispatcherList.addLast(new CommitLogDispatcherBuildIndex()); } } 而在 Broker 初始化的时候，我们看到其又添加了计算位映射的分发请求服务，并且将此分发服务放在链表的第一个位置:\npublic class BrokerController { public boolean initialize() throws CloneNotSupportedException { this.messageStore.getDispatcherList() .addFirst(new CommitLogDispatcherCalcBitMap(this.brokerConfig, this.consumerFilterManager)); } } 由此，在每次收到新的消息之后，分发请求的需要经过如下三个分发请求服务进行处理:\n我们在这部分只介绍计算位映射的服务类实现。如下，dispatch 方法用来分发请求里面的消息，对于这每一条消息，首先根据话题取得所有的消费过滤数据。这每一条数据代表的就是一条 SQL 过滤语句信息。我们在这个地方，需要一一遍历这些过滤信息，从而完成计算位服务的需求:\npublic class CommitLogDispatcherCalcBitMap implements CommitLogDispatcher { @Override public void dispatch(DispatchRequest request) { Collection\u0026lt;ConsumerFilterData\u0026gt; filterDatas = consumerFilterManager.get(request.getTopic()); Iterator\u0026lt;ConsumerFilterData\u0026gt; iterator = filterDatas.iterator(); while (iterator.hasNext()) { ConsumerFilterData filterData = iterator.next(); // ...  } } } 在拿到 ConsumerFilterData 信息之后，其会根据这条信息内的 SQL 语句编译后的表达式来对这条消息进行检查匹配 (evaluate)，看这条消息是否满足 SQL 语句所设置的条件。如果满足，那么会将先前在客户端注册阶段计算好的 BloomFilterData 中的映射位信息设置到 filterBitMap 中，即将相应的位数组 BitsArray 中的相应位设置为 1 。在验证完所有的 SQL 语句之后，会将这些所有的字节数组放置到 request 请求之中，以便交由下一个请求分发服务进行使用:\n@Override public void dispatch(DispatchRequest request) { BitsArray filterBitMap = BitsArray.create(this.consumerFilterManager.getBloomFilter().getM()); while (iterator.hasNext()) { ConsumerFilterData filterData = iterator.next(); MessageEvaluationContext context = new MessageEvaluationContext(request.getPropertiesMap()); Object ret = filterData.getCompiledExpression().evaluate(context); // eval true  if (ret != null \u0026amp;\u0026amp; ret instanceof Boolean \u0026amp;\u0026amp; (Boolean) ret) { consumerFilterManager .getBloomFilter() .hashTo(filterData.getBloomFilterData(), filterBitMap); } } request.setBitMap(filterBitMap.bytes()); } (5) 存储位映射 MessageStore 在开启扩展消费队列的配置之后，每一个消费队列在创建的时候，都会额外创建一个扩展消费队列。每一个扩展消费队列文件的大小默认为 48MB:\npublic class ConsumeQueue { public ConsumeQueue(final String topic, /** 其它参数 **/) { // ...  if (defaultMessageStore.getMessageStoreConfig().isEnableConsumeQueueExt()) { this.consumeQueueExt = new ConsumeQueueExt(topic, /** 其它参数 **/); } } } 在计算位映射一节中，计算好位字节数组之后，我们这里需要通过第二个分发请求服务 CommitLogDispatcherBuildConsumeQueue 来存储这些字节信息。通过如下代码，我们知道它将请求中的位映射信息、消息存储时间、标签码这三条信息封装为 ConsumeQueueExt.CqExtUnit ，然后放入到扩展消费队列文件中。\npublic class ConsumeQueue { public void putMessagePositionInfoWrapper(DispatchRequest request) { long tagsCode = request.getTagsCode(); if (isExtWriteEnable()) { ConsumeQueueExt.CqExtUnit cqExtUnit = new ConsumeQueueExt.CqExtUnit(); cqExtUnit.setFilterBitMap(request.getBitMap()); cqExtUnit.setMsgStoreTime(request.getStoreTimestamp()); cqExtUnit.setTagsCode(request.getTagsCode()); long extAddr = this.consumeQueueExt.put(cqExtUnit); if (isExtAddr(extAddr)) { tagsCode = extAddr; } } } } 我们注意到在上述代码中，put 函数返回的是一个 long 类型的扩展地址，当这个数值满足 isExtAddr 要求后，其会将当前的标签码设置为刚才返回的扩展地址。那么这是为什么呢?\n我们首先来看 ConsumeQueueExt 文件在存放数据成功后是如何返回信息的:\npublic class ConsumeQueueExt { public static final long MAX_ADDR = Integer.MIN_VALUE - 1L; public long put(final CqExtUnit cqExtUnit) { if (mappedFile.appendMessage(cqExtUnit.write(this.tempContainer), 0, size)) { return decorate(wrotePosition + mappedFile.getFileFromOffset()); } return 1; } public long decorate(final long offset) { if (!isExtAddr(offset)) { return offset + Long.MIN_VALUE; } return offset; } public static boolean isExtAddr(final long address) { return address \u0026lt;= MAX_ADDR; } } MAX_ADDR 是一个很小很小的值，为 -2147483649， 即写入位置如果不小于这个值，那么我们就认定为它不是扩展地址。需要将修正后的 写入偏移量 + Long.MIN_VALUE 确定为扩展地址。当读取信息的时候，其先读取 ConsumeQueue 文件中的最后的 Hash 标签码值，如果其通过 isExtAddr() 函数返回的是 true，那么我们就可以使用这个地址，再通过一个叫做 unDecorate() 函数将其修正为正确的 ConsumeQueueExt 文件的写入地址，从而接着读取想要的信息:\npublic long unDecorate(final long address) { if (isExtAddr(address)) { return address - Long.MIN_VALUE; } return address; } 这个地方，我们发现 ConsumeQueue 中的最后一个 long 型数值，可能存储的是标签 Hash 码，也可能存储的是扩展消费队列的写入地址，所以需要通过 isExtAddr() 来分情况判断。\n下图为 ConsumeQueue 文件和 ConsumeQueueExt 文件中存取信息的不同:\n(6) 消息过滤 在上小节我们提到了有关扩展消费队列地址和标签 Hash 码存储的不同，所以当在取消息的时候，先得从消费队列文件中取出 tagsCode，然后检查是否是扩展消费队列地址，如果是，那么就需要从扩展消费队列文件中读取正确的标签 Hash 码，如下代码所示：\npublic class DefaultMessageStore implements MessageStore { public GetMessageResult getMessage(final String group, /** 其它参数 **/) { ConsumeQueueExt.CqExtUnit cqExtUnit = new ConsumeQueueExt.CqExtUnit(); for (; i \u0026lt; bufferConsumeQueue.getSize() \u0026amp;\u0026amp; i \u0026lt; maxFilterMessageCount; i += ConsumeQueue.CQ_STORE_UNIT_SIZE) { long tagsCode = bufferConsumeQueue.getByteBuffer().getLong(); boolean extRet = false, isTagsCodeLegal = true; if (consumeQueue.isExtAddr(tagsCode)) { extRet = consumeQueue.getExt(tagsCode, cqExtUnit); if (extRet) { tagsCode = cqExtUnit.getTagsCode(); } else { isTagsCodeLegal = false; } } } } } 当获取到这条消息在扩展消费队列文件中存取的信息后，就会和标签匹配一节所讲述的一致，会进行两道过滤机制。我们先来看第一道 ConsumeQueue 文件匹配:\npublic class ExpressionMessageFilter implements MessageFilter { @Override public boolean isMatchedByConsumeQueue(Long tagsCode, ConsumeQueueExt.CqExtUnit cqExtUnit) { byte[] filterBitMap = cqExtUnit.getFilterBitMap(); BloomFilter bloomFilter = this.consumerFilterManager.getBloomFilter(); BitsArray bitsArray = BitsArray.create(filterBitMap); return bloomFilter.isHit(consumerFilterData.getBloomFilterData(), bitsArray); } } ExpressionMessageFilter 依据 CqExtUnit 中存储的位数组重新创建了比特数组 bitsArray，这个数组信息中已经存储了不同 SQL 表达式是否匹配这条消息的结果。isHit() 函数会一一检查 BloomFilterData 中存储的位信息是否映射在 BitsArray 中。只要有任何一位没有映射，那么就可以立刻判断出这条消息肯定不符合 SQL 语句的条件。\n因为布隆过滤器有一定的错误率，其只能精确的判断消息是否一定不在集合中，返回成功的只能确定为消息可能在集合中。因此通过布隆过滤器检查后还需要经过第二道过滤机制，即 SQL 编译后的表达式亲自验证是否匹配:\npublic class ExpressionMessageFilter implements MessageFilter { @Override public boolean isMatchedByCommitLog(ByteBuffer msgBuffer, Map\u0026lt;String, String\u0026gt; properties) { MessageEvaluationContext context = new MessageEvaluationContext(tempProperties); Object ret = realFilterData.getCompiledExpression().evaluate(context); if (ret == null || !(ret instanceof Boolean)) { return false; } return (Boolean) ret; } } 通过在验证 SQL 表达式是否满足之前，提前验证是否命中布隆过滤器，可以有效的避免许多不必要的验证:\n四、自定义匹配 消息的自定义匹配需要开启过滤服务器、上传过滤类、过滤服务器委托过滤消息等步骤，下面我们一一进行说明。\n(1) 过滤服务器 在启动 Broker 服务器的时候，如果指定了下面一行设置:\nbrokerConfig.setFilterServerNums(int filterServerNums); 即将过滤服务器的数量设定为大于 0，那么 Broker 服务器在启动的时候，将会启动 filterServerNums 个过滤服务器。过滤服务器是通过调用 shell 命令的方式，启用独立进程进行启动的。\npublic class FilterServerManager { public void createFilterServer() { int more = this.brokerController.getBrokerConfig().getFilterServerNums() - this.filterServerTable.size(); String cmd = this.buildStartCommand(); for (int i = 0; i \u0026lt; more; i++) { FilterServerUtil.callShell(cmd, log); } } } 过滤服务器在初始化的时候，会启动定时器每隔 10 秒注册一次到 Broker 服务器:\npublic class FiltersrvController { public boolean initialize() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { FiltersrvController.this.registerFilterServerToBroker(); } }, 3, 10, TimeUnit.SECONDS); } } Broker 服务器在收到来自过滤服务器的注册信息之后，会把过滤服务器的地址信息、注册时间等放到过滤服务器表中:\npublic class FilterServerManager { private final ConcurrentMap\u0026lt;Channel, FilterServerInfo\u0026gt; filterServerTable = new ConcurrentHashMap\u0026lt;Channel, FilterServerInfo\u0026gt;(16); } 同样，Broker 服务器也需要定时将过滤服务器地址信息同步给所有 Namesrv 命名服务器，上述整个流程如下图所示:\n(2) 过滤类 当消费者通过使用自定义匹配过滤消息的时候，这个时候会将存储订阅信息的 SubscriptionData 中的 filterClassSource 设置为 true，以表征这个客户端需要过滤类来进行消息的匹配和过滤。\n消费者客户端在启动过程中，还会定时地上传本地的过滤类源码到过滤服务器:\npublic class MQClientInstance { private void startScheduledTask() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { MQClientInstance.this.sendHeartbeatToAllBrokerWithLock(); } }, 1000, this.clientConfig.getHeartbeatBrokerInterval(), TimeUnit.MILLISECONDS); } public void sendHeartbeatToAllBrokerWithLock() { // ...  this.uploadFilterClassSource(); } } 其中过滤服务器的地址列表是在从 Namesrv 服务器获取话题路由信息的时候取得的，话题路由信息不光存储了消息队列数据，还存储了各个 Broker 所关联的过滤服务器列表:\npublic class TopicRouteData extends RemotingSerializable { // ...  private HashMap\u0026lt;String/* brokerAddr */, List\u0026lt;String\u0026gt;/* Filter Server */\u0026gt; filterServerTable; } 当过滤服务器接收到来自消费者客户端的源码之后，其会首先首先生成一个键为 话题@组 的字符串来查阅过滤类信息是否已经存在于内存里面的 filterClassTable 表中且文件通过 CRC 校验。如果没有存在或校验失败，那么就需要先编译并加载这个类:\npublic class DynaCode { public void compileAndLoadClass() throws Exception { String[] sourceFiles = this.uploadSrcFile(); this.compile(sourceFiles); this.loadClass(this.loadClass.keySet()); } } 默认情况下，编译后的类存放于 $HOME/rocketmq_filter_class/$PID 目录下，类的源文件和类的字节码文件名也会相应的加上当前时间戳来确定:\n上述流程图如下:\n(3) 过滤消息 当消费者客户端启用自定义匹配过滤消息后，发往服务器的数据中也包含了过滤标志位，这样每次拉取消息的服务器也由原来的 Broker 服务器变更为 Filtersrv 过滤服务器，其中过滤服务器地址的选择是随机确定的:\npublic class PullAPIWrapper { public PullResult pullKernelImpl(final MessageQueue mq, /** 其它参数 **/) throws Exception { // ...  if (findBrokerResult != null) { if (PullSysFlag.hasClassFilterFlag(sysFlagInner)) { // 从过滤服务器拉取消息  brokerAddr = computPullFromWhichFilterServer(mq.getTopic(), brokerAddr); } // ...  } } } 过滤服务器在启动的时候，内部还启动了一个 PullConsumer 客户端，用以从 Broker 服务器拉取消息:\npublic class FiltersrvController { private final DefaultMQPullConsumer defaultMQPullConsumer = new DefaultMQPullConsumer(MixAll.FILTERSRV_CONSUMER_GROUP); public void start() throws Exception { this.defaultMQPullConsumer.start(); // ...  } } 当过滤服务器收到真正的消费者发来的消费消息的请求之后，其会委托内部的 PullConsumer 使用包含在请求体内的偏移量去 Broker 服务器拉取所有消息，此时这些消息是完全没有过滤的：\npublic class DefaultRequestProcessor implements NettyRequestProcessor { private RemotingCommand pullMessageForward(final ChannelHandlerContext ctx, final RemotingCommand request) throws Exception { MessageQueue mq = new MessageQueue(); mq.setTopic(requestHeader.getTopic()); mq.setQueueId(requestHeader.getQueueId()); mq.setBrokerName(this.filtersrvController.getBrokerName()); // 设置偏移量和最大数量  long offset = requestHeader.getQueueOffset(); int maxNums = requestHeader.getMaxMsgNums(); // 委托内部消费者从 Broker 服务器拉取消息  pullConsumer.pullBlockIfNotFound(mq, null, offset, maxNums, pullCallback); } } 过滤服务器从 Broker 服务器获取到完整的消息列表之后，会遍历消息列表，然后使用过滤类一一进行匹配，最终将匹配成功的消息列表返回给客户端:\npublic class DefaultRequestProcessor implements NettyRequestProcessor { private RemotingCommand pullMessageForward(final ChannelHandlerContext ctx, final RemotingCommand request) throws Exception { final PullCallback pullCallback = new PullCallback() { @Override public void onSuccess(PullResult pullResult) { switch (pullResult.getPullStatus()) { case FOUND: List\u0026lt;MessageExt\u0026gt; msgListOK = new ArrayList\u0026lt;MessageExt\u0026gt;(); for (MessageExt msg : pullResult.getMsgFoundList()) { // 使用过滤类过滤消息  boolean match = findFilterClass.getMessageFilter().match(msg, filterContext); if (match) { msgListOK.add(msg); } } break; // ...  } } }; // ...  } } 上述流程如下图所示:\n扫描下面二维码，在手机端阅读：\n"});index.add({'id':75,'href':'/docs/rocketmq/','title':"RocketMQ 源码分析",'content':"RocketMQ RocketMQ 是阿里巴巴集团开源的一款分布式消息中间件，其采用纯 Java 语言编写，本博客基于 RocketMQ 4.2.0 版本，为大家分析和讲解其内部几个关键模块的运行原理。\n目录：\n RocketMQ 消息发送流程 RocketMQ 消息存储流程 RocketMQ 消息接受流程 RocketMQ 消息过滤流程 RocketMQ 消息索引流程 RocketMQ 定时消息和重试消息 RocketMQ 主备同步  "});index.add({'id':76,'href':'/docs/tutorial/unix-command/ssh/','title':"ssh",'content':"ssh 本文展示了一些常见的 ssh 命令，了解一些 ssh 技巧将有利于任何系统管理员、网络工程师或安全专业人员。\n连接远程主机 localhost:~$ ssh -v -p 22 -C neo@remoteserver  -v：打印 debug 日志信息，用于打印连接时候的一些日志。 -p 22：指定连接远程主机的哪个端口，默认情况下，不用指定，因为 ssh 默认端口就是 22。编辑 sshd_config 文件，可以修改默认的 ssh 的监听端口。 -C：传输数据的时候，是否对数据启用压缩。 neo@remoteserver：neo 代表远程主机的用户名，remoteserver 代表远程主机的 IP 或者域名。添加 -4 选项，可以只连接 IPv4 连接；添加 -6 选项，只连接 IPv6 连接。  拷贝文件到远程服务器 远程拷贝文件的命令 scp 建立在 ssh 命令之上：\nlocalhost:~$ scp mypic.png neo@remoteserver:/media/data/mypic_2.png  mypic.png：代表本地电脑上的图片 /media/data/mypic_2.png：代表你想把图片拷贝到远程主机的哪个路径  流量代理 SSH 代理特性被放在第1位是有充分理由的。它的功能比许多用户意识到的要强大得多，它允许您使用几乎任何应用程序访问远程服务器可以访问的任何系统。ssh客户机可以仅用一行代码，就可以使用SOCKS代理服务器在连接隧道上通信。\nlocalhost:~$ ssh -D 8888 user@remoteserver localhost:~$ netstat -pan | grep 8888 tcp 0 0 127.0.0.1:8888 0.0.0.0:* LISTEN 23880/ssh 在这里，我们启动在TCP端口8888上运行的socks代理服务器，第二个命令检查端口是否正在监听。127.0.0.1表示服务仅在本地主机上运行。我们可以使用稍微不同的命令来监听所有接口，包括以太网或wifi，这将允许我们网络上的其他应用程序（浏览器或其他）连接到ssh socks代理服务。\nlocalhost:~$ ssh -D 0.0.0.0:8888 user@remoteserver 现在我们可以配置浏览器来连接socks代理。在 Firefox 中选择 preferences | general | network settings。添加浏览器连接到的IP地址和端口。\n生成 SSH Key ssh-keygen 是一个为 ssh 创建新的身份验证密钥对的工具。此类密钥对用于自动登录、单点登录和对主机进行身份验证。\n生成密钥对的最简单方法是不带参数运行 ssh-keygen。在这种情况下，它将提示输入要在其中存储密钥的文件。\nssh-keygen 拷贝 SSH Key 到远程主机 拷贝 ~/.ssh/id_rsa.pub 中的所有内容，追加到远程主机的 ~/.ssh/authorized_keys 这个文件中。这样，下次 ssh 就无需输入密码了。\nlocalhost:~$ ssh-copy-id user@remoteserver 远程执行命令 ssh命令可以链接到其他命令以获得常见的管道乐趣。将要在远程主机上运行的命令添加为引号中的最后一个参数。\nlocalhost:~$ ssh remoteserver \u0026#34;cat /var/log/nginx/access.log\u0026#34; | grep badstuff.php 参考  SSH Examples, Tips \u0026amp; Tunnels SSH Command  扫描下面二维码，在手机端阅读：\n"});index.add({'id':77,'href':'/docs/books/the_transformation_of_enterprise_it_architecture/','title':"企业 IT 架构转型之道",'content':"企业 IT 架构转型之道 共享服务体系搭建 SOA 的主要特性：\n 面向服务的分布式计算。 服务间松散耦合。 支持服务的组装。 服务注册和自动发现。 以服务契约方式定义服务交互方式。  基于 “中心化” 的 ESB 服务调用方式  “去中心化” 服务架构调用方式   数据拆分实现数据库能力线性扩展 数据库的读写分离 读写分离基本原理是让主数据库处理事务性增、改、删（INSERT、UPDATE、DELETE）操作，而从数据库专门负责处理查询（SELECT）操作，在数据库的后台会把事务性操作导致的主数据库中的数据变更同步到集群中的从数据库。\n数据库分库分表 采用分库分表的方式将业务数据拆分后，如果每一条SQL语句中都能带有分库分表键，SQL语句的执行效率最高：\n但不是所有的业务场景在进行数据库访问时每次都能带分库分表键的。比如在买家中心的界面中，要显示买家test1过去三个月的订单列表信息。此时就出现了我们所说的全表扫描，一条SQL语句同时被推送到后端所有数据库中运行。如果是高并发情况下同时请求的话，为了数据库整体的扩展能力，则要考虑下面描述的异构索引手段来避免这样的情况发生。对于在内存中要进行大数据量聚合操作和计算的SQL请求，如果这类SQL的不是大量并发或频繁调用的话，平台本身的性能影响也不会太大，如果这类SQL请求有并发或频繁访问的要求，则要考虑采用其他的平台来满足这一类场景的要求，比如Hadoop这类做大数据量离线分析的产品，如果应用对请求的实时性要求比较高，则可采用如内存数据库或HBase这类平台。\n所谓“异构索引表”，就是采用异步机制将原表内的每一次创建或更新，都换另一个维度保存一份完整的数据表或索引表。本质上这是互联网公司很多时候都采用的一个解决思路：“拿空间换时间”。也就是应用在创建或更新一条按照订单ID为分库分表键的订单数据时，也会再保存一份按照买家ID为分库分表键的订单索引数据。\n基于订单索引表实现买家订单列表查看流程示意：\n实现对数据的异步索引创建有多种实现方式，其中一种就是从数据库层采用 binlog 数据复制的方式实现。\n采用数据异构索引的方式在实战中基本能解决和避免90%以上的跨join或全表扫描的情况，是在分布式数据场景下，提升数据库服务性能和处理吞吐能力的最有效技术手段。但在某些场景下，比如淘宝商品的搜索和高级搜索，因为商品搜索几乎是访问淘宝用户都会进行的操作，所以调用非常频繁，如果采用SQL语句的方式在商品数据库进行全表扫描的操作，则必然对数据库的整体性能和数据库连接资源带来巨大的压力。面对此类场景，我们不建议采用数据库的方式提供这样的搜索服务，而是采用专业的搜索引擎平台来行使这样的职能，如Lucene、Solr、ElasticSearch 等。\n异步化与缓存原则 业务流程异步化 以淘宝的交易订单为例，目前淘宝的订单创建流程需要调用超过200个服务，就算所有服务的调用时间都控制在20ms内返回结果，整个订单创建的时间也会超过4s：\n以异步化方式将上述交易创建过程中，对于有严格先后调用关系的服务保持顺序执行，对于能够同步执行的所有服务均采用异步化方式处理。阿里巴巴内部使用消息中间件的方式实现了业务异步化，提高了服务的并发处理，从而大大减少整个业务请求处理所花的时间。\n数据库事务异步化 扣款是一个要求事务一致性的典型场景，稍微数据不一致带来的后果都可能是成百上千（可能在某些借款项目中达到上百万的金额）的金额差异。所以在传统的实现方式中，整个扣款的逻辑代码都是在一个大的事务中，通过数据库的事务特性来实现这样一个稍显复杂的业务一致性。\n数据库事务的异步化：通俗来说，就是将大事务拆分成小事务，降低数据库的资源被长时间事务锁占用而造成的数据库瓶颈，就能大大提升平台的处理吞吐量和事务操作的响应时间。\n在实际的改造方案中，同样基于消息服务提供的异步机制，将整个还款流程进行异步化的处理：\n事务与柔性事务 不管是业务流程异步化，还是数据库事务异步化，其实都面临一个如何保证业务事务一致性的问题。面对这个问题目前并没有完美的解决方案，本节会介绍淘宝是如何对订单创建场景实现业务一致的实践，以及近一两年来我们在分布式事务上所作出的创新尝试，供各技术同行在解决此类问题时借鉴和参考。\n关于数据库事务，核心是体现数据库ACID（原子性、一致性、隔离性和持久性）属性，即作为一个事务中包含的所有逻辑处理操作在作用到数据库上时，只有这个事务中所有的操作都成功，对数据库的修改才会永久更新到数据库中，任何一个操作失败，对于数据库之前的修改都会失效。在分布式领域，基于CAP理论和在其基础上延伸出的BASE理论，有人提出了“柔性事务”的概念。\n（1）CAP理论\n一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。“一致性”指更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致。“可用性”指用户在访问数据时可以得到及时的响应。“分区容错性”指分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。\nCAP定理并不意味着所有系统的设计都必须抛弃三个要素之中的一个。CAP三者可以在一定程度上衡量，并不是非黑即白的，例如可用性从0%到100%有不同等级。\n（2）BASE理论\nBASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency, CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（EventualConsitency）。BASE是指基本可用（Basically Available）、柔性状态（Soft State）、最终一致性（Eventual Consistency）。\n  “基本可用”是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。\n  “柔性状态”是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是柔性状态的体现。MySQLReplication的异步复制也是一种柔性状态体现。\n  “最终一致性”是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。\n  对于如何实现高可用，我们认为：\n 高可用=系统构建在多机分布式系统 高性能=分布式系统的副产品  分布式系统内通信和单机内通信最大的区别是：单机系统总线不会丢消息，而网络会。一台向另一台机器通信的结果可能是收到、未收到、不知道收到没收到。消息不可靠带来的副作用是：数据或者状态在多机之间同步的成本很高。大家都知道Paxos协议。在多机间通信不存在伪造或篡改的前提下，可以经由Paxos协议达成一致性。成本是发给Paxos系统的信息（数据）需要至少同步发送到一半以上多数（Quorum）的机器确认后，才能认为是成功。这样大幅增加了信息更新的延迟，因此分布式系统的首选不是这种强同步而是最终一致。\n（3）两阶段提交\n数据在按照业务领域（用户中心、交易中心）的不同被拆分到不同的数据库后，在某些业务场景（比如订单创建）下，就必然会出现同一个事务上下文中，需要协调多个资源（数据库）以保证业务的事务一致性，对于这样的场景，业界早就有基于两阶段提交方式实现的分布式事务，两阶段提交协议包含了两个阶段：第一阶段（也称准备阶段）和第二阶段（也称提交阶段）。\nX/Open组织为基于两阶段协议的分布式事务处理系统提出了标准的系统参考模型（X/Open事务模型）以及不同组件间与事务协调相关的接口，使不同厂商的产品能够互操作。X/Open事务模型如图所示。\n从图中可以看出，X/Open模型定义了两个标准接口：TX接口用于应用程序向事务管理器发起事务、提交事务和回滚事务（即确定事务的边界和结果）; XA接口形成了事务管理器和资源管理器之间的通信桥梁，用于事务管理器将资源管理器（如数据库、消息队列等）加入事务、并控制两阶段提交。\n事务管理器一般由专门的中间件提供，或者在应用服务器中作为一个重要的组件提供。资源管理器如数据库、消息队列等产品一般也会提供对XA接口的支持，通过使用符合X/Open标准的分布式事务处理，能够简化分布式事务类应用的开发。\n两阶段提交协议的关键在于“预备”操作。分布式事务协调者在第一阶段通过对所有的分布式事务参与者请求“预备”操作，达成关于分布式事务一致性的共识。分布式事务参与者在预备阶段必须完成所有的约束检查，并且确保后续提交或放弃时所需要的数据已持久化。在第二队段，分布式事务协调者根据之前达到的提交或放弃的共识，请求所有的分布式事务参与者完成相应的操作。很显然，在提交事务的过程中需要在多个资源节点之间进行协调，而各节点对锁资源的释放必须等到事务最终提交时，这样，比起一阶段提交，两阶段提交在执行同样的事务时会消耗更多时间。\n事务执行时间的延长意味着锁资源发生冲突的概率增加，当事务的并发量达到一定数量的时候，就会出现大量事务积压甚至出现死锁，系统性能和处理吞吐率就会严重下滑，也就是系统处理的吞吐率与资源上的时间消耗成反比（参考阿姆达尔定理）。这就是为什么今天在互联网应用场景中鲜有人会选择这样传统的分布式事务方式，而选择柔性事务处理业务事务的主要原因。\n（4）柔性事务如何解决分布式事务问题\n  引入日志和补偿机制。类似传统数据库，柔性事务的原子性主要由日志保证。事务日志记录事务的开始、结束状态，可能还包括事务参与者信息。参与者节点也需要根据重做或回滚需求记录REDO/UNDO日志。当事务重试、回滚时，可以根据这些日志最终将数据恢复到一致状态。为避免单点，事务日志是记录在分布式节点上的，数据REDO/UNDO日志一般记录在业务数据库上，可以保证日志与业务操作同时成功/失败。通常柔性事务能通过日志记录找回事务的当前执行状态，并根据状态决定是重试异常步骤（正向补偿），还是回滚前序步骤（反向补偿）。\n  可靠消息传递。根据“不知道成功还是失败”状态的处理，消息投递只有两种模式：1）消息仅投递一次，但是可能会没有收到；2）消息至少投递一次，但可能会投递多次。在业务一致性的高优先级下，第一种投递方式肯定是无法接受的，因此只能选择第二种投递方式。由于消息可能会重复投递，这就要求消息处理程序必须实现幂等（幂等=同一操作反复执行多次结果不变）。每种业务场景不同，实现幂等的方法也会有所不同，最简单的幂等实现方式是根据业务流水号写日志，阿里内部一般把这种日志叫做排重表。\n  实现无锁。如何很好地解决数据库锁问题是实现高性能的关键所在。所以选择放弃锁是一个解决问题的思路，但是放弃锁并不意味着放弃隔离性。实现事务隔离的方法有很多，在实际的业务场景中可灵活选择以下几种典型的实现方式。\n 避免事务进入回滚。如果事务在出现异常时，可以不回滚也能满足业务的要求，也就是要求业务不管出现任何情况，只能继续朝事务处理流程的顺向继续处理，这样中间状态即使对外可见，由于事务不会回滚，也不会导致脏读。 辅助业务变化明细表。比如对资金或商品库存进行增减处理时，可采用记录这些增减变化的明细表的方式，避免所有事务均对同一数据表进行更新操作，造成数据访问热点，同时使得不同事务中处理的数据互不干扰，实现对资金或库存信息处理的隔离。 乐观锁。数据库的悲观锁对数据访问具有极强的排他性，也是产生数据库处理瓶颈的重要原因，采用乐观锁则在一定程度上解决了这个问题。乐观锁大多是基于**数据版本（Version）**记录机制实现。例如通过在商品表中增加记录版本号的字段，在事务开始前获取到该商品记录的版本号，在事务处理最后对该商品数据进行数据更新时，可通过在执行最后的修改update语句时进行之前获取版本号的比对，如果版本号一致，则update更新数据成功，修改该数据到新的版本号；如果版本号不一致，则表示数据已经被其他事务修改了，则重试或放弃当前事务。     （5）柔性事务在阿里巴巴内部的几种实现\n 消息分布式事务  基于消息实现的分布式事务仅支持正向补偿，即不会像传统事务方式出现异常时依次进行回滚，会通过消息的不断重试或人工干预的方式让该事务链路继续朝前执行，而避免出现事务回滚。\n 支付宝XTS框架  XTS是TCC（Try/Confirm/Cancel）型事务，属于典型的补偿型事务。\n 阿里巴巴AliWare TXC事务服务  标准模式下无需开发人员自行进行事务回滚或补偿的代码，平台支持自动按事务中事务操作的顺序依次回滚和补偿。关键原理：\n大促秒杀活动催生缓存技术的高度使用 首先一定要让负责秒杀场景的商品中心应用实例（图中“秒杀IC”）与满足普通商品正常访问的商品中心应用实例（图中IC）隔离部署，通过服务分组方式，保持两个运行环境的隔离，避免因为秒杀产生的过大访问流量造成整个商品中心的服务实例均受影响，产生太大范围的影响。\n因为秒杀在正式开始前，一定会有大量的用户停留在商品的详情页（图中Detail）等待着秒杀活动的开始，同时伴随有大量的页面刷新访问（心急或担心页面没有正常刷新的买家们），此时，如果每一次刷新都要从后端的商品数据库（图中ICDB）中获取商品相关信息，则一定会给数据库带来巨大的压力，在淘宝早期举办秒杀活动时就出现了秒杀活动还没开始，因为商品详情页访问太大，造成平台提前进入不可访问状态的情况。所以一定是通过缓存服务器（图中Tair），将商品的详细信息（包括库存信息）保存在缓存服务器上，商品详情页和购买页所有有关商品的信息均是通过缓存服务器获取，则无需访问后端数据库。\n如图中“本地缓存”所示，可通过给网页资源设置Expires和Last-Modified返回头信息进行有效控制，从而尽可能减少对后端服务端的访问次数。\n避免商品出现超卖（即成功下单的订单中商品的库存数量大于商品现有的库存量，则称为商品超卖），核心技术是利用数据库的事务锁机制，即不允许同一商品的库存记录在同一时间被不同的两个数据库事务修改。在前柔性事务介绍中所提到的，用户在进行商品下单操作中，会进行一系列的业务逻辑判断和操作，对于商品库存信息这一访问热点数据，如果采用数据库的悲观锁（比如select语句带for update）模式，则会给订单处理带来很大的性能阻塞，所以会采用乐观锁的方式实现商品库存的操作。实现的方式也比较简单，也就是在最后执行库存扣减操作时，将事务开始前获取的库存数量带入到SQL语句中与目前数据库记录中的库存数量进行判断，如果数量相等，则该条更新库存的语句成功执行；如果不相等，则表示该商品的库存信息在当前事务执行过程中已经被其他事务修改，则会放弃该条update的执行，可以采用重试的机制重新执行该事务，避免商品超卖的发生，具体的SQL语句示意如下：\nupdate auction_auctions set quantity = #inQuantity#, where auction_id = #itemId# and quantity = #dbQuantity# 如果参与大促的商品拥有较大库存数量的时候，需要将之前仅仅作为商品信息浏览的缓存的作用，提升到为库存操作提供事务支持的角色。\n打造数字化运营能力 每一个URL请求都会生成一个全局唯一的ID，鹰眼（类似于 Twitter 的 Zipkin）平台中称为TraceID，这个ID会出现在该请求中所有服务调用、数据库、缓存、消息服务访问时生成的所有日志中。因为上述所有的资源访问均是在分布式环境下进行的，如何将该TraceID平滑地传递到各个服务节点上呢？如果要求应用程序中实现服务链路日志的打印和TraceID的传递，则在程序中有大量的日志打印代码，而且需要将TraceID采用业务数据的方式传递给下一服务节点，这些都给应用带来了非常大的代码侵入。\n阿里巴巴在中间件层面上统一实现了鹰眼的上下文创建以及日志埋点功能，让调用上下文在中间件的网络请求中传递，同时将调用上下文信息保存在了本地ThreadLocal中，从而实现了鹰眼平台所需的调用上下文和日志信息对于应用开发人员完全透明。\n埋点日志一般包含：\n TraceID、RPCID、开始时间、调用类型、对端IP。 处理耗时。 处理结果（ResultCode）。 数据传输量：请求大小/响应大小。  打造平台稳定性能力 限流和降级 淘宝技术团队开发的开源模块nginx-http-sysguard，主要用于当访问负载和内存达到一定的阀值之时，会执行相应的动作，比如直接返回503,504或者其他URL请求返回代码，一直等到内存或者负载回到阀值的范围内，站点恢复可用。\n流量调度 流量调度的核心是通过秒级获取服务器系统运行指标以及业务指标，通过流量调度平台设置的决策算法以及规则，当发现满足规则条件的指标状态发生时，对线上环境的服务器进行下线等操作，以屏蔽这些单点或局部出现故障的应用实例对整体平台产生扩展式的影响。\n业务开关 Switch 平台本身所提供的功能比较简单，但对于业务场景和环境复杂的分布式架构，这个平台确实能大大提升应用适应各种不同场景的自动化能力，比如通过开关的方式将正常环境下的应用逻辑切换到适配秒杀场景；当发现升级后的应用出现问题时，只需通过开关切换的方式就能让升级后的应用秒级切换到升级前的业务代码中。最重要的是在平台处于大促秒杀、应用异常时，业务开关在服务降级中所起的作用，相当于平台的最后一道保护屏障。\n"});index.add({'id':78,'href':'/docs/tutorial/network/','title':"网络协议",'content':"计算机网络协议    网络协议          DHCP HTTP HTTP2 HTTP3 HTTPS CDN   DNS TCP UDP       "});index.add({'id':79,'href':'/docs/programmer-interview/design-pattern/','title':"设计模式",'content':"设计模式    行为模式     观察者模式    六大设计原则  单一职责原则：一个类应该只有一个发生变化的原因 开闭原则：一个软件实体，如类、模块和函数应该对扩展开放，对修改关闭 里氏替换原则：所有引用基类的地方必须能透明地使用其子类的对象 迪米特法则：只与你的直接朋友交谈，不跟 “陌生人” 说话。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 接口隔离原则：客户端不应该依赖它不需要的接口；类间的依赖关系应该建立在最小的接口上。 依赖倒置原则：上层模块不应该依赖底层模块，它们都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。  "});index.add({'id':80,'href':'/docs/tutorial/awk/','title':"AWK 教程",'content':"AWK 教程  简介 Patterns  "});index.add({'id':81,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock-with-cooldown/','title':"Best Time to Buy and Sell Stock With Cooldown",'content':"Best Time to Buy and Sell Stock With Cooldown 题目 LeetCode 地址：Best Time to Buy and Sell Stock With Cooldown\n有一个数组，第 i 个元素的值代表第 i 天的股票价格，如果你最多只能进行任意次交易（某天买入一支股票，然后过几天卖掉），你卖出一只股票后，接下来的一天不能买，必须要到后天才能买。也就是说有冷静期1天。请问你能收获的最大利润是多少？\n答案 // https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/ // 交易任意多次，只不过 buy sell 之后的第二天必须 cooldown 隔天才能再次 buy // // https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/discuss/240277/Java-solution-in-Chinese public class BestTimetoBuyandSellStockwithCooldown { public int maxProfit(int[] prices) { if (prices == null || prices.length \u0026lt;= 1) { return 0; } // 买入只能是从前天买入 buy[i] = sell[i - 2] - prices[i];  // 卖出可以昨天卖出 sell[i] = buy[i - 1] + prices[i];  int[] sell = new int[prices.length]; int[] buy = new int[prices.length]; buy[0] = -prices[0]; for (int i = 1; i \u0026lt; prices.length; i++) { // 前天卖出后，剩下 sell 钱，然后又接着买入 sell[i - 2] - prices[1]  // 维持昨天的  buy[i] = Math.max(buy[i - 1] /** 不买 */, (i \u0026gt;= 2 ? sell[i - 2] : 0) - prices[i] /** 从前天卖出的接着买入 */); // 接着买：buy[i - 1] + prices[i]  // 维持昨天的 sell  sell[i] = Math.max(sell[i - 1] /** 不卖 */, buy[i - 1] + prices[i] /** 从昨天接着买入 */); } return sell[prices.length - 1]; } } 扫描下面二维码，在手机上阅读这篇文章：\n"});index.add({'id':82,'href':'/docs/tutorial/git/git-checkout/','title':"Git checkout",'content':"Git checkout git checkout 检出命令，可以用来切换分支、查看某个 commit 的代码等。\ndetached HEAD 当你执行 git checkout [commitId] 时，你会看到下面的文件警告：\nNote: switching to '467dd6520'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command. Example: git switch -c \u0026lt;new-branch-name\u0026gt; Or undo this operation with: git switch - Turn off this advice by setting config variable advice.detachedHead to false HEAD is now at 467dd65 version 3 这代表什么意思？什么叫做 detached HEAD ？执行命令 .git/HEAD 查看一下 HEAD 头指针的指向就会明白。\n$ cat .git/HEAD ref: refs/heads/master # 检出到某一个 commitId $ git checkout 467dd6520 $ cat .git/HEAD 467dd6520221c270098166e2be6389fbb2c3b1b9 原来 detached HEAD 指的是 HEAD 头指针指向了一个具体的 commitId，而不是一个分支（引用）。\n切换分支 切换到 branch-xxx 分支，且暂存区和工作区的文件也会跟着变动：\ngit checkout branch-xxx 撤销本地修改 本地对某个文件做了修改，现在要恢复到修改之前的状态：\ngit checkout -- fileName 撤销本地所有文件的修改：\ngit checkout -- . # 或 git checkout . 创建并切换到新分支 git checkout -b newBranchName  使用 -B 可以强制创建某个分支，并覆盖掉本地已经存在的分支，慎用！\n 扫描下面二维码，在手机端阅读：\n"});index.add({'id':83,'href':'/docs/tutorial/network/https/','title':"HTTPS",'content':"HTTPS 为什么出现 HTTPS 防止 敏感数据 (银行卡号、账号密码等) 被劫持。HTTPS 会对 HTTP 的请求和响应进行加密，因此即使劫持，攻击者看到的也都是一些随机的字符串。\nRSA 应用场景 （1）加密\nBob 想要给 Alice 发送 \u0026ldquo;Hello Alice!\u0026rdquo; 这个信息：\n 公钥加密 私钥解密  （2）数字签名\n 私钥签名 公钥验证  HTTPS 握手 CA 证书验证过程 抓包可以抓到吗 可以。\n参考  Public-key cryptography  "});index.add({'id':84,'href':'/docs/javascript/','title':"JavaScript 专栏",'content':"JavaScript 专栏 本专栏用于整理在 JavaScript 中最常使用的、必知必会的基础知识点，方便大家温故而知新。\n"});index.add({'id':85,'href':'/docs/books/redis_5_source_code/','title':"Redis 5 设计与源码分析",'content':"Redis 5 设计与源码分析 Redis 5.0 新特性  新增Streams数据类型，这是 Redis 5.0 最重要的改进之一。可以把Streams当作消息队列。 新的模块API、定时器、集群及字典。 RDB中持久化存储LFU和LRU的信息。 将集群管理功能完全用C语言集成到redis-cli中，Redis 3.x 和 Redis4.x 的集群管理是通过Ruby脚本实现的。 有序集合新增命令ZPOPMIN/ZPOPMAX。 改进HyperLogLog的实现。 新增Client Unblock和Client ID。 新增LOLWUT命令。 Redis主从复制中的从不再称为Slave，改称Replicas。 Redis 5.0引入动态哈希，以平衡CPU的使用率和相应性能，可以通过配置文件进行配置。Redis 5.0默认使用动态哈希。 Redis核心代码进行了部分重构和优化。  简单动态字符串 （1） 长度小于 32 的短字符串\nstruct __attribute__ ((__packed__))sdshdr5 { unsigned char flags; // 低 3 位存储类型，高 5 位存储长度  char buf[]; // 柔性数组 } 结构如下：\n（2） 长度大于 31 的字符串\n此处仅展示一个示例：\nstruct __attribute__ ((__packed__))sdshdr8 { uint8_t len; // 已使用长度  uint8_t alloc; // 已分配的字节总长度  unsigned char flags; // 低 3 位存储类型  char buf[]; // 柔性数组 } SDS 读操作的复杂度多为O(1)，直接读取成员变量；涉及修改的写操作，则可能会触发扩容。\n跳跃表 对于有序集合的底层实现，我们可以使用数组、链表、平衡树等结构。数组不便于元素的插入和删除；链表的查询效率低，需要遍历所有元素；平衡树或者红黑树等结构虽然效率高但实现复杂。Redis采用了一种新型的数据结构——跳跃表。跳跃表的效率堪比红黑树，然而其实现却远比红黑树简单。\ntypedef struct zskiplistNode { sds ele; double score; struct zskiplistNode *backward; struct zskiplistLevel { struct zskiplistNode *forward; unsigned int span; } level[]; } zskiplistNode; typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length; int level; } zkiplist; 在Redis中，跳跃表主要应用于有序集合的底层实现（有序集合的另一种实现方式为压缩列表）。zset插入第一个元素时，会判断下面两种条件：\n zset-max-ziplist-entries的值是否等于0； zset-max-ziplist-value小于要插入元素的字符串长度。  满足任一条件Redis就会采用跳跃表作为底层实现，否则采用压缩列表作为底层实现方式。一般情况下，不会将zset-max-ziplist-entries配置成0，元素的字符串长度也不会太长，所以在创建有序集合时，默认使用压缩列表的底层实现。\nzset新插入元素时，会判断以下两种条件：\n zset中元素个数大于zset_max_ziplist_entries； 插入元素的字符串长度大于zset_max_ziplist_value。  当满足任一条件时，Redis便会将zset的底层实现由压缩列表转为跳跃表。值得注意的是，zset在转为跳跃表之后，即使元素被逐渐删除，也不会重新转为压缩列表。\n跳跃表的原理简单，其查询、插入、删除的平均复杂度都为O(logN)。\n压缩列表 压缩列表ziplist本质上就是一个字节数组，是Redis为了节约内存而设计的一种线性数据结构，可以包含多个元素，每个元素可以是一个字节数组或一个整数。\nRedis的有序集合、散列和列表都直接或者间接使用了压缩列表。当有序集合或散列表的元素个数比较少，且元素都是短字符串时，Redis便使用压缩列表作为其底层数据存储结构。\n元素的结构示意图：\n字典 Redis自带客户端就是使用times 33散列函数来计算字符串的Hash值，Redis服务端的Hash函数使用的是siphash算法，主要功能与客户端Hash函数类似，其优点是针对有规律的键计算出来的Hash值也具有强随机分布性，但算法较为复杂。\n整数集合 整数集合（intset）是一个有序的、存储整型数据的结构。\n127.0.0.1:6379\u0026gt; sadd testset 1 2 1 6 (integer) 4 127.0.0.1:6379\u0026gt; object encoding testset \u0026#34;intset\u0026#34; intset是按从小到大有序排列的，所以通过防御性判断之后使用二分法进行元素的查找。\nquicklist的实现 quicklist是Redis底层最重要的数据结构之一，它是Redis对外提供的6种基本数据结构中List的底层实现，在Redis 3.2版本中引入，能够在时间效率和空间效率间实现较好的折中。quicklist由List和ziplist结合而成。quicklist是一个双向链表，链表中的每个节点是一个ziplist结构。quicklist可以看成是用双向链表将若干小型的ziplist连接到一起组成的一种数据结构。\nStream Redis Stream的结构如图所示，它主要由消息、生产者、消费者、消费组4部分组成。\nxadd mystream1 * name zk age 20 mystream1为Stream的名称；*代表由Redis自行生成消息ID;name、age为该消息的field; zk、20则为对应的field的值。\n每个消息都由以下两部分组成。\n 每个消息有唯一的消息ID，消息ID严格递增。 消息内容由多个field-value对组成。  "});index.add({'id':86,'href':'/docs/rocketmq/rocketmq-message-indexing-flow/','title':"RocketMQ 消息索引流程",'content':"RocketMQ 消息索引流程 讲述 RocketMQ 消息索引服务\n一、消息查询方式 对于 Producer 发送到 Broker 服务器的消息，RocketMQ 支持多种方式来方便地查询消息:\n(1) 根据键查询消息 如下所示，在构建消息的时候，指定了这条消息的键为 “OrderID001”:\nMessage msg = new Message(\u0026#34;TopicTest\u0026#34;, \u0026#34;TagA\u0026#34;, \u0026#34;OrderID001\u0026#34;, // Keys  \u0026#34;Hello world\u0026#34;.getBytes(RemotingHelper.DEFAULT_CHARSET)); 那么，当这条消息发送成功后，我们可以使用 queryMsgByKey 命令查询到这条消息的详细信息:\nMQAdminStartup.main(new String[] { \u0026#34;queryMsgByKey\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;localhost:9876\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;TopicTest\u0026#34;, \u0026#34;-k\u0026#34;, \u0026#34;OrderID001\u0026#34; }); (2) 根据ID(偏移量)查询消息 消息在发送成功之后，其返回的 SendResult 类中包含了这条消息的唯一偏移量 ID (注意此处指的是 offsetMsgId):\n用户可以使用 queryMsgById 命令查询这条消息的详细信息:\nMQAdminStartup.main(new String[] { \u0026#34;queryMsgById\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;localhost:9876\u0026#34;, \u0026#34;-i\u0026#34;, \u0026#34;0A6C73D900002A9F0000000000004010\u0026#34; }); (3) 根据唯一键查询消息 消息在发送成功之后，其返回的 SendResult 类中包含了这条消息的唯一 ID:\n用户可以使用 queryMsgByUniqueKey 命令查询这条消息的详细信息:\nMQAdminStartup.main(new String[] { \u0026#34;queryMsgByUniqueKey\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;localhost:9876\u0026#34;, \u0026#34;-i\u0026#34;, \u0026#34;0A6C73D939B318B4AAC20CBA5D920000\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;TopicTest\u0026#34; }); (4) 根据消息队列偏移量查询消息 消息发送成功之后的 SendResult 中还包含了消息队列的其它信息，如消息队列 ID、消息队列偏移量等信息:\nSendResult [sendStatus=SEND_OK, msgId=0A6C73D93EC518B4AAC20CC4ACD90000, offsetMsgId=0A6C73D900002A9F000000000000484E, messageQueue=MessageQueue [topic=TopicTest, brokerName=zk-pc, queueId=3], queueOffset=24] 根据这些信息，使用 queryMsgByOffset 命令也可以查询到这条消息的详细信息:\nMQAdminStartup.main(new String[] { \u0026#34;queryMsgByOffset\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;localhost:9876\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;TopicTest\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;zk-pc\u0026#34;, \u0026#34;-i\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;24\u0026#34; }); 二、ID (偏移量) 查询 (1) 生成 ID ID (偏移量) 是在消息发送到 Broker 服务器存储的时候生成的，其包含如下几个字段：\n Broker 服务器 IP 地址 Broker 服务器端口号 消息文件 CommitLog 写偏移量  public class CommitLog { class DefaultAppendMessageCallback implements AppendMessageCallback { public AppendMessageResult doAppend(final long fileFromOffset, /** 其它参数 **/) { String msgId = MessageDecoder .createMessageId(this.msgIdMemory, msgInner.getStoreHostBytes(hostHolder), wroteOffset); // ...  } } } (2) 使用 ID 查询 Admin 端查询的时候，首先对 msgId 进行解析，取出 Broker 服务器的 IP 、端口号和消息偏移量:\npublic class MessageDecoder { public static MessageId decodeMessageId(final String msgId) throws UnknownHostException { byte[] ip = UtilAll.string2bytes(msgId.substring(0, 8)); byte[] port = UtilAll.string2bytes(msgId.substring(8, 16)); // offset  byte[] data = UtilAll.string2bytes(msgId.substring(16, 32)); // ...  } } 获取到偏移量之后，Admin 会对 Broker 服务器发送一个 VIEW_MESSAGE_BY_ID 的请求命令，Broker 服务器在收到请求后，会依据偏移量定位到 CommitLog 文件中的相应位置,然后取出消息，返回给 Admin 端:\npublic class DefaultMessageStore implements MessageStore { @Override public SelectMappedBufferResult selectOneMessageByOffset(long commitLogOffset) { SelectMappedBufferResult sbr = this.commitLog .getMessage(commitLogOffset, 4); // 1 TOTALSIZE  int size = sbr.getByteBuffer().getInt(); return this.commitLog.getMessage(commitLogOffset, size); } } 三、消息队列偏移量查询 根据队列偏移量查询是最简单的一种查询方式，Admin 会启动一个 PullConsumer ，然后利用用户传递给 Admin 的队列 ID、队列偏移量等信息，从服务器拉取一条消息过来:\npublic class QueryMsgByOffsetSubCommand implements SubCommand { @Override public void execute(CommandLine commandLine, Options options, RPCHook rpcHook) throws SubCommandException { // 根据参数构建 MessageQueue  MessageQueue mq = new MessageQueue(); mq.setTopic(topic); mq.setBrokerName(brokerName); mq.setQueueId(Integer.parseInt(queueId)); // 从 Broker 服务器拉取消息  PullResult pullResult = defaultMQPullConsumer.pull(mq, \u0026#34;*\u0026#34;, Long.parseLong(offset), 1); } } 四、消息索引服务 在继续讲解剩下两种查询方式之前，我们必须先介绍以下 Broker 端的消息索引服务。\n在之前提到过，每当一条消息发送过来之后，其会封装为一个 DispatchRequest 来下发给各个转发服务，而 CommitLogDispatcherBuildIndex 构建索引服务便是其中之一:\nclass CommitLogDispatcherBuildIndex implements CommitLogDispatcher { @Override public void dispatch(DispatchRequest request) { if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) { DefaultMessageStore.this.indexService.buildIndex(request); } } } (1) 索引文件结构 消息的索引信息是存放在磁盘上的，文件以时间戳命名的，默认存放在 $HOME/store/index 目录下。由下图来看，一个索引文件的结构被分成了三部分:\n 前 40 个字节存放固定的索引头信息，包含了存放在这个索引文件中的消息的最小/大存储时间、最小/大偏移量等状况 中间一段存储了 500 万个哈希槽位，每个槽内部存储的是索引文件的地址 (索引槽) 最后一段存储了 2000 万个索引内容信息，是实际的索引信息存储的地方。每一个槽位存储了这条消息的键哈希值、存储偏移量、存储时间戳与下一个索引槽地址  RocketMQ 在内存中还维护了一个索引文件列表，对于每一个索引文件，前一个文件的最大存储时间是下一个文件的最小存储时间，前一个文件的最大偏移量是下一个文件的最大偏移量。每一个索引文件都索引了在某个时间段内、某个偏移量段内的所有消息，当文件满了，就会用前一个文件的最大偏移量和最大存储时间作为起始值，创建下一个索引文件:\n(2) 添加消息 当有新的消息过来后，构建索引服务会取出这条消息的键，然后对字符串 “话题#键” 构建索引。构建索引的步骤如下:\n 找出哈希槽: 生成字符串哈希码，取余落到 500W 个槽位之一，并取出其中的值，默认为 0 找出索引槽: IndexHeader 维护了 indexCount，实际存储的索引槽就是直接依次顺延添加的 存储索引内容: 找到索引槽后，放入键哈希值、存储偏移量、存储时间戳与下一个索引槽地址。下一个索引槽地址就是第一步哈希槽中取出的值，0 代表这个槽位是第一次被索引，而不为 0 代表这个槽位之前的索引槽地址。由此，通过索引槽地址可以将相同哈希槽的消息串联起来，像单链表那样。 更新哈希槽: 更新原有哈希槽中存储的值  我们以实际例子来说明。假设我们需要依次为键的哈希值为 “{16,29,29,8,16,16}” 这几条消息构建索引，我们在这个地方忽略了索引信息中存储的存储时间和偏移量字段，只是存储键哈希和下一索引槽信息，那么:\n 放入 16: 将 “16|0” 存储在第 1 个索引槽中，并更新哈希槽为 16 的值为 1，即哈希槽为 16 的第一个索引块的地址为 1 放入 29: 将 “29|0” 存储在第 2 个索引槽中，并更新哈希槽为 29 的值为 2，即哈希槽为 29 的第一个索引块的地址为 2 放入 29: 取出哈希槽为 29 中的值 2，然后将 “29|2” 存储在第 3 个索引槽中，并更新哈希槽为 29 的值为 3，即哈希槽为 29 的第一个索引块的地址为 3。而在找到索引块为 3 的索引信息后，又能取出上一个索引块的地址 2，构成链表为： “[29]-\u0026gt;3-\u0026gt;2” 放入 8: 将 “8|0” 存储在第 4 个索引槽中，并更新哈希槽为 8 的值为 4，即哈希槽为 8 的第一个索引块的地址为 4 放入 16: 取出哈希槽为 16 中的值 1，然后将 “16|1” 存储在第 5 个索引槽中，并更新哈希槽为 16 的值为 5。构成链表为: “[16]-\u0026gt;5-\u0026gt;1” 放入 16: 取出哈希槽为 16 中的值 5，然后将 “16|5” 存储在第 6 个索引槽中，并更新哈希槽为 16 的值为 6。构成链表为: “[16]-\u0026gt;6-\u0026gt;5-\u0026gt;1”  整个过程如下图所示:\n(3) 查询消息 当需要根据键来查询消息的时候，其会按照倒序回溯整个索引文件列表，对于每一个在时间上能够匹配用户传入的 begin 和 end 时间戳参数的索引文件，会一一进行消息查询：\npublic class IndexService { public QueryOffsetResult queryOffset(String topic, String key, int maxNum, long begin, long end) { // 倒序  for (int i = this.indexFileList.size(); i \u0026gt; 0; i--) { // 位于时间段内  if (f.isTimeMatched(begin, end)) { // 消息查询  } } } } 而具体到每一个索引文件，其查询匹配消息的过程如下所示:\n 确定哈希槽: 根据键生成哈希值，定位到哈希槽 定位索引槽: 哈希槽中的值存储的就是链表的第一个索引槽地址 遍历索引槽: 沿着索引槽地址，依次取出下一个索引槽地址，即沿着链表遍历，直至遇见下一个索引槽地址为非法地址 0 停止 收集偏移量: 在遇到匹配的消息之后，会将相应的物理偏移量放到列表中，最后根据物理偏移量，从 CommitLog 文件中取出消息  public class DefaultMessageStore implements MessageStore { @Override public QueryMessageResult queryMessage(String topic, String key, int maxNum, long begin, long end) { for (int m = 0; m \u0026lt; queryOffsetResult.getPhyOffsets().size(); m++) { long offset = queryOffsetResult.getPhyOffsets().get(m); // 根据偏移量从 CommitLog 文件中取出消息  } } } 以查询哈希值 16 的消息为例，图示如下:\n五、唯一键查询消息 (1) 构建键 消息的唯一键是在客户端发送消息前构建的:\npublic class DefaultMQProducerImpl implements MQProducerInner { private SendResult sendKernelImpl(final Message msg, /** 其它参数 **/) throws XXXException { // ...  if (!(msg instanceof MessageBatch)) { MessageClientIDSetter.setUniqID(msg); } } } 创建唯一 ID 的算法:\npublic class MessageClientIDSetter { public static String createUniqID() { StringBuilder sb = new StringBuilder(LEN * 2); sb.append(FIX_STRING); sb.append(UtilAll.bytes2string(createUniqIDBuffer())); return sb.toString(); } } 唯一键是根据客户端的进程 ID、IP 地址、ClassLoader 哈希码、时间戳、计数器这几个值来生成的一个唯一的键，然后作为这条消息的附属属性发送到 Broker 服务器的:\npublic class MessageClientIDSetter { public static void setUniqID(final Message msg) { if (msg.getProperty(MessageConst.PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX) == null) { msg.putProperty(MessageConst.PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX, createUniqID()); } } } (2) 索引键 当服务器收到客户端发送过来的消息之后，索引服务便会取出客户端生成的 uniqKey 并为之建立索引，放入到索引文件中:\npublic class IndexService { public void buildIndex(DispatchRequest req) { // ...  if (req.getUniqKey() != null) { indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey())); } // ...  } } (3) 使用键查询 客户端在生成消息唯一键的时候，在 ByteBuffer 的第 11 位到第 14 位放置的是当前的时间与当月第一天的时间的毫秒差:\npublic class MessageClientIDSetter { private static byte[] createUniqIDBuffer() { long current = System.currentTimeMillis(); if (current \u0026gt;= nextStartTime) { setStartTime(current); } // 时间差 [当前时间 - 这个月 1 号的时间]  // putInt 占据的是第 11 位到第 14 位  buffer.putInt((int) (System.currentTimeMillis() - startTime)); } private synchronized static void setStartTime(long millis) { Calendar cal = Calendar.getInstance(); cal.setTimeInMillis(millis); cal.set(Calendar.DAY_OF_MONTH, 1); cal.set(Calendar.HOUR_OF_DAY, 0); cal.set(Calendar.MINUTE, 0); cal.set(Calendar.SECOND, 0); cal.set(Calendar.MILLISECOND, 0); // 开始时间设置为这个月的 1 号  startTime = cal.getTimeInMillis(); // ...  } } 我们知道消息索引服务的查询需要用户传入 begin 和 end 这连个时间值，以进行这段时间内的匹配。所以 RocketMQ 为了加速消息的查询，于是在 Admin 端对特定 ID 进行查询的时候，首先取出了这段时间差值，然后与当月时间进行相加得到 begin 时间值:\npublic class MessageClientIDSetter { public static Date getNearlyTimeFromID(String msgID) { ByteBuffer buf = ByteBuffer.allocate(8); byte[] bytes = UtilAll.string2bytes(msgID); buf.put((byte) 0); buf.put((byte) 0); buf.put((byte) 0); buf.put((byte) 0); // 取出第 11 位到 14 位  buf.put(bytes, 10, 4); buf.position(0); // 得到时间差值  long spanMS = buf.getLong(); Calendar cal = Calendar.getInstance(); long now = cal.getTimeInMillis(); cal.set(Calendar.DAY_OF_MONTH, 1); cal.set(Calendar.HOUR_OF_DAY, 0); cal.set(Calendar.MINUTE, 0); cal.set(Calendar.SECOND, 0); cal.set(Calendar.MILLISECOND, 0); long monStartTime = cal.getTimeInMillis(); if (monStartTime + spanMS \u0026gt;= now) { cal.add(Calendar.MONTH, -1); monStartTime = cal.getTimeInMillis(); } // 设置为这个月(或者上个月) + 时间差值  cal.setTimeInMillis(monStartTime + spanMS); return cal.getTime(); } } 由于发送消息的客户端和查询消息的 Admin 端可能不在一台服务器上，而且从函数的命名 getNearlyTimeFromID 与上述实现来看，Admin 端的时间戳得到的是一个近似起始值，它尽可能地加速用户的查询。而且太旧的消息(超过一个月的消息)是查询不到的。\n当 begin 时间戳确定以后，Admin 便会将其它必要的信息如话题、Key等信息封装到 QUERY_MESSAGE 的包中，然后向 Broker 服务器传递这个请求，来进行消息的查询。Broker 服务器在获取到这个查询消息的请求后，便会根据 Key 从索引文件中查询符合的消息，最终返回到 Admin 端。\n六、键查询消息 (1) 构建键 我们提到过，在发送消息的时候，可以填充一个 keys 的值，这个值将会作为消息的一个属性被发送到 Broker 服务器上:\npublic class Message implements Serializable { public void setKeys(String keys) { this.putProperty(MessageConst.PROPERTY_KEYS, keys); } } (2) 索引键 当服务器收到客户端发送过来的消息之后，索引服务便会取出这条消息的 keys 并将其用空格进行分割，分割后的每一个字符串都会作为一个单独的键，创建索引，放入到索引文件中:\npublic class IndexService { public void buildIndex(DispatchRequest req) { // ...  if (keys != null \u0026amp;\u0026amp; keys.length() \u0026gt; 0) { // 使用空格进行分割  String[] keyset = keys.split(MessageConst.KEY_SEPARATOR); for (int i = 0; i \u0026lt; keyset.length; i++) { String key = keyset[i]; if (key.length() \u0026gt; 0) { indexFile = putKey(indexFile, msg, buildKey(topic, key)); } } } } } 由此我们也可以得知，keys 键的设置通过使用空格分割字符串，一条消息可以指定多个键。\n(3) 使用键查询 keys 键查询的方式也是通过将参数封装为 QUERY_MESSAGE 请求包中去请求服务器返回相应的信息。由于键本身不能和时间戳相关联，因此 begin 值设置的是 0，这是和第五节的不同之处:\npublic class QueryMsgByKeySubCommand implements SubCommand { private void queryByKey(final DefaultMQAdminExt admin, final String topic, final String key) throws MQClientException, InterruptedException { // begin: 0  // end: Long.MAX_VALUE  QueryResult queryResult = admin.queryMessage(topic, key, 64, 0, Long.MAX_VALUE); } } 扫描下面二维码，在手机端阅读：\n"});index.add({'id':87,'href':'/docs/tutorial/unix-command/top/','title':"top",'content':"top 本文介绍 top 命令的常见例子！top 可以显示系统运行的进程和资源等情况的有用信息。\n基础展示 top 上述命令将会显示：\n 红色区域：系统的统计信息 蓝色区域：系统所有的进程列表信息  默认情况下，top 命令每隔 3 秒刷新一次。\n 红色区域\n 第一行展示的是：时间、电脑运行多久了、多少人登录着电脑、过去 1、5、15 分钟电脑的平均负载。 第二行展示的是，任务的总数量，以及各个状态的任务数量。 第三行展示的是 CPU 的一些信息。  CPU 信息的每一列的含义：\n us：用户态 CPU 占用处理器的总时间 sy：内核态 CPU 占用处理器的总时间 ni：使用手动设置的 nice 值执行进程所花费的时间。 id：CPU空闲时间的数量。 wa：CPU等待I/O完成所花费的时间。 hi：维护硬件中断所花费的时间。 si：维护软件中断所花费的时间。 st：由于运行虚拟机而损失的时间（“窃取时间”）。  第四行显示物理内存的总量（以kibibytes为单位），以及空闲、使用、缓冲或缓存的内存量。 第五行显示交换内存的总量（也以kibibytes为单位），以及空闲、使用和可用的内存量。后者包括预期可以从缓存中恢复的内存。\n 蓝色区域的，进程列表中的各个列的信息如下：\n PID：进程ID。 USER：进程的所有者。 PR：流程优先级。 NI：这个过程很有价值。 VIRT：进程使用的虚拟内存量。 RES：进程使用的常驻内存量。 SHR：进程使用的共享内存量。 S： 进程的状态。（有关此字段可以采用的值，请参见下面的列表）。 %CPU：自上次更新以来进程使用的CPU时间的份额。 %MEM：使用的物理内存份额。 TIME+：任务使用的总CPU时间（以百分之一秒为单位）。 COMMAND：命令名或命令行（名称+选项）。  内存值以kibibytes为单位显示。进程的状态可以是以下之一：\n D： 不间断睡眠 R： 跑步 S： 睡觉 T： 跟踪（停止） Z： 僵尸  按 Q 退出 top 命令。\n滚动区域 您可以按向上或向下箭头、Home、End和Page Up或Down键上下移动并访问所有进程。\n更改数字单位 让我们把显示单位改为合理的值。\n按大写 E 循环显示这些选项中用于显示内存值的单位：kibibytes、mebibytes、gibibytes、tebibytes、pebibytes和exbibytes。\n按小写 e 对进程列表中的值执行相同的操作：kibibytes、mebibytes、gibibytes、tebibytes和pebibytes。\n我们按 E 将红色区域的内存单位设置为gibibytes，按 e 将进程列表内存单位设置为mebibytes。\n更改统计区域的显示 如果您有多核CPU，请按 1 更改显示并查看每个CPU的个别统计信息。我们的电脑上有四个CPU。我们按1，看看他们每个 CPU 的使用情况。\n按 t 可以修改 CPU 的展示方式：\n按 m 可以修改内存的展示方式：\n高亮显示 您可以按 “z” 键为 top 添加颜色。\n根据列排序 默认情况下，进程列表按 %CPU 列排序。您可以按以下键更改排序列：\n P： %CPU 列 M： %MEM 列 N： PID 列 T： TIME+ 列  在下图中，进程列表按PID列排序。\n显示完整命令行 按 “c” 可以调整 COMMAND 列，在显示进程名和完整命令行之间进行切换。\n参考  How to Use the Linux top Command (and Understand Its Output)  扫描下面二维码，在手机端阅读：\n"});index.add({'id':88,'href':'/docs/programmer-interview/algorithm/best-time-to-buy-and-sell-stock-with-transaction-fee/','title':"Best Time to Buy and Sell Stock with Transaction Fee",'content':"Best Time to Buy and Sell Stock with Transaction Fee  每次交易都需要交易费用\n // 可以交易任意多次 // 只不过每一次都有小费 // // https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/discuss/160964/java-Using-State-Machine-like-stock-III public class BestTimetoBuyandSellStockwithTransactionFee { public int maxProfit(int[] prices, int fee) { if (prices == null || prices.length \u0026lt;= 1) { return 0; } int s0 = 0; int s1 = s0 - prices[0]; // 买入  for (int i = 1; i \u0026lt; prices.length; i++) { // 这两种状态都能转移到 s0 状态:  //  // s0 -\u0026gt; s0  // s1 卖出 -\u0026gt; s0  s0 = Math.max(s0, s1 + prices[i] - fee); // s1 卖出又能重新买入: s1 -\u0026gt; s0  // 怎样能转移到 s1  s1 = Math.max(s1, s0 - prices[i]); // s0 转移到 s1  } return s0; } } "});index.add({'id':89,'href':'/docs/tutorial/unix-command/cat/','title':"cat",'content':"cat cat 命令的常见用法！\n查看文件内容 要使用 cat 显示文件的内容，只需传递要查看的一个或多个文件的名称。文件内容将打印到标准输出并在终端中可见。下面的示例假设文件foo.txt 文件只有一行“Hello World”。\ncat foo.txt Hello world 如果文件的内容很长，则全部内容将写入终端。在这种情况下，很难找到文件的某些部分。在寻找特定内容时，grep 可能是一个更好的选择。\n将一个文件的内容写入到另外一个文件 使用cat工具结合重定向，可以将文件内容写入新的文件。下面的示例假设文件foo.txt文件只有一行“Hello World”并将其写入bar.txt文件.\ncat foo.txt \u0026gt; bar.txt cat bar.txt Hello world 如果 bar.txt 文件不存在，那么 cat 工具会自动创建 bar.txt 文件。\n将一个文件的内容追加到另外一个文件 cat wine.txt \u0026gt;\u0026gt; beer.txt 多个文件合并为一个 cat *.txt \u0026gt; combined.txt 上述命令行，将当前目录以 .txt 结尾的文件，合并到 combined.txt 文件中。\ncat 输出显示行号 -n 参数可以显示文件的行号：\ncat -n /usr/share/dict/words 1 A 2 a 3 aa 4 aal 5 aalii 输出的每行行尾显示 $ 符号 cat -e test hello everyone, how do you do?$ $ Hey, am fine.$ How\u0026#39;s your training going on?$ $ 多个空行压缩为一个空行 cat -s blanks.txt -s 选项可以将多个相邻的空行压缩为一个空行，并不是消除所有空行，而是仅仅保留一行空行。比如：\nLine one Line two Line three 压缩为：\nLine one Line two Line three 参考  Linux and Unix cat command tutorial with examples  扫描下面二维码，在手机端阅读：\n"});index.add({'id':90,'href':'/docs/tutorial/network/cdn/','title':"CDN",'content':"CDN  核心理念：就近访问数据\n CDN 分发系统架构 回溯 由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。\nCDN 可缓存的内容  静态资源：图片、CSS、JS、HTML 文件、流媒体等 动态数据：缓存动态数据  如何找到合适的边缘节点  基于 DNS 的全局负载均衡。\n 根据用户 IP、所处的运营商、请求 URL 中携带的参数、服务器的负载情况等综合分析后，全局负载均衡服务器返回一台缓存服务器的 IP 地址。\n动态资源如何缓存  边缘计算：动态数据生成的计算、逻辑、存储等，也放到边缘节点，然后定时地从数据源同步存储的数据等内容 路径优化：源站到边缘节点的路径经过优化 (调整 TCP 参数、多路复用、数据压缩)，采用更为可靠的路径来传输  CDN 挂掉怎么办 为了防止 CDN 挂掉，引入的时候要加入一个判断：\n\u0026lt;script src=\u0026#34;http://cdn.static.runoob.com/libs/jquery/1.10.2/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;window.jQuery || document.write(\u0026#39;\u0026lt;script src=\u0026#34;js/vendor/jquery-1.10.2.min.js\u0026#34;\u0026gt;\u0026lt;\\/script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt; 这段代码第一行很简单就是你正常引入 cdn 的地址。下面一行的话就是首先判断 Windows.jQuery 是否存在；也就是说判断一下这个CDN是不是挂掉了，如果没有挂掉，那么就直接使用，如果挂掉了，那么就要在后面引入自己的本地资源库。这样就可以保证在可以使用 cdn 的时候使用 cdn 不可以使用的时候就加载本地的。\n"});index.add({'id':91,'href':'/docs/tutorial/devops/','title':"DevOps",'content':"DevOps 参考  《DevOps 实践指南》  "});index.add({'id':92,'href':'/docs/tutorial/git/git-stash/','title':"Git 保存当前进度",'content':"Git 保存当前进度 git stash 命令可以帮助我们保存和恢复日常的工作进度。\n应用场景 你正在 dev 分支上开发项目的某个新功能，开发到一半的时候，master 分支的代码（线上正在运行的代码）出现了一个 bug，需要紧急修复。你现在需要从 dev 分支切换到 master 分支修 BUG，而你现在在 dev 分支正在开发的代码也不可能开发到一半就要 push 上去，此时就可以先在 dev 分支把代码给 stash 起来，也就是暂存起来，然后再切换到 master 分支。等 master 分支修复好了后，再切回 dev 分支，执行 stash pop 把这部分代码给恢复出来即可。\n下面示例几个基础用法：\n保存当前工作进度 git stash 显示进度列表 git stash list stash 就是一个栈数据结构管理的，你可以保存多次工作进度，并且恢复的时候也可以选择恢复哪个。\n恢复进度 # 恢复最新保存的工作进度，并将工作进度从 stash 列表中清除 git stash pop # 恢复某个指定的 stash (git stash list 可以看到) git stash pop [\u0026lt;stash\u0026gt;] 命令 git stash apply [\u0026lt;stash\u0026gt;] 同 git stash pop，只是不会从 stash 列表中删除恢复的进度。\n删除一个存储的进度 git stash drop [\u0026lt;stash\u0026gt;] 删除所有存储的进度 git stash clear 扫描下面二维码，在手机端阅读：\n"});index.add({'id':93,'href':'/docs/rocketmq/rocketmq-timing-message-and-retry-message/','title':"RocketMQ 定时消息和重试消息",'content':"RocketMQ 定时消息和重试消息 讲述 RocketMQ 定时消息和重试消息\n一、定时消息概述 RocketMQ 支持 Producer 端发送定时消息，即该消息被发送之后，到一段时间之后才能被 Consumer 消费者端消费。但是当前开源版本的 RocketMQ 所支持的定时时间是有限的、不同级别的精度的时间，并不是任意无限制的定时时间。因此在每条消息上设置定时时间的 API 叫做 setDelayTimeLevel，而非 setDelayTime 这样的命名:\nMessage msg = new Message(\u0026#34;TopicTest\u0026#34; /* Topic */, \u0026#34;TagA\u0026#34; /* Tag */, (\u0026#34;Hello RocketMQ \u0026#34; + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */); msg.setDelayTimeLevel(i + 1); 默认 Broker 服务器端有 18 个定时级别:\npublic class MessageStoreConfig { private String messageDelayLevel = \u0026#34;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h\u0026#34;; } 这 18 个定时级别在服务器端启动的时候，会被解析并放置到表 delayLevelTable 中。解析的过程就是上述字符串按照空格拆分开，然后根据时间单位的不同再进一步进行计算，得到最终的毫秒时间。级别就是根据这些毫秒时间的顺序而确定的，例如上述 1s 延迟就是级别 1， 5s 延迟就是级别 2，以此类推:\npublic class ScheduleMessageService extends ConfigManager { public boolean parseDelayLevel() { for (int i = 0; i \u0026lt; levelArray.length; i++) { // ...  int level = i + 1; long delayTimeMillis = tu * num; // 级别:延迟时间  this.delayLevelTable.put(level, delayTimeMillis); } } } 二、定时消息预存储 客户端在为某条消息设置上定时级别的时候，实际上级别这个字段会被作为附属属性放到消息中:\npublic class Message implements Serializable { public void setDelayTimeLevel(int level) { this.putProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL, String.valueOf(level)); } } 我们先前的文章提到过，发送到 Broker 服务器的消息会被存储到 CommitLog 消息文件中。那么在此处即使是定时消息也不例外，将定时消息存储下来是为了保证消息最大程度地不丢失。然而毕竟和普通消息不同，在遇到定时消息后，CommitLog 会将这条消息的话题和队列 ID 替换成专门用于定时的话题和相应的级别对应的队列 ID。真实的话题和队列 ID 会作为属性放置到这条消息中。\npublic class CommitLog { public PutMessageResult putMessage(final MessageExtBrokerInner msg) { // Delay Delivery  if (msg.getDelayTimeLevel() \u0026gt; 0) { topic = ScheduleMessageService.SCHEDULE_TOPIC; queueId = ScheduleMessageService.delayLevel2QueueId(msg.getDelayTimeLevel()); // Backup real topic, queueId  MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic()); MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId())); msg.setPropertiesString(MessageDecoder.messageProperties2String(msg.getProperties())); // 替换 Topic 和 QueueID  msg.setTopic(topic); msg.setQueueId(queueId); } } } 随后，这条消息会被存储在 CommitLog 消息文件中。而我们知道后台重放消息服务 ReputMessageService 会一直监督 CommitLog 文件是否添加了新的消息。当有了新的消息后，重放消息服务会取出消息并封装为 DispatchRequest 请求，然后将其分发给不同的三个分发服务，建立消费队列文件服务就是这其中之一。而此处当取消息封装为 DispatchRequest 的时候，当遇到定时消息时，又多做了一些额外的事情。\n当遇见定时消息时，CommitLog 计算 tagsCode 标签码与普通消息不同。对于定时消息，tagsCode 值设置的是这条消息的投递时间，即建立消费队列文件的时候，文件中的 tagsCode 存储的是这条消息未来在什么时候被投递:\npublic class CommitLog { public DispatchRequest checkMessageAndReturnSize(java.nio.ByteBuffer byteBuffer, final boolean checkCRC, final boolean readBody) { // Timing message processing  { String t = propertiesMap.get(MessageConst.PROPERTY_DELAY_TIME_LEVEL); if (ScheduleMessageService.SCHEDULE_TOPIC.equals(topic) \u0026amp;\u0026amp; t != null) { int delayLevel = Integer.parseInt(t); if (delayLevel \u0026gt; 0) { tagsCode = this.defaultMessageStore.getScheduleMessageService() .computeDeliverTimestamp(delayLevel,storeTimestamp); } } } } } 如下是，发送了 10 条定时级别分别为 1-10 的消息以后，$HOME/store/consumequeue 文件下的消费队列文件的分布情况:\n不同的定时级别对应于不同的队列 ID，定时级别减 1 得到的就是队列 ID 的值。因此级别 1-10 对应的是 0-9 的队列 ID:\npublic class ScheduleMessageService extends ConfigManager { public static int delayLevel2QueueId(final int delayLevel) { return delayLevel - 1; } } 三、定时消息再存储 Broker 启动的时候，会开启一个调度消息服务，此服务会监控所有定时消息队列，每一个消息队列会创建一个专门的延时消息投递任务用以到达规定时间后投递此消息:\npublic class ScheduleMessageService extends ConfigManager { public void start() { for (Map.Entry\u0026lt;Integer, Long\u0026gt; entry : this.delayLevelTable.entrySet()) { Integer level = entry.getKey(); Long timeDelay = entry.getValue(); Long offset = this.offsetTable.get(level); if (timeDelay != null) { this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME); } } } } 每个消息队里的消息投递任务，会检查自己跟踪的消息队列，并从此消息队列所对应的定时级别的偏移量中检查是否有新的定时消息到来。其中定时级别的偏移量是维护在内存中的偏移量表 offsetTable 中。每隔 10 秒钟，这个表会被持久化到磁盘上的 delayOffset.json 文件中一次:\npublic class ScheduleMessageService extends ConfigManager { private final ConcurrentMap\u0026lt;Integer /* level */, Long/* offset */\u0026gt; offsetTable = new ConcurrentHashMap\u0026lt;Integer, Long\u0026gt;(32); public void start() { // 每隔 10 秒钟持久化一次  this.timer.scheduleAtFixedRate(new TimerTask() { @Override public void run() { ScheduleMessageService.this.persist(); } }, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval()); } } delayOffset.json 文件中存储的示例信息如下所示：\nDeliverDelayedMessageTimerTask 任务会从消费任务队列文件中取出最新的定时消息的 tagsCode ，并计算出的当前是否已经到了这条消息投递的时间。如果到了，即 countdown \u0026lt; 0，那么便会从 CommitLog 文件中取出消息，修正消息的话题和队列 ID 等信息，然后重新存储此条消息。如果还没有到，那么便会重新执行一个定时时间设置为 countdown 毫秒的定时任务。在完成之后，会更新当前的偏移量表，为下一次做准备:\nclass DeliverDelayedMessageTimerTask extends TimerTask { public void executeOnTimeup() { // ...  for (; i \u0026lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) { // 是否到时间  long countdown = deliverTimestamp - now; if (countdown \u0026lt;= 0) { // 取出消息  MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset(offsetPy, sizePy); // 修正消息，设置上正确的话题和队列 ID  MessageExtBrokerInner msgInner = this.messageTimeup(msgExt); // 重新存储消息  PutMessageResult putMessageResult = ScheduleMessageService.this.defaultMessageStore .putMessage(msgInner); } else { // countdown 后投递此消息  ScheduleMessageService.this .timer .schedule(new DeliverDelayedMessageTimerTask(this.delayLevel, nextOffset), countdown); // 更新偏移量  } } // end of for  // 更新偏移量  } } 四、消息重试概述 消息重试分为消息发送重试和消息接受重试，消息发送重试是指消息从 Producer 端发送到 Broker 服务器的失败以后的重试情况，消息接受重试是指 Consumer 在消费消息的时候出现异常或者失败的重试情况。\nProducer 端通过配置如下这两个两个 API 可以分别配置在同步发送和异步发送消息失败的时候的重试次数:\nDefaultMQProducer producer = new DefaultMQProducer(\u0026#34;please_rename_unique_group_name\u0026#34;); producer.setRetryTimesWhenSendAsyncFailed(3); producer.setRetryTimesWhenSendFailed(3); Consumer 端在消费的时候，如果接收消息的回调函数出现了以下几种情况:\n 抛出异常 返回 NULL 状态 返回 RECONSUME_LATER 状态 超时 15 分钟没有响应  那么 Consumer 便会将消费失败的消息重新调度直到成功消费:\nconsumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List\u0026lt;MessageExt\u0026gt; msgs, ConsumeConcurrentlyContext context) { // 抛出异常  // 返回 NULL 或者 RECONSUME_LATER 状态  return ConsumeConcurrentlyStatus.RECONSUME_LATER; } }); 五、Producer 消息发送重试 发送失败的重试方式，主要表现在发送消息的时候，会最多尝试 getRetryTimesWhenSendFailed() 次发送，当成功发送以后，会直接返回发送结果给调用者。当发送失败以后，会继续进行下一次发送尝试，核心代码如下所示：\npublic class DefaultMQProducerImpl implements MQProducerInner { private SendResult sendDefaultImpl(Message msg, /** 其他参数 **/) throws MQClientException, RemotingException, MQBrokerException, InterruptedException { int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1; int times = 0; for (; times \u0026lt; timesTotal; times++) { // 尝试发送消息，发送成功 return，发送失败 continue  } } } 六、Consumer 消息接受重试 (1) 订阅重试话题 Consumer 在启动的时候，会执行一个函数 copySubscription() ，当用户注册的消息模型为集群模式的时候，会根据用户指定的组创建重试组话题并放入到注册信息中:\npublic class DefaultMQPushConsumerImpl implements MQConsumerInner { public synchronized void start() throws MQClientException { switch (this.serviceState) { case CREATE_JUST: // ...  this.copySubscription(); // ...  this.serviceState = ServiceState.RUNNING; break; } } private void copySubscription() throws MQClientException { switch (this.defaultMQPushConsumer.getMessageModel()) { case BROADCASTING: break; case CLUSTERING: // 重试话题组  final String retryTopic = MixAll.getRetryTopic(this.defaultMQPushConsumer.getConsumerGroup()); SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(this.defaultMQPushConsumer.getConsumerGroup(), retryTopic, SubscriptionData.SUB_ALL); this.rebalanceImpl.getSubscriptionInner().put(retryTopic, subscriptionData); break; default: break; } } } 假设用户指定的组为 “ORDER”，那么重试话题则为 “%RETRY%ORDER”，即前面加上了 “%RETRY%” 这个字符串。\nConsumer 在一开始启动的时候，就为用户自动注册了订阅组的重试话题。即用户不单单只接受这个组的话题的消息，也接受这个组的重试话题的消息。这样一来，就为下文用户如何重试接受消息奠定了基础。\n(2) 失败消息发往重试话题 当 Consumer 客户端在消费消息的时候，抛出了异常、返回了非正确消费的状态等错误的时候，这个时候 ConsumeMessageConcurrentlyService 会收集所有失败的消息，然后将每一条消息封装进 CONSUMER_SEND_MSG_BACK 的请求中，并将其发送到 Broker 服务器:\npublic class ConsumeMessageConcurrentlyService implements ConsumeMessageService { public void processConsumeResult(final ConsumeConcurrentlyStatus status, /** 其他参数 **/) { switch (this.defaultMQPushConsumer.getMessageModel()) { case BROADCASTING: // ...  break; case CLUSTERING: for (int i = ackIndex + 1; i \u0026lt; consumeRequest.getMsgs().size(); i++) { MessageExt msg = consumeRequest.getMsgs().get(i); // 重新将消息发往 Broker 服务器  boolean result = this.sendMessageBack(msg, context); } // ...  break; default: break; } } } 当消费失败的消息重新发送到服务器后，Broker 会为其指定新的话题重试话题，并根据当前这条消息的已有的重试次数来选择定时级别，即将这条消息变成定时消息投放到重试话题消息队列中。可见消息消费失败后并不是立即进行新的投递，而是有一定的延迟时间的。延迟时间随着重试次数的增加而增加，也即投递的时间的间隔也越来越长:\npublic class SendMessageProcessor extends AbstractSendMessageProcessor implements NettyRequestProcessor { private RemotingCommand consumerSendMsgBack(final ChannelHandlerContext ctx, final RemotingCommand request) throws RemotingCommandException { // 指定为重试话题  String newTopic = MixAll.getRetryTopic(requestHeader.getGroup()); int queueIdInt = Math.abs(this.random.nextInt() % 99999999) % subscriptionGroupConfig.getRetryQueueNums(); // 指定为延时信息，设定延时级别  if (0 == delayLevel) { delayLevel = 3 + msgExt.getReconsumeTimes(); } msgExt.setDelayTimeLevel(delayLevel); // 重试次数增加  msgInner.setReconsumeTimes(msgExt.getReconsumeTimes() + 1); // 重新存储  PutMessageResult putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner); // ...  } } 当然，消息如果一直消费不成功，那也不会一直无限次的尝试重新投递的。当重试次数大于最大重试次数 (默认为 16 次) 的时候，该消息将会被送往死信话题队列，认定这条话题投递无门:\npublic class SendMessageProcessor extends AbstractSendMessageProcessor implements NettyRequestProcessor { private RemotingCommand consumerSendMsgBack(final ChannelHandlerContext ctx, final RemotingCommand request) throws RemotingCommandException { // 重试次数大于最大重试次数  if (msgExt.getReconsumeTimes() \u0026gt;= maxReconsumeTimes || delayLevel \u0026lt; 0) { // 死信队列话题  newTopic = MixAll.getDLQTopic(requestHeader.getGroup()); queueIdInt = Math.abs(this.random.nextInt() % 99999999) % DLQ_NUMS_PER_GROUP; } // ...  } } 上述客户端消费失败信息的流程图如下所示:\n扫描下面二维码，在手机端阅读：\n"});index.add({'id':94,'href':'/docs/books/','title':"书籍",'content':"书籍  书籍是人类进步的阶梯。\u0026ndash; 高尔基\n "});index.add({'id':95,'href':'/docs/books/in-depth_analysis_of_the_core_technology_of_apache_dubbo/','title':"深度剖析 Apache Dubbo 核心技术",'content':"深度剖析 Apache Dubbo 核心技术 SPI 扩展 Dubbo 支持扩展的核心接口上，都会通过类似 @SPI(\u0026quot;dubbo\u0026quot;) 这样的注解，来标识当前接口的默认实现。如果你想替换掉这个默认实现，那么需要两个步骤。第一，实现 Protocol 接口，然后在 META-INF/dubbo 目录下创建一个名字为 org.apache.dubbo.rpc.Protocol 的文本文件。这个 META-INF 目录如果使用的是 IDEA 开发，那么其应该放到 resources 目录下的顶层，这样打 jar 包的时候，其也会被复制到 jar 包的第一级目录。内容如下：\nmyProtocol = com.zk.MyProtocol 第二，需要在 XML 配置文件中，声明使用这个扩展实现：\n\u0026lt;dubbo:protocol name=\u0026#34;myProtocol\u0026#34;\u0026gt; 其实 JDK 本身也提供了 SPI 扩展，Dubbo 之所以没有使用默认提供的实现，是因为：\n JDK 标准的 SPI 一次性实例化扩展点的所有实现，如果有些没有使用到，那么会浪费资源。 扩展点加载失败的异常提示不是很好。 增强了 Ioc 和 AOP 的支持。  性能 Dubbo 会给每个服务提供者的实现类生产一个 Wrapper 类，这个 Wrapper 类里面最终调用服务提供者的接口实现类，Wrapper 类的存在是为了减少反射的调用。当服务提供方收到消费方发来的请求后，需要根据消费者传递过来的方法名和参数反射调用服务提供者的实现类，而反射本身是有性能开销的，Dubbo 把每个服务提供者的实现类通过 JavaAssist 包装为一个 Wrapper 类以减少反射调用开销。\n其实就是由反射改为了比较方法名称，然后调用，伪代码如下：\nGreetingServiceImpl impl = (GreetingServiceImpl) object; if (\u0026#34;sayHello\u0026#34;.equals(methodName) \u0026amp;\u0026amp; argClass.length == 1) { return impl.sayHello((String) argObject[0]); } if (\u0026#34;testGeneric\u0026#34;.equals(methodName) \u0026amp;\u0026amp; argClass.length == 1) { return impl.testGeneric((Pojo) arrObject[0]); } 容错 异常情况下的，代码逻辑应该怎么走？Dubbo 提供了如下几种容错方案：\n 失败重试：通常用于读操作或者具有幂等的写操作。需要注意的是，重试会带来更长延迟。 快速失败：抛出异常。 安全失败：忽略异常，场景：写入审计日志。 失败自动恢复：后台记录失败请求，并按照策略后期再重试，场景：消息通知。 并行调用：通常用于实时性要求较高的读操作，但需要浪费更多服务资源。 广播调用：通常用于通知所有提供者更新缓存或日志等本地资源信息。  负载均衡  随机策略 轮循策略 最少活跃调用数 一致性 Hash 策略  协议设计 服务消费端如何把服务请求信息序列化为二进制数据、服务提供方如何把消费端发送的二进制数据反序列化为可识别的POJO对象、Dubbo的应用层协议是怎么样的？\n看一下这个 \u0026ldquo;request flag and serialization id\u0026rdquo;：高四位标示请求类型：\n低四位标示序列化方式，其枚举值如下：\n再后面的一字节是只在响应报文里才设置（在请求报文里不设置），用来标示响应的结果码，具体定义如下：\n在此列出这个编码格式，是想要学习 Dubbo 是如果用较少的字节头，编码较多的信息的。还有编码的粒度，响应码这部分，并没有直接定义与业务紧密关联的状态码，比如 \u0026ldquo;磁盘存储失败\u0026rdquo; 等状态码，相反定义的是较为粗粒度的状态码，更为细粒度的可以放到 \u0026ldquo;body\u0026rdquo; 里面。\n"});index.add({'id':96,'href':'/docs/programmer-interview/algorithm/binary-search/','title':"Binary Search 二分搜索",'content':"Binary Search // https://leetcode.com/problems/binary-search/ // public class BinarySearch { public int search(int[] nums, int target) { if (nums == null || nums.length == 0) { return -1; } int lo = 0; int hi = nums.length - 1; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } else if (nums[m] \u0026gt; target) { hi = m - 1; } else { lo = m + 1; } } return -1; } } "});index.add({'id':97,'href':'/docs/tutorial/network/dns/','title':"DNS",'content':"DNS DNS 使用的传输层协议 主要使用 UDP 协议，端口 53, 也有一些采用 TCP 来实现。\nDNS 层次结构  根 DNS 服务器 ：返回顶级域 DNS 服务器的 IP 地址 顶级域 DNS 服务器：返回权威 DNS 服务器的 IP 地址 权威 DNS 服务器 ：返回相应主机的 IP 地址  DNS 数据库的记录类型    类型 描述     A、AAAA IP 地址   MX SMTP mail exchangers   NS name servers   CNAME domain name aliases    DNS 的解析流程 DNS 做负载均衡 DNS 除了可以通过名称映射为 IP 地址，它还可以做另外一件事，就是负载均衡。\n一个应用要访问数据库，在这个应用里面应该配置这个数据库的 IP 地址，还是应该配置这个数据库的域名呢？显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如果有多个应用都配置了这台数据库的话，一换 IP 地址，就需要将这些应用全部修改一遍。但是如果配置了域名，则只要在 DNS 服务器里，将域名映射为新的 IP 地址，这个工作就完成了，大大简化了运维。\n某个应用要访问另外一个应用，如果配置另外一个应用的 IP 地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。但是，访问它的应用，如何在多个之间进行负载均衡？只要配置成为域名就可以了。在域名解析的时候，我们只要配置策略，这次返回第一个 IP，下次返回第二个 IP，就可以实现负载均衡了。\nDNS 做全局负载均衡 为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的 IP 地址。当用户访问某个域名的时候，这个 IP 地址可以轮询访问多个数据中心。如果一个数据中心因为某种原因挂了，只要在 DNS 服务器里面，将这个数据中心对应的 IP 地址删除，就可以实现一定的高可用。另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体验就会非常好，访问速度就会超快。这就是全局负载均衡的概念。\n 图中的 AZ 代表 Available Zone  对于不需要做全局负载均衡的简单应用来讲，yourcompany.com 的权威 DNS 服务器可以直接将 object.yourcompany.com 这个域名解析为一个或者多个 IP 地址，然后客户端可以通过多个 IP 地址，进行简单的轮询，实现简单的负载均衡。\n但是对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而需要专门的设备或者服务器来做这件事情，这就是全局负载均衡器（GSLB，Global Server Load Balance）。\n这种情况下，在 yourcompany.com 的 DNS 服务器中，返回的就不再是 IP 了，而是通过配置 CNAME 的方式，给 object.yourcompany.com 起一个别名，例如 object.vip.yourcomany.com，然后告诉本地 DNS 服务器，让它请求 GSLB 解析这个域名，GSLB 就可以在解析这个域名的过程中，通过自己的策略实现负载均衡。\n"});index.add({'id':98,'href':'/docs/tutorial/git/merge-multiple-commit/','title':"Git 多次提交合并成一次提交",'content':"Git 多次提交合并成一次提交 你在 dev 分支上开发某个功能，在本地执行了三次 commit，注意这三次 commit 都没有 push 到远程分支，都只是在本地存在。现在你想要在 push 之前，将你本地的这多个 commit 合并成一个 commit，请问应该怎么做？\n答案是：git rebase -i HEAD~N，N 代表你想把最近的几条 commitId 记录合并。具体操作步骤如下：\n查看提交记录 git log 查看提交记录：\n871adf OK, feature Z is fully implemented --- newer commit --┐ 0c3317 Whoops, not yet... | 87871a I'm ready! | 643d0e Code cleanup |-- Join these into one afb581 Fix this and that | 4e9baa Cool implementation | d94e78 Prepare the workbench for feature Z -------------------┘ 6394dc Feature Y --- older commit  假设 6394dc 提交已经 push 上去了 你现在想把 d94e78 ~ 871adf 这几个 commit 合并一下  即最终你再次执行 git log 想要看到的效果如下：\n84d1f8 Feature Z --- newer commit (result of rebase) 6394dc Feature Y --- older commit 从 d94e78 ~ 871adf 共有 7 个 commit，因此执行命令：\ngit rebase -i HEAD~7  -i 是 --interactive 参数的缩写表达，即交互的 rebase\n 现在只有 7 个 commit，数起来还简单一些。假设，我需要将 70 个 commit 合并，难不成我还要一个一个精确的数？答案是不需要。后面加上最后一次的 commitId 也可以，含义是从这个 commitId 之后的多个 commit 都要合并到一起，但是并不包含这个 commit：\ngit rebase -i 6394dc 合并 commit 记录 执行 git rebase -i HEAD~7 命令后，你将会进入到命令行编辑其中（比如 Vi 中），然后选择这些 commit 如何进行合并。\n 在这个地方特别需要注意，在编辑器中，现在看到的旧的提交位于第一行，新的提交位于最后一行，顺序和 git log 查看的顺序颠倒了。\n pick d94e78 Prepare the workbench for feature Z --- older commit pick 4e9baa Cool implementation pick afb581 Fix this and that pick 643d0e Code cleanup pick 87871a I'm ready! pick 0c3317 Whoops, not yet... pick 871adf OK, feature Z is fully implemented --- newer commit 然后下面其实也有注释：\n# Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec = run command (the rest of the line) using shell 下面对 p 和 s 命令，简单解释：\n p：这条 commit 依然保留，不要被去掉，这条 commit 的信息也依然维持原样，最终的 git log 也依然能看到这条 commit s：将这条 commit 与前一次 commit  合并到一起  我们在编辑器中，使用 s 来将最近 6 次的 commit 合并到第一次 commit 上，修改如下：\npick d94e78 Prepare the workbench for feature Z --- older commit s 4e9baa Cool implementation s afb581 Fix this and that s 643d0e Code cleanup s 87871a I'm ready! s 0c3317 Whoops, not yet... s 871adf OK, feature Z is fully implemented --- newer commit 然后输入 :wq 保存编辑器的内容。\n创建一个新的 commit 上述当离开编辑器的时候，Git 会再次弹出一个编辑器让你输入此次合并 commit 的信息输入界面：\nPrepare the workbench for feature Z Cool implementation Fix this and that Code cleanup I'm ready! Whoops, not yet... OK, feature Z is fully implemented 你可以在这个地方修改信息，也可以直接使用 Git 给你生成好的信息，之后再次输入 :wq 保存即可。\n参考  Squash commits into one with Git Git Interactive Rebase, Squash, Amend and Other Ways of Rewriting History  扫描下面二维码，在手机端阅读：\n"});index.add({'id':99,'href':'/docs/tutorial/unix-command/os-release/','title':"os-release",'content':"如何知道是 Ubuntu 还是 Cent OS 系统 ? $ cat /etc/os-release NAME=\u0026#34;Ubuntu\u0026#34; VERSION=\u0026#34;20.04.1 LTS (Focal Fossa)\u0026#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026#34;Ubuntu 20.04.1 LTS\u0026#34; VERSION_ID=\u0026#34;20.04\u0026#34; HOME_URL=\u0026#34;https://www.ubuntu.com/\u0026#34; SUPPORT_URL=\u0026#34;https://help.ubuntu.com/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.launchpad.net/ubuntu/\u0026#34; PRIVACY_POLICY_URL=\u0026#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026#34; VERSION_CODENAME=focal UBUNTU_CODENAME=focal "});index.add({'id':100,'href':'/docs/rocketmq/rocketmq-master-slave-sync/','title':"RocketMQ 主备同步",'content':"RocketMQ 主备同步 介绍 RocketMQ 的主备同步机制\n一、简介 RocketMQ 通过 Master-Slave 主备机制，来实现整个系统的高可用，具体表现在:\n Master 磁盘坏掉，Slave 依然保存了一份 Master 宕机，不影响消费者继续消费  二、搭建环境 我们在一台机器上搭建一个 Master 一个 Slave 的环境:\n为了能够将 Master 和 Slave 搭建在同一台计算机上，我们除了需要将 Broker 的角色设置为 SLAVE ，还需要为其指定单独的 brokerId、 storePathRootDir、 storePathCommitLog。\n// SLAVE 角色 messageStoreConfig.setBrokerRole(BrokerRole.SLAVE); // 一个机器如果要启动多个 Broker，那么每个 Broker 的 store 根目录必须不同 messageStoreConfig.setStorePathRootDir(storePathRootDir); // 一个机器如果要启动多个 Broker，那么每个 Broker 的 storePathCommitLog 根目录必须不同 messageStoreConfig.setStorePathCommitLog(storePathCommitLog); // 设置 Slave 的 Master HA 地址 messageStoreConfig.setHaMasterAddress(\u0026#34;localhost:10912\u0026#34;); // SLAVE 角色的 brokerId 必须大于 0 brokerConfig.setBrokerId(1); 注意 Slave 和 Master 的 brokerName 必须一致，即它们必须处于同一个 BrokerData 数据结构里面。实际上在做了如上的修改之后， Slave 和 Master 依旧不能同时运行在同一台机器上，因为 Slave 本身也可以称为 Master，接受来自其他 Slave 的请求，因此当运行 Slave 的时候，需要将 HAService 里面的启动 AcceptSocketService 运行的相关方法注释掉。\n三、建立连接 当一个 Broker 在启动的时候，会调用 HAService 的 start() 方法:\npublic class HAService { public void start() throws Exception { this.acceptSocketService.beginAccept(); this.acceptSocketService.start(); this.groupTransferService.start(); this.haClient.start(); } } AcceptSocketService 服务的功能是 Master 等待接受来自其它客户端 Slave 的连接，当成功建立连接后，会将这条连接 HAConnection 放入到 connectionList 连接列表里面。而 HAClient 服务的功能是 Slave 主动发起同其它 Master 的连接。\n四、数据传输 当启动 HAService 之后，一旦 Master 发现和 Slave 不同步，那么Master 会自动开始同步消息到 Slave，无需其它的触发机制。\n(1) 消息异步传输 如果 Master Broker 的角色是 ASYNC_MASTER，那么消息等待从 Master 同步到 Slave 的方式是异步传输的方式。这意味当一条消息发送到 Master Broker 的时候，Master Broker 在存储完这条消息到本地之后，并不会等待消息同步到 Slave Broker 才返回。这种方式会缩短发送消息的响应时间。\n(2) 消息同步传输 如果 Master Broker 的角色是 SYNC_MASTER，那么消息等待从 Master 同步到 Slave 的方式是同步传输的方式。除此之外，进入同步方式还得满足另外两个条件：\n 消息体的 PROPERTY_WAIT_STORE_MSG_OK 属性值为 true，即这条消息允许等待 Slave 相比 Master 落下的同步进度不能超过 256MB  public class CommitLog { public void handleHA(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { if (BrokerRole.SYNC_MASTER == this.defaultMessageStore.getMessageStoreConfig().getBrokerRole()) { HAService service = this.defaultMessageStore.getHaService(); // 消息是否允许等待同步  if (messageExt.isWaitStoreMsgOK()) { // Slave 是否没有落下 Master 太多  if (service.isSlaveOK(result.getWroteOffset() + result.getWroteBytes())) { // 等待同步完成  // ...  } // Slave problem  else { // Tell the producer, slave not available  putMessageResult.setPutMessageStatus(PutMessageStatus.SLAVE_NOT_AVAILABLE); } } } } } 其中 isSlaveOK 方法就是用来检测 Slave 和 Master 落下的同步进度是否太大的:\npublic class HAService { public boolean isSlaveOK(final long masterPutWhere) { boolean result = this.connectionCount.get() \u0026gt; 0; result = result \u0026amp;\u0026amp; ((masterPutWhere - this.push2SlaveMaxOffset.get()) \u0026lt; this.defaultMessageStore .getMessageStoreConfig() .getHaSlaveFallbehindMax()); // 默认 256 * 1024 * 1024 = 256 MB  return result; } } 如果上面两个条件不满足的话，那么 Master 便不会再等待消息同步到 Slave 之后再返回，能尽早返回便尽早返回了。\n消息等待是否同步到 Slave 是借助 CountDownLatch 来实现的。当消息需要等待的时候，便会构建一个 GroupCommitRequest ，每个请求在其内部都维护了一个 CountDownLatch ，然后通过调用 await(timeout) 方法来等待消息同步到 Slave 之后，或者超时之后自动返回。\npublic static class GroupCommitRequest { private final CountDownLatch countDownLatch = new CountDownLatch(1); public void wakeupCustomer(final boolean flushOK) { this.flushOK = flushOK; this.countDownLatch.countDown(); } public boolean waitForFlush(long timeout) { try { this.countDownLatch.await(timeout, TimeUnit.MILLISECONDS); return this.flushOK; } catch (InterruptedException e) { log.error(\u0026#34;Interrupted\u0026#34;, e); return false; } } } 我们再重点来看几个循环体和唤醒点:\n GroupTransferService 服务的是否处理请求的循环体和唤醒点:  class GroupTransferService extends ServiceThread { public synchronized void putRequest(final CommitLog.GroupCommitRequest request) { // ...  // 放入请求，唤醒  if (hasNotified.compareAndSet(false, true)) { waitPoint.countDown(); // notify  } } public void run() { // 循环体  while (!this.isStopped()) { try { // putRequest 会提前唤醒这句话  this.waitForRunning(10); this.doWaitTransfer(); } catch (Exception e) { log.warn(this.getServiceName() + \u0026#34; service has exception. \u0026#34;, e); } } } }  HAConnection 的是否进行消息传输的循环体和唤醒点：  class WriteSocketService extends ServiceThread { @Override public void run() { // 循环体  while (!this.isStopped()) { SelectMappedBufferResult selectResult = HAConnection.this.haService.getDefaultMessageStore().getCommitLogData(this.nextTransferFromWhere); if (selectResult != null) { // 传输（写入）消息  } else { // 等待 100 毫秒或者提前被唤醒  HAConnection.this.haService.getWaitNotifyObject().allWaitForRunning(100); } } } } public class CommitLog { public void handleHA(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes()); service.putRequest(request); // 提前唤醒 WriteSocketService  service.getWaitNotifyObject().wakeupAll(); } }  Slave 汇报进度唤醒 GroupTransferService， 等待同步完成唤醒 GroupCommitRequest 的 CountDownLatch:  class ReadSocketService extends ServiceThread { private boolean processReadEvent() { // 唤醒 GroupTransferService  HAConnection.this.haService.notifyTransferSome(HAConnection.this.slaveAckOffset); } } class GroupTransferService extends ServiceThread { // 被唤醒  public void notifyTransferSome() { this.notifyTransferObject.wakeup(); } private void doWaitTransfer() { for (CommitLog.GroupCommitRequest req : this.requestsRead) { boolean transferOK = HAService.this.push2SlaveMaxOffset.get() \u0026gt;= req.getNextOffset(); // 5 次重试  for (int i = 0; !transferOK \u0026amp;\u0026amp; i \u0026lt; 5; i++) { // 等待被唤醒或者超时  this.notifyTransferObject.waitForRunning(1000); transferOK = HAService.this.push2SlaveMaxOffset.get() \u0026gt;= req.getNextOffset(); } // 唤醒 GroupCommitRequest 的 CountDownLatch  req.wakeupCustomer(transferOK); } } } public static class GroupCommitRequest { // 被唤醒  public void wakeupCustomer(final boolean flushOK) { this.flushOK = flushOK; this.countDownLatch.countDown(); } } 下图是上图一个完整的消息唤醒链:\n五、主备消费 当消费者在消费的时候，如果 Master 突然宕机，那么消费者会自动切换到 Slave 机器上继续进行消费。\n六、消费建议 RocketMQ 提供了自动从 Slave 读取老数据的功能。这个功能主要由 slaveReadEnable 这个参数控制。默认是关的（slaveReadEnable = false）。推荐把它打开，主从都要开。这个参数打开之后，在客户端消费数据时，会判断，当前读取消息的物理偏移量跟最新的位置的差值，是不是超过了内存容量的一个百分比（accessMessageInMemoryMaxRatio = 40 by default）。如果超过了，就会告诉客户端去备机上消费数据。如果采用异步主从，也就是 brokerRole 等于 ASYNC_AMSTER 的时候，你的备机 IO 打爆，其实影响不太大。但是如果你采用同步主从，那还是有影响。所以这个时候，最好挂两个备机。因为 RocketMQ 的主从同步复制，只要一个备机响应了确认写入就可以了，一台 IO 打爆，问题不大。参考自阿里中间件团队博客。\n七、异常处理 Q: Master(Slave) 读取来自 Slave(Master) 的消息异常 (IOException、 read() 返回 -1 等) 的时候怎么处理? A: 打印日志 + 关闭这条连接\nQ: Master(Slave) 长时间没有收到来自 Slave(Master) 的进度汇报怎么处理? A: 每次读取之后更新 lastReadTimestamp 或者 lastWriteTimestamp，一旦发现在 haHousekeepingInterval 间隔内 (默认 20秒) 这个时间戳都没有改变的话，关闭这条连接\nQ: Slave 检测到来自 Master 汇报的本次传输偏移量和本地的传输偏移量不同时怎么处理? A: 打印日志 + 关闭这条连接\nQ: Master 如何知道 Slave 是否真正的存储了刚才发送过去的消息? A: Slave 存储完毕之后，通过向 Master 汇报进度来完成。相当于 TCP 的 ACK 机制。\nQ: Master 宕掉 A: 无论 Maser 是主动关闭 Mater，还是 Master 因为异常而退出，Slave 都会每隔 5 秒重连一次 Master\n扫描下面二维码，在手机端阅读：\n"});index.add({'id':101,'href':'/docs/books/everyone-is-architect/','title':"人人都是架构师 (一)",'content':"人人都是架构师 - 分布式系统架构落地与瓶颈突破 分布式系统应对高并发、大流量的常用手段：\n 扩容 动静分离 缓存 服务降级 限流  限流 常见算法：\n 令牌桶，Nginx 限流模块用的是这个：限制的是流量的平均流入速率，允许一定程度上的突发流量。 漏桶：限制的是流出速率，并且这个速率还是保持不变的，不允许突发流量。  Nginx 限流 http { # 每个 IP 的 session 空间大小 limit_zone one $binary_remote_addr 20m; # 每个 IP 每秒允许发起的请求数 limit_req_zone $binary_remote_addr zone=req_one:20m rate=10r/s; # 每个 IP 能够发起的并发连接数 limit_conn one 10; # 缓存还没有来得及处理的请求 limit_req zone=req_one burst=100; } 消峰  活动分时段 答题验证  高并发读 \u0026ldquo;马某出轨王某\u0026rdquo;、\u0026ldquo;iPhone SE 2020 发布\u0026rdquo; 等这种热点新闻的 key 会始终落在同一个缓存节点上，分布式缓存一定会出现单点瓶颈，其资源连接容易瞬间耗尽。有如下两种方案解决这个问题：\n 基于 Redis 的集群多写多读方案。  多写如何保持一致性：将 Key 配置在 ZooKeeper，客户端监听 ZNode，一旦变化，全量更新本地持有的 Key   LocalCache 结合 Redis 集群的多级 Cache 方案。  LocalCache 拉取下来的商品数量有 5 个，但是实际上只有 4 个了，怎么解决？对于这种读场景，允许接受一定程度上的数据脏读，最终扣减库存的时候再提示商品已经售罄即可。    实时热点自动发现 交易系统产生的相关数据、上游系统中埋点上报的数据这两个，异步写入日志，对日志进行次数统计和热点分析\n高并发写 InnoDB 行锁 乐观锁扣减：\nSELECT stock, version FROM item WHERE item_id = 1; UPDATE ITEM SET version = version + 1, stock = stock - 1 WHERE item_id = 1 AND version = version; 引入条件 \u0026ldquo;实际库存数 \u0026gt;= 扣减库存数\u0026rdquo;：\nUPDATE item SET stock = stock - 1 WHERE item_id = 1 AND stock \u0026gt;= 1; 查询队列中等待拿锁的线程：\nSELECT * FROM information_schema.INNODB_TRX WHERE trx_state = \u0026#39;LOCK_WAIT\u0026#39;; Redis Redis 读写能力远胜任何类型的关心型数据库。使用 Redission 实现分布式锁，避免超卖：\nRedissionClient redission = null; try { redission = Redission.create(config); RLock lock = redission.getLock(\u0026#34;testLock\u0026#34;); // lock(long leaseTime, TimeUnit unit)  // 某个线程没有获取到锁，那么这个线程只能在队列中阻塞等待，与 InnoDB 如出一辙  lock.lock(20, TimeUnit.MILLISECONDS); lock.unlock(); // tryLock(long waitTime, long leaseTime, TimeUnit unit)  // 并发较大的情况下，建议使用这个  boolean result = lock.tryLock(10, 20, TimeUnit.MILLISECONDS); if (result) { lock.forceUnlock(); } } finally { if (null != redission) { redission.shutdown(); } } 扣除库存成功后的消息，通过消息队列写入到数据库中，由于才用了排队机制，并发写入数据库的流量可控，数据库负载压力始终保持在一个恒定的范围内。\n批处理 如何有效减少获取锁的次数，提升系统整体的 TPS？\n批量提交扣减商品库：先收集扣减请求，达到某个阈值，对请求进行合并，获取一次分布式锁。缺点：库存不足，这一批全部扣减失败。\n控制单机并发写  单机排队串行写 抢购限流  分布式 SequenceID 生成 Shark（一款开源的 MySQL 分库分表中间件）内部提供了生成 SequenceID 的 API （底层支持数据库和 ZooKeeper 作为申请 SequenceID 的存储系统）：\nCREATE TABLE shark_sequenceid( s_id INT NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, s_type INT NOT NULL COMMENT \u0026#39;类型\u0026#39;, s_useData BIGINT NOT NULL COMMENT \u0026#39;申请占位数量\u0026#39;, PRIMARY KEY(s_id) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE utf8mb4_bin; 通过如下 API 获取：\n// (int idcNum, int type, long memData) SequenceIDManager.getSequenceId(100, 10, 5000); 第一个参数：IDC 机房编码，第二个参数：业务类别，第三个参数：向数据库申请的 ID 缓存数，返回一个长度为 19 位的 SequenceID。\nShark 只是负责封装 ID 的生成逻辑，真正保证唯一性和连续性的还是单点数据库。\n多维度复杂查询 Solr 的目的就是要替换 SQL 中的 like '%香水%' 这种模糊查询，因为数据库会采用全表扫描。\n"});index.add({'id':102,'href':'/docs/tutorial/sentinel/','title':"阿里巴巴 Sentinel",'content':"阿里巴巴 Sentinel "});index.add({'id':103,'href':'/docs/programmer-interview/','title':"👍 程序员面试题",'content':"👍 程序员面试题    目录         Java 前端 数据结构 算法 设计模式    "});index.add({'id':104,'href':'/docs/programmer-interview/algorithm/circular-array/','title':"Circular Array (循环数组)",'content':"Circular Array 循环数组 来自一亩三分地，微软面试官问的问题。过去任意 1 秒内来自同一 IP 的请求是否超过 100 次，可以用循环数组可以做。\n 微软\n // // https://www.javaguides.net/2018/09/queue-implementation-using-circular-array-in-java.html // package com.zk.algorithm.array; /** * Queue Implementation using Circular Array * @author Ramesh Fadatare * */ public class CircularArray { // Array used to implement the queue.  private int[] queueRep; // 添加数据，存放在 (rear + 1) % size，size++  // 取出数据，(front + 1) % size，size--  private int size, front, rear; // Length of the array used to implement the queue.  private static final int CAPACITY = 16; //Default Queue size  // Initializes the queue to use an array of default length.  public CircularArray (){ queueRep = new int [CAPACITY]; size = 0; front = 0; rear = 0; } // Initializes the queue to use an array of given length.  public CircularArray (int cap){ queueRep = new int [ cap]; size = 0; front = 0; rear = 0; } // Inserts an element at the rear of the queue. This method runs in O(1) time.  public void enQueue (int data)throws NullPointerException, IllegalStateException{ if (size == CAPACITY) throw new IllegalStateException (\u0026#34;Queue is full: Overflow\u0026#34;); else { size++; queueRep[rear] = data; rear = (rear+1) % CAPACITY; } } // Removes the front element from the queue. This method runs in O(1) time.  public int deQueue () throws IllegalStateException{ // Effects: If queue is empty, throw IllegalStateException,  // else remove and return oldest element of this  if (size == 0) throw new IllegalStateException (\u0026#34;Queue is empty: Underflow\u0026#34;); else { size--; int data = queueRep [ (front % CAPACITY) ]; queueRep [front] = Integer.MIN_VALUE; front = (front+1) % CAPACITY; return data; } } // Checks whether the queue is empty. This method runs in O(1) time.  public boolean isEmpty(){ return (size == 0); } // Checks whether the queue is full. This method runs in O(1) time.  public boolean isFull(){ return (size == CAPACITY); } // Returns the number of elements in the queue. This method runs in O(1) time.  public int size() { return size; } // Returns a string representation of the queue as a list of elements, with  // the front element at the end: [ ... , prev, rear ]. This method runs in O(n)  // time, where n is the size of the queue.  public String toString(){ String result = \u0026#34;[\u0026#34;; for (int i = 0; i \u0026lt; size; i++){ result += Integer.toString(queueRep[ (front + i) % CAPACITY ]); if (i \u0026lt; size -1) { result += \u0026#34;, \u0026#34;; } } result += \u0026#34;]\u0026#34;; return result; } public static void main(String[] args) { CircularArray arrayQueue = new CircularArray(); arrayQueue.enQueue(10); arrayQueue.enQueue(20); arrayQueue.enQueue(30); arrayQueue.enQueue(40); arrayQueue.enQueue(50); arrayQueue.enQueue(60); arrayQueue.enQueue(70); arrayQueue.enQueue(80); arrayQueue.enQueue(90); arrayQueue.deQueue(); System.out.println(arrayQueue.toString()); } } "});index.add({'id':105,'href':'/docs/tutorial/git/git-branch/','title':"Git 分支",'content':"Git 分支 Git 的分支管理命令：git branch。\n示例 列举本地所有分支 git branch  当前分支会用 * 标识出来，也会用特别的颜色标识出来\n 列举本地和远程所有分支 git branch -a 创建分支 git branch \u0026lt;branchName\u0026gt; 删除分支 # 删除时，会检查此分支是否已经合并到其它分支，否则拒绝删除 git branch -d \u0026lt;branchName\u0026gt; # 不管有没有合并到其它分支，都强制删除分支 git branch -D \u0026lt;branchName\u0026gt; 重命名分支 # 如果版本库已经存在 newbranch，则拒绝重命名 git branch -m \u0026lt;oldbranch\u0026gt; \u0026lt;newbranch\u0026gt; # 如果版本库已经存在 newbranch，则强制重命名 git branch -M \u0026lt;oldbranch\u0026gt; \u0026lt;newbranch\u0026gt; 创建并切换分支 git checkout -b \u0026lt;new_branch\u0026gt; 扫描下面二维码在手机端阅读：\n"});index.add({'id':106,'href':'/docs/tutorial/network/tcp/','title':"TCP",'content':"TCP TCP 头 三次握手 四次挥手 起始序列号 ISN ISN 为什么不从 1 开始 client 第一次尝试建立连接: client 发送 1,2,3 数据包给 server，我们假设这个时候 3 这个数据包丢失了。紧接着 client 第二次尝试建立连接: client 发送 1,2 给 server，主观上并没有想发送 3 这个数据包，但是第一次的 3 数据包可能又会回来发送给 server，因此造成数据错误)。\n起始序列号 ISN 如何计算 起始 ISN 是基于时钟的，每 4 毫秒加一，转一圈要 4.55 个小时。\nTCP 初始化序列号不能设置为一个固定值，因为这样容易被攻击者猜出后续序列号，从而遭到攻击。 RFC1948 中提出了一个较好的初始化序列号 ISN 随机生成算法。\nISN = M + F (localhost, localport, remotehost, remoteport) M 是一个计时器，这个计时器每隔 4 毫秒加 1。F 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值。要保证 Hash 算法不能被外部轻易推算得出，用 MD5 算法是一个比较好的选择。\n异常情况如何响应 出现如下异常情况，TCP 会如何响应。\nServer 端口没有打开 内核给 Client 响应一个 RST\nServer 所在的机器都没有开机 Client 等待 6s 重发一个 SYN，无响应则等待 24s 再发一个，若总共等待 75s 之后，则触发 ETIMEDOUT。通过 setConnectTimeout() 来改善这个等待时间，使其可以在合理的毫秒以内返回。\nServer 宕机 客户端持续重传，并且 read 调用会阻塞，一段时间后 TIMEOUT，使用 setReadTimeout() 来改善这个阻塞时间。\nServer 宕机又重启 收到了一个这个连接上不该有的，所以响应一个 RST。\nTCP close() vs shutdown()  调用 close() 函数会把读和写两个方向的，同时关闭 调用 shutdown() 可以只关闭一半的 TCP 连接。  大量 TIME_WAIT 状态 如果存在大量的 TIME_WAIT，往往是因为短连接太多，不断的创建连接，然后释放连接，从而导致很多连接在这个状态，可能会导致无法发起新的连接。解决的方式往往是：\n 打开 tcp_tw_recycle 和 tcp_timestamps 选项； 打开 tcp_tw_reuse 和 tcp_timestamps 选项； 程序中使用 SO_LINGER，应用强制使用 rst 关闭。  什么时候发送 RST 响应 当客户端收到 Connection Reset，往往是收到了 TCP 的 RST 消息，RST 消息一般在下面的情况下发送：\n 试图连接一个未被监听的服务端； 对方处于 TIME_WAIT 状态，或者连接已经关闭处于 CLOSED 状态，或者重新监听 seq num 不匹配； 发起连接时超时，重传超时，keepalive 超时 在程序中使用 SO_LINGER，关闭连接时，放弃缓存中的数据，给对方发送 RST  "});index.add({'id':107,'href':'/docs/tutorial/zipkin/','title':"Zipkin 源码分析",'content':"Zipkin 源码分析 Zipkin是一款开源的分布式实时数据追踪系统（Distributed Tracking System），基于 Google Dapper的论文设计而来，由 Twitter 公司开发贡献。其主要功能是聚集来自各个异构系统的实时监控数据。\n架构图 四大组件 Collector Zipkin 收集上来的数据，交由 Collector 进行校验、存储、索引，以便后续查询。\nStorage 搜索 Web UI 官方提供的例子  brave-webmvc-example  "});index.add({'id':108,'href':'/docs/cloud-plus-bbs/','title':"云+社区技术沙龙",'content':"云+社区技术沙龙 云+社区沙龙 online，是腾讯云推出的一系列由技术专家、高级工程师、技术总监等在线直播的技术分享沙龙。本专栏整理收录了观看部分课程的心得体会。如需更多课程请访问腾讯云沙龙 。\n"});index.add({'id':109,'href':'/docs/books/the-art-of-readable-code/','title':"编写可读代码的艺术",'content':"编写可读代码的艺术 代码应当易于理解 Q: 可读性基本定理？\n 可读性基本原理：使别人理解它所需的时间最小化。\n Q: 代码总是越小越好？\n 减少代码行数是一个好目标，但是把理解代码所需的时间最小化是一个更好的目标。\n 表面层次的改进 把信息装到名字里 （1）选择专业的词\ndef getPage(url) 上述例子，get 词没有表达出很多信息。从本地缓存得到一个页面，还是从数据库中，或者从互联网中？如果从互联网中，应该使用更为专业的名字：fetchPage(url) 或 downloadPage(url)。\nclass BinaryTree { int size(); } 上述例子，size() 返回的是什么？树的高度，节点树，还是树在内存中所占的空间？size() 没有承载更多的信息，更专业的词汇是 height()、numNodes() 或 memoryBytes()。\n英语是一门丰富的语言，有很多词汇可以选择。下面是一些例子，这些单词更富有表现力，可能更适合你的语境：\n   单词 更多选择     send deliver、dispatch、announce、distribute、route   find search、extract、locate、recover   start launch、create、begin、open   make create、set up、build、generate、compose、add、new    （2）避免像 tmp 和 retval 这样泛泛的名字\n除非你有更好的理由！\n（3）用具体的名字代替抽象的名字\nsearchCanStart() 比 canListenOnPort() 更具体一些，这个名字直接描述了方法所做的事情。\n（4）为名字附带更多信息\n附带单位：\n   单词 更多选择     start(int delay) delay -\u0026gt; delay_secs   createCache(int size) size -\u0026gt; size_mb   throttleDownload(float limit) limit -\u0026gt; max_kbps   rotate(float angle) angle -\u0026gt; degrees_cw    附带其它重要属性：\n（5）名字应该有多长\n作用域大的名字采用更长的名字，不要用让人费解的一个或两个字母的名字来命名在几屏之间都可见的变量。对于只存在于几行之间的变量用短一点的名字更好。\n不会误解的名字 （1）例子：filter()\nresults = database.allObjects.filter(\u0026#34;year \u0026lt;= 2011\u0026#34;); 这里的 filter 是挑出还是减掉？\n（2）例子：clip(text, length)\n这里的 clip 是从尾部删除掉 length 的长度？还是裁掉最大长度为 length 的一段？\n（3）推荐用 min 和 max 来表示极限\n优化前 CART_TOO_BIG_LIMIT = 10; if shoppingCart.numItems() \u0026gt;= CART_TOO_BIG_LIMIT: error(\u0026#34;too many items in cart\u0026#34;)   优化后 static final MAX_ITEMS_IN_CART = 10; if shoppingCart.numItems() \u0026gt; MAX_ITEMS_IN_CART: error(\u0026#34;too many items in cart.\u0026#34;)    （4）推荐用 first 和 last 来表示包含的范围\nset.printKeys(first = \u0026#34;Bart\u0026#34;, last = \u0026#34;Maggie\u0026#34;) （5）推荐用 begin 和 end 来表示包含/排除范围\n（6）给布尔值命名\n 通常加上 is、has、can、should 这样的词，可以把布尔值变得更明确（存疑：JSON 序列化是否有影响？） 最好避免使用反义名字  （7）与使用者的期望相匹配\n如果 getMean() 做的是一个比较重的任务，并不是一个轻量级访问器，那么这样的命名换成 computeMean() 会更好一些。\n审美 （1）重新安排换行来保持一致\nNO  YES   （2）个人风格与一致性\n 一致的风格比 “正确” 的风格更为重要\n 该写什么样的注释 好的注释记录你在写代码时的重要想法。\n写出言简意赅的注释 （1）保持紧凑\nNO  YES   （2）具名函数参数的注释\nNO - 让人困惑  YES   简化循环和逻辑 把控制流变得易读 （1）条件语句中参数的顺序\n if (length \u0026gt;= 10) 和 if (10 \u0026lt;= length) 第一个更易读 whiler (bytes_received \u0026lt; bytes_expected) 和 while(bytes_expected \u0026gt; bytes_received) 第一个更易读  通用的规则是什么？怎么决定 a \u0026lt; b 好还是 b \u0026gt; a 好？\n上述指导原则与英语的语法一致。\n（2）if/else 语句块的顺序\n 首先处理正逻辑，而非负逻辑。 先处理掉简单的情况。 先处理有趣或者可疑的情况。  （3）?: 条件表达式\n下述代码应该是用 if/else 拆分开：\nreturn exponent \u0026gt;= 0 ? mantissa * (1 \u0026lt;\u0026lt; exponent) : mantissa / (1 \u0026lt;\u0026lt; -exponent);  默认情况下都用 if/else，三目运算发 ?: 只有在最简单的情况下使用。\n （4）避免 do/while 循环\n 我的经验是，do 语句是错误和困惑的来源……我倾向于把条件放在“前面我能看到的地方”。其结果是，我倾向于避免使用 do 语句。\u0026mdash; C++ 开创者 Bjarne Stroustrup《C++ 程序设计语言》\n （5）从函数中提前返回\n从函数中提前返回，常常很受欢迎。\npublic boolean contains(String str, String substr) { if (str == null || substr == null) { return false; } if (substr.equals(\u0026#34;\u0026#34;)) { return true; } // do other things ... } （6）最小化嵌套\n每个嵌套层次都在读者的 “思维栈” 上又增加了一个条件。\n 提早返回来减少嵌套 在循环中，提早返回的技术是：continue  拆分超长的表达式 （1）用作解释的变量\nNO if line.split(\u0026#39;:\u0026#39;)[0].strip() == \u0026#39;root\u0026#39;   YES username = line.split(\u0026#39;:\u0026#39;)[0].strip() if username == \u0026#39;root\u0026#39;    （2）总结变量\nNO if (request.user.id == document.owner_id) { // user 可疑编辑文档 } if (request.user.id != document.owner_id) { // 文档只可读 }   YES final boolean user_own_document = (request.user.id == document.owner_id); if (oser_own_document) { // user 可以编辑文档 } if (!oser_own_document) { // 文档只可读 }    变量与可读性  让你的变量对尽量少的代码行可见 操作一个变量的地方越多，越难确定它的当前值。  重新组织代码 抽取不相关的子问题  纯工具函数抽取到一起，从项目中拆分出的独立库越多越好 永远都不要安于使用不理想的接口，你总是可以创建你自己的包装函数来隐藏接口的粗陋细节 不要太过：引入很多小函数对可读性是不利的，因为读者要关注更多东西。   把一般项目和项目专有的代码分开，其结果是，大部分代码都是一般代码，其余的是让你的程序与众不同的核心部分。\n 一次只做一件事  列出所有任务，一些任务变为单独的函数或类，另外一些可以称为一个函数中的逻辑段落。  把想法变成代码  用自然语言描述程序，然后用这个描述来帮助你写出更自然的代码。  NO  易读无反义(即时有空语句体)   少写代码  最好读的代码就是没有代码\n  从项目中消除不必要的功能，不要过度设计 重新考虑需求，解决版本最简单的问题，只要能完成工作就行 经常性地通读标准库的整个 API，保持对它们的熟悉程度  精度代码 测试与可读性  测试应当具有可读性，以便其他程序员可以舒服地改变或增加测试 对使用者隐去不重要的细节，以便更重要的细节会更突出 选择一组最简单的输入，它能完整地使用被测代码  扫描下面二维码，在手机端阅读：\n"});index.add({'id':110,'href':'/docs/programmer-interview/algorithm/container-with-most-water/','title':"Container With Most Water",'content':"Container With Most Water // 两个柱子中间包含最多的水 // 可以看一下这道题的这个图 // 这个是两个柱子之间的所能容纳的水的矩形面积 // // https://leetcode.com/problems/container-with-most-water/ // // [1,8,6,2,5,4,8,3,7] // ↑ ↑ // 7 * 7 = 49 // public class ContainerWithMostWater { public int maxArea(int[] height) { int maxArea = Integer.MIN_VALUE; int lo = 0; int hi = height.length - 1; // O(n)  while (lo \u0026lt; hi) { maxArea = Math.max(maxArea, Math.min(height[lo], height[hi]) * (hi - lo)); // =======================================  // 此处这个地方，必须是小的一边移动  // 因为大的移动的话，面积一定变小 (宽度变小，而且高度不会超过小的)  // 而小的移动有可能变大  //  // 另外一种解释：  // 我们选择一个高的，以便容纳更多的水  // https://leetcode.com/problems/container-with-most-water/discuss/6100/Simple-and-clear-proofexplanation  // =======================================  if (height[lo] \u0026lt; height[hi]) { lo++; } else { hi--; } } return maxArea; } } "});index.add({'id':111,'href':'/docs/tutorial/git/git-merge-branch/','title':"Git 分支合并",'content':"Git 分支合并 某个功能在开发分支上开发完毕后，需要合并到 master 分支，合并分支有两种方式：\n git merge git rebase  分支现状展示  从 master 分支的 A 提交点，拉取了分支 user2/i18n user2/i18n 的功能开发总共有两个 commit：E 和 F master 在 A 提交点之后，又有 B、C 和 D 这三个提交被合入进来  Git merge 我们使用 git merge 来合并 user2/i18n 分支到 master 分支上：\n# 切换到 master 分支 git checkout master # 合并 user2/i18n 分支 git merge user2/i18n 合并后的分支示意图：\nGit rebase 我们使用 git rebase 来合并 user2/i18n 到 master 分支上：\ngit checkout user2/i18n git rebase master # 如果有冲突，则需要解决冲突 # 解决完冲突，使用 git add -u 将完成冲突解决的文件加入到暂存区 # git rebase --continue # 直接推送，用本地的 user2/i18n 分支更新远程的 master 分支即可 git push origin user2/i18n:master rebase 之后的分支示例：\nmerge 和 rebase 对比  rebase 相比 merge 少了一次 merge 产生的提交，减轻了代码审核的负担 rebase 产生的分支关系图，比 merge 产生的分支关系图更为简单，更有顺序性  扫描下面二维码，在手机端阅读：\n"});index.add({'id':112,'href':'/docs/tutorial/eureka/','title':"Netflix Eureka 源码分析",'content':"Netflix Eureka 源码分析 "});index.add({'id':113,'href':'/docs/tutorial/network/udp/','title':"UDP",'content':"UDP UDP 头 与 TCP 区别  无连接 不保证不丢失、不保证按序到达 基于数据报，一个一个发，一个一个收 无拥塞控制，让我发，我就发，管它洪水滔天 无状态服务  应用场景  需要资源少，网络情况较好的内网，或者对于丢包不敏感的应用 不需要一对一沟通建立连接，可以广播的应用: DHCP 处理速度快、低时延、可以容忍少数丢包 Quick UDP Internet Connections: Google 提出的，降低通信时延 流媒体协议，有的帧比较重要，有的不重要 实时游戏。游戏玩家多，服务器却不多，而维护 TCP 需要一些数据结构 IoT 物联网 4G 网络，数据流量上网的数据面对的协议 GTP-U 就是基于 UDP 的  "});index.add({'id':114,'href':'/docs/tools/','title':"实用工具",'content':"阮一峰周刊提到的工具集 阮一峰《科技爱好者周刊》中列举的所有好用的工具集，持续收录，目前已经将前 28 期的所有实用的工具收录整理在下表中，方便小伙伴们使用。直接在本文 Ctrl(Command) + F 查找工具集的名字，即可快速搜索定位到你想找的工具。\n   类别 🍄 🍄     Resource 吴恩达《Machine Learning Yearning》 Google 面试自学手册    Android 开发工程师面试指南 《React in patterns》    《技术面试需掌握的基础知识整理》 各大互联网公司技术架构    收集开源书籍：love2.io Facebook 机器学习教程    AWK 编程语言 Python 100天从新手到大师    14000种鸟叫 网页设计常见错误    GO 高级教程 《Node.js 调试指南》    网站架构101 Google 数据集搜索    V8引擎官方网站    工具 火焰🔥图生成: flamebearer 合并多张图片    Tabler - Dashboard 浏览器自动化框架: Remote Browser    网页日历库 优化图片下载: img-2    开源图标库 PNG 图片压缩    AutoCAD 在线版 自动生成背景图片    各种 CSS 使用技巧 手绘风格组件库    HTML转PDF：ReLaXed 自动格式化 Python 代码    代理服务器：goproxy Terminal 显示 Dashboard    免费存储 JSON: jsonstore.io MAC 免费软件    生成 .gitignore 生成 localhost 证书    React 拖放库 开源代码片段管理服务    各种软件的 Cheatsheet 开源的在线图片编辑器    查找重复代码 命令行画出柱状图    设计原型产品 UI 开源短网址服务    命令行操作录制成SVG Crontab UI    APK安装在Ubuntu 任何网页转为 RSS    下载 Youtube 视频     "});index.add({'id':115,'href':'/docs/books/the-wisdom-of-trading-stocks/','title':"炒股的智慧",'content':"炒股的智慧  如果要我用一句话解释何以一般股民败多胜少，那就是：人性使然！说的全面些，就是这些永远不变的人性\u0026ndash;讨厌风险、急着发财、自以为是、赶潮跟风、因循守旧和耿于报复\u0026ndash;使股民难以逃避开股市的陷阱。说得简单些，就是好贪小便宜、吃不得小亏的心态使得一般股民几乎必然地成为了输家。\n 炒股的挑战 炒股与人性 炒股最重要的原则就是止损。但人性是好贪小便宜，不肯吃小亏，只有不断地因为贪了小便宜却失去大便宜，不肯吃小亏最终却吃了大亏，你才能最终学会不贪小便宜，不怕吃小亏。\n特殊的赌局 问问你自己喜欢做决定吗？喜欢独自为自己地决定负全部责任吗？对 99% 的人来说，答案是否定的。股市这一恒久的赌局却要求你每时每刻都要做理性的决定，并且为决定的结果负全部的责任！这就淘汰了一大部分股民，因为他们没有办法长期承受这样的心理能力。\n一般股民何以失败 人性讨厌风险 纽约有位叫做夏皮诺的心理医生，请了一批人来做两个实验：\n 实验一  选择：第一，75% 的机会得到 1000 美元，但有 25% 的机会什么都得不到；第二，确定得到 700 美元。结果 80% 的人选择了第二选择，大多数人宁愿少些，也要确定的利润。\n 股民好获小利，买进的股票升了一点，便迫不及待地脱手。这只股票或许有 75% 继续上升地机会，但为了避免 25% 什么都得不到的可能性，股民宁可少赚些。任何炒过股的读者都明白，要用较出场价更高地价位重新入场是多么困难。股价一天比一天高，你只能做旁观者。\n  实验二  选择：第一，75% 的机会付出 1000 美元，但有 25% 的机会什么都不付；第二，确定付出 700 美元。结果 75% 的人选择了第一选择。他们为了搏 25% 什么都不付的机会，从数学上讲多失去了 50 美元。\n 一旦买进的股票跌了，股民便死皮赖脸不肯止损，想象出各种各样的理由说服自己下跌只是暂时的。其真正的原因只不过为了搏那 25% 可能全身而退的机会！结果是小亏慢慢积累成大亏。\n 人的发财心太急 心一旦大了，行动上就开始缺少谨慎。首先我每次买股买得太多，其次止损止得太迟。我为此遭受了巨大的损失。\n人好自以为是 一天结束的时候，股票以某一价钱收盘。你有没有思考过它代表了什么？它代表了股市的参与者在今天收市的时候对该股票的认同。\n不要太固执己见，不要对自己的分析抱太大的信心。认真观察股市，不对时就认错。否则，你在这行生存的机会是不大的。\n人好跟风 人好报复 在股市中，买进的股票跌了，你就再多买一点，因为第二次买的价钱较上次为低，所以平均进价摊低了。从心理上看，你的心态和赌场亏钱时一样。一方面你亏不起，另一方面你在报复股市，报复股市让你亏钱。同时内心希望，只要赢一手，就能连本带利全回来。因为平均进价摊低了，股票的小反弹就能提供你全身而退的机会。\n这样的心态是极其有害的。股票跌的时候通常有它跌的理由，常常下跌的股票会越跌越低。这样被套牢，你就越陷越深，直到你心理无法承受的地步。一个致命的大亏损，常常就彻底淘汰了一位股民。\n人总是迟疑不决，心怀侥幸 每个炒股人都会经过这个过程。20元买好股票，定好18元止损，当股票跌到18元时，你有没有想想再等等？或许股票马上反弹！股票又跌到16元，你会不会拍自己的脑壳说：“真该按定好的规矩办！18元时就走人；现若股票反弹5毛钱我就一定说再见。”\n现股票跌到10元了，你准备怎么办？你会发毛吗？你会不会发狠：“老子这次拼了！现在就是不走，我倒要看看你最低会跌到什么地方？”\n当然，最后的结局很少例外，通常是股票学校又多了位交了学费毕不了业的炒股\n股票分析的基本知识 技术分析 升势：一浪比一浪高     朋友，你认为什么因素使投资者入市买股票？华尔街有过调查，使一般投资者入场买股票的原因最主要的就是因为股票在升！你明白吗？一般投资者入场买股票主要不是因为股票的成本收益比率低或红利高，而是因为股票在升！升！升！而投资人卖股票的最主要原因是因为股票在跌！在跌！\n双肩图：\n头肩图：\n 股市操纵可能改变每天的或短期的波动，但不可能改变大势。\n 成功的要素 几乎所有的行家，他们对炒股的首要建议便是尽量保住你的本金。要做到保本的办法只有两个：一是快速止损；二是别一次下注太多。\n什么是你对风险的承受力？最简单的办法就是问自己睡得好吗？\n专业炒手明白股票买卖不可能每次都正确，那么在错误的时候为何要付大的代价？但在在他们正确的时候，他们试图从中得到最大的利润。\n只要股票运动正常（是否正常的判定，请参考上一章节），便必须按兵不动。这是很难的一件事，你要克服对脱手获利的冲动。如果你确定股票运动正常，你的胜算很大，这时你应该在这只股票商适当加大下注的比重。\n在股市不断赚钱的秘诀：通过你自己的观察和研究，不断累技经验，将自己每次入场获胜的概率从 50% 提高到 60%，甚至 70%；而且每次进场不要下注太大，应只是本金的小部分。这样长期以来，你就能 ”久赌必赢“。\n股票走升势的正常运动  股票的危险信号   炒股的最基本信条是在任何时候，你手上持有股票的上升潜力必须大过下跌的可能，否则你就不应留在手里。看到危险信号，表示你的获胜概率此时已不超过 50%。\n何时买股票，何时卖股票 选买点最最重要点时选择止损点。在你进场之前，你必须很清楚若股票的运动与你的预期不合，你必须在何点止损离场。你在投资做生意，不要老是想你要赚多少钱，首先应该清除自己亏得起多少。这个止损点不应超出投资额的 20%。\n何时买股票 阻力线和支撑线：\n    你选择哪一只股票？\n突破阻力线之前的蓄劲期左是 3 个月，右是半年，应该买半年的：\n何时卖股票 选择卖点 大家应记住自己在做声音，就如同做服装生意一样，一有合理的利润，就可以卖出去。一个大走势，头和尾都是很难抓住的，炒手们应学习怎样抓中间的一截，能抓到波幅的 70% 就算是很好的成绩了。\n要决定何时卖股票，最简单的方法就是问自己：我愿此时买进这只股票吗？如果答案是否定的，你就可以考虑卖掉这只股票。\n总结：\n 注意危险信号 保本第一 亏小钱 遇有暴力，拿了再说。(两星期股票从 20 升到 40) 第一天转头（转头表示收市低于开市）的时候你就可以把股票卖掉。别期待好事情会没完没了。这样的暴升是股价短期到顶的信号，特别是最后两天，交易量猛增，公司并没有特别的好消息。 小心交易量猛增，股价却不升。这也是危险信号，它告诉你有人乘这个机会在出货。它通常是股票到顶的信号，起码短期内如此。 用移动止损来卖股票。止损点放在每个波浪的波谷，随着波浪往上翻，你将卖点由 A-\u0026gt;B-\u0026gt;C-\u0026gt;D-\u0026gt;E 往上移。这样就能保证你不会在升势时过早离场。同时这样做很简单，情绪上的波动很小。这是一般不能全时专职炒股人的最常用方法。它提供了一个原则，遵照这一原则，你不会情绪化地过早离场，导致一个 10000 元的赚钱机会只赚到 2000 元。   华尔街将炒股的诀窍归纳成两句话：截短亏损，让利润奔跑！英文叫 Cut loss short，let profit run！意思是一见股票情况不对，即刻止损，把它缩得越短越好！一旦有了利润，就必须让利润奔跑，从小利润跑成大利润。\n     华尔街的家训  止损！止损！止损！（炒股行的最高行为准则） 分散风险。（你有 10 次好运，第 11 次好运不见得会落在你头上） 避免买太多种股票 有疑问的时候，离场！ 忘掉你的入场价。（股票跌了，还留在手中干什么？这和你在什么价位进价有什么关系？） 别频繁交易。（频繁交易常常是因为枯燥无聊） 不要向下摊平。（第一次入场后，纸面上没有利润的话不要加码） 别让利润变成亏损。（你给股票 10% 左右的喘息空间，一只正常上升的股票，不会轻易跌 10% 的） 跟着股市走，别跟朋友走！ 该卖股票的时候，要当机立断，千万别迟疑！ 别把 “股价很低了” 当成买的理由，也别将 “股价很高了” 当成卖的理由！（别试着去接往下掉的刀子，它会把你的手扎的血淋淋的！） 定好计划，按既定方针办 别爱上任何股票 市场从来不会错，你自己的想法常常是错的   我赚到大钱的诀窍不在于我怎么思考，而在于我能安坐不动，坐着不动！明白吗？\u0026ndash; 杰西·利物莫\n 从有招迈向无招 抓住大机会  经济史是一部基于假象和谎言的连续剧，经济史的演绎从不基于真实的剧本，但它铺平了累积巨额财富的道路。做法就是认清其假象，投入其中，在假象被公众认识之前退出游戏。\u0026ndash; 索罗斯\n 和炒手们谈谈天 "});index.add({'id':116,'href':'/docs/programmer-interview/algorithm/countofsmallerafterself/','title':"Count Of Smaller After Self",'content':"Count Of Smaller After Self 题目 数组里面的每一个数字，排在这个数字后面的小于这个数字的有多少个数字\n解法 import java.util.ArrayList; import java.util.List; // Input: [5,2,6,1] // Output: [2,1,1,0] // // 统计小于自己的有多少个数字 public class CountOfSmallerAfterSelf { public List\u0026lt;Integer\u0026gt; countSmaller(int[] nums) { List\u0026lt;Integer\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { int curr = nums[i]; int count = 0; for (int j = i + 1; j \u0026lt; nums.length; j++) { if (nums[j] \u0026lt; curr) { count++; } } result.add(count); } return result; } } "});index.add({'id':117,'href':'/docs/tutorial/git/git-fix-conflict/','title':"Git 解决冲突",'content':"Git 解决冲突 某个分支的代码想要合并到其它分支，可能会产生冲突，产生的原因就是这两个分支都对代码的同一个区域做了修改，Git 本身并不知道应该采用哪个修改最为合适，因此需要你来决定。\n解决冲突 如下所示是冲突代码的示例：\n A 和 B 之间的代码，是你本地的代码所做的改动 B 和 C 之间的代码，是远程代码所做的改动  你的工作是重新编辑 A 到 C 区域之间的内容，去掉 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; 、=======、\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 符号，重新编辑 A 和 C 之间的代码，以让整个项目运行起来。\n编辑完之后，可以通过 git add 命令将冲突的文件假如到暂存区，然后再 git commit，就完成了冲突解决。\n打开图形界面工具解决冲突 使用图形化工具来帮助你解决冲突，不过需要事先安装工具。打开图形界面工具的命令如下：\ngit mergetool 打开之前，也可以使用 git config 进行简单的配置，比如使用 vimdiff 工具作为默认的冲突解决工具：\ngit config merge.tool vimdiff git config merge.conflictstyle diff3 git config mergetool.prompt false 放弃合并操作 你暂时不想解决冲突：\ngit reset 参考  How to resolve merge conflicts in Git  扫描下面二维码，在手机端阅读：\n"});index.add({'id':118,'href':'/docs/tutorial/distributed-storage/','title':"分布式存储",'content':"分布式存储 "});index.add({'id':119,'href':'/docs/books/ddia/','title':"设计数据密集型应用程序",'content':"设计数据密集型应用程序  设计数据密集型应用程序 - 可靠 \u0026amp; 可扩展 \u0026amp; 可维护 设计数据密集型应用程序 - 数据模型 \u0026amp; 查询语言 设计数据密集型应用程序 - 存储和读取 设计数据密集型应用程序 - 编码与演化 设计数据密集型应用程序 - Replication 设计数据密集型应用程序 - Partitioning 设计数据密集型应用程序 - 事务 设计数据密集型应用程序 - 分布式系统的难点  "});index.add({'id':120,'href':'/docs/tutorial/git/git-tag/','title':"Git tag",'content':"Git tag 什么是 Tags Tag 是对某次 commit 的一个有意义的命名，比如某个重大的版本发布，某个重大的 BUG 修复等。如下展示了前端开发框架 React 在开发过程中标记的各个版本的 Tag 列表。\n显示版本库的 Tag 列表 git tag 创建 Tag # 在最新的提交是创建一个 Tag git tag myTag # 创建一个带有说明信息的 Tag git tag -m \u0026#34;My fir st annotated tag.\u0026#34; myTag2 删除 Tag git tag -d myTag 重命名 Tag 只能先删除旧的 Tag，然后创建一个新的\n将 Tag 推送到远程服务器 # 将 myTag 推送到远程服务器 git push origin myTag # 将本地所有 Tag 推送到远程服务器 git push origin refs/tags/* # 或 git push origin --tags 扫描下面二维码，在手机端阅读：\n"});index.add({'id':121,'href':'/docs/programmer-interview/algorithm/designcircularqueue/','title':"设计循环队列",'content':"Design Circular Queue 题目 设计循环队列\n解法 public class DesignCircularQueue { static class MyCircularQueue { private int[] queue; private int frontIndex; private int rearIndex; private int size; /** Initialize your data structure here. Set the size of the queue to be k. */ public MyCircularQueue(int k) { this.size = k + 1; this.queue = new int[k + 1]; } /** Insert an element into the circular queue. Return true if the operation is successful. */ public boolean enQueue(int value) { if (isFull()) { return false; } queue[rearIndex] = value; rearIndex = (rearIndex + 1) % size; return true; } /** Delete an element from the circular queue. Return true if the operation is successful. */ public boolean deQueue() { if (isEmpty()) { return false; } frontIndex = (frontIndex + 1) % size; return true; } /** Get the front item from the queue. */ public int Front() { if (isEmpty()) { return -1; } return queue[frontIndex]; } /** Get the last item from the queue. */ public int Rear() { if (isEmpty()) { return -1; } return rearIndex == 0 ? queue[size - 1] : queue[rearIndex - 1]; } /** Checks whether the circular queue is empty or not. */ public boolean isEmpty() { return frontIndex == rearIndex; } /** Checks whether the circular queue is full or not. */ public boolean isFull() { return (rearIndex + 1) % size == frontIndex; } } } "});index.add({'id':122,'href':'/docs/tutorial/git/git-add-and-rm/','title':"Git add 和 Git rm",'content':"Git add 和 Git rm  git add 用来从工作区向暂存区添加文件 git rm 用来从工作区向暂存区删除文件  git add 示例 git add [--all|-A] git add . git add -u Git 1.X 版本  假设 . 当前指向的目录是 .git 文件所在的目录\n Git 2.X 版本  假设 . 当前指向的目录是 .git 文件所在的目录\n git rm 示例 # 只从工作区删除文件 rm xxx.txt # 只从暂存区删除文件 git rm --cached # 从工作区和暂存区都删除这个文件 git rm xxx.txt # 递归强制删除 xxx_folder 中的所有文件 # -r: recursive # -f: override the up-to-date check git rm -rf xxx_folder 参考  Difference between “git add -A” and “git add .”  扫描下面二维码，在手机端阅读：\n"});index.add({'id':123,'href':'/docs/programmer-interview/algorithm/findfirstandlastpositionofelementinsortedarray/','title':"有序数组查找最小和最大元素的位置",'content':"有序数组查找最小和最大元素的位置 public class FindFirstandLastPositionofElementinSortedArray { public int[] searchRange(int[] nums, int target) { int minIndex = searchMinIndex(nums, target); if (minIndex == -1) { return new int[]{ -1, -1 }; } int maxIndex = searchMaxIndex(nums, target); return new int[] { minIndex, maxIndex }; } private int searchMaxIndex(int[] nums, int target) { int lo = 0; int hi = nums.length - 1; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] \u0026gt; target) { hi = m - 1; } else if (nums[m] \u0026lt; target) { lo = m + 1; } else { if (m == nums.length - 1 || nums[m + 1] \u0026gt; target) { return m; } lo = m + 1; } } return -1; } private int searchMinIndex(int[] nums, int target) { int lo = 0; int hi = nums.length - 1; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] \u0026gt; target) { hi = m - 1; } else if (nums[m] \u0026lt; target) { lo = m + 1; } else { if (m == 0 || nums[m - 1] \u0026lt; target) { return m; } hi = m - 1; } } return -1; } } "});index.add({'id':124,'href':'/docs/tutorial/git/git-push-and-pull/','title':"Git push 和 Git pull",'content':"Git push 和 Git pull git push 、git pull 用于向远程分支推送文件，以及从远程分支拉取文件等。\n远程版本库地址 .git/config 文件中记录了当前仓库远程版本库的地址：\nvi .git/config 直接修改这个地址保存后，当前版本库的远程版本库的地址也就变化了。Git 本身也提供了用来操纵版本库地址的命令：\n# 添加远程版本库地址 git remote add origin git@github.com:facebook/react.git # 更新远程版本库地址 git remote set-url origin git@github.com:facebook/react.git push 和 pull push 命令和 pull 命令的语法相似：\ngit push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; git pull \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; 不带参数，执行命令 git push 的过程（git pull 同理）：\n 如果当前分支有 remote（如何知道是否有 remote？还是看 .git/config 文件，如下图所示，每个 branch 的 remote 都不是空的），那么 git push 相当于执行了 git push \u0026lt;remote\u0026gt; 如果没有设置，则相当于执行 git push origin  一般而言，你这个项目本身应该只有一个版本库地址，如下图所示，版本库的名称就叫做 origin，它的地址就是 url 后面的那一部分：\n一般情况下，在日常开发过程中，直接使用不带参数的 git push 和 git pull 命令就可以完成当前分支文件的推送和拉取操作。\ngit push -u 部分程序员在 git push 的时候，会加上 -u 这个参数，这个主要目的是为了能够使用不带参数的 git pull 命令，更方便地拉取分支代码。其相当于是下面两条命令的结合（master 也可以换成其它分支名字，此处仅仅是一个例子）：\ngit push -u origin master git push origin master ; git branch --set-upstream master origin/master 假如我们本地新建了一个分支，git push 不带 -u，你 git pull 的时候，git 会提示你不能直接这样用：\n$ git checkout -b test $ git push origin test $ git pull You asked me to pull without telling me which branch you want to merge with, and \u0026#39;branch.test.merge\u0026#39; in your configuration file does not tell me, either. Please specify which branch you want to use on the command line and try again (e.g. \u0026#39;git pull \u0026lt;repository\u0026gt; \u0026lt;refspec\u0026gt;\u0026#39;). See git-pull(1) for details. If you often merge with the same branch, you may want to use something like the following in your configuration file: [branch \u0026#34;test\u0026#34;] remote = \u0026lt;nickname\u0026gt; merge = \u0026lt;remote-ref\u0026gt; [remote \u0026#34;\u0026lt;nickname\u0026gt;\u0026#34;] url = \u0026lt;url\u0026gt; fetch = \u0026lt;refspec\u0026gt; See git-config(1) for details. 假设 git push 的时候，带上了 -u 这个参数：\n$ git push -u origin test Branch test set up to track remote branch test from origin. Everything up-to-date $ git pull Already up-to-date. fetch git fetch 也可以拉取远程分支的文件，它和 git pull 的不同之处在于，它不会自动执行 git merge。\ngit pull = git fetch + git merge\n参考  What is the difference between \u0026lsquo;git pull\u0026rsquo; and \u0026lsquo;git fetch\u0026rsquo;? What exactly does the “u” do? “git push -u origin master” vs “git push origin master”  扫描下面二维码，在手机端阅读：\n"});index.add({'id':125,'href':'/docs/programmer-interview/algorithm/findmedianfromdatastream/','title':"数据流寻找中位数",'content':"数据流寻找中位数 import java.util.Comparator; import java.util.PriorityQueue; // https://leetcode.com/problems/find-median-from-data-stream/ // 剑指 Offer 41 题 // public class FindMedianfromDataStream { // 堆顶是最小的  // 最小堆的堆顶是最大值  //  private PriorityQueue\u0026lt;Integer\u0026gt; minQueue = new PriorityQueue\u0026lt;\u0026gt;(); // 堆顶是最大的  // 最大堆的堆顶是最小的值  private PriorityQueue\u0026lt;Integer\u0026gt; maxQueue = new PriorityQueue\u0026lt;\u0026gt;(new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); private boolean sameSize = true; public FindMedianfromDataStream() { } public void addNum(int num) { if (sameSize) { minQueue.offer(num); maxQueue.offer(minQueue.poll()); } else { maxQueue.offer(num); minQueue.offer(maxQueue.poll()); } sameSize = !sameSize; } public double findMedian() { if (sameSize) { return (minQueue.peek() + maxQueue.peek()) / 2.0; } return maxQueue.peek(); } } "});index.add({'id':126,'href':'/docs/tutorial/git/git-commit/','title':"Git commit",'content':"Git commit git commit 可以将暂存区的文件，commit 提交到本地仓库中。\ngit commit -m -m 代表 message 信息的意思。git commit 需要一个信息作为它的参数，这个信息是对此次 commit 的简短描述，消息应该放到双引号里面。\ngit commit -m \u0026#34;my brief description about commit\u0026#34;  如果没有携带 -m 参数，Git 也会弹出编辑器让你输入消息的。\n git commit -a -a 选项代表 all，即所有。该选项可以将本地工作区所有改动的/被删除的文件，直接 commit 到仓库中，而无需调用 git add/rm 命令手动添加或删除。\ngit commit -am \u0026#34;My message\u0026#34;  -a 并不会将新添加的文件 commit 到版本库中。\n git commit \u0026ndash;amend --amend 选项可以让你修改上一次提交的信息。\n# 第一次提交信息 git commit -m \u0026#34;my first message\u0026#34; # 你对 my first message 这个描述不满意 # 所以使用下面命令来修正成你想要的信息 git commit --amend -m \u0026#34;an updated commit message\u0026#34; 参考  Git Commit Command Explained  扫描下面二维码，在手机端阅读：\n"});index.add({'id':127,'href':'/docs/programmer-interview/algorithm/findminimuminrotatedsortedarray/','title':"旋转有序数组中寻找最小数字",'content':"旋转有序数组中寻找最小数字 // 没有重复元素 // 1 2 3 4 5 6 7 // // 5 6 7 1 2 3 4 // lo hi // public class FindMinimuminRotatedSortedArray { public int findMin(int[] nums) { int lo = 0; // always point to 前半部分  int hi = nums.length - 1; // always point to 后半部分  if (nums[lo] \u0026gt; nums[hi]) { while (lo \u0026lt; hi) { if (hi - lo == 1) { return nums[hi]; } int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] \u0026lt; nums[0]) { // middle 位于后半部分  hi = m; } else if (nums[m] \u0026gt; nums[nums.length - 1]) { // middle 位于前半部分  // min 在后半部分  //  // ==================================  // 注意这个地方， lo = m，而不是 lo = m + 1  // m + 1 有可能越界，跑到后半部分  // ==================================  lo = m; } } } return nums[lo]; } } "});index.add({'id':128,'href':'/docs/programmer-interview/algorithm/findpeakelement/','title':"寻找峰值元素",'content':"寻找峰值元素 题目 你给出一个整数数组(size为n)，其具有以下特点：\n 相邻位置的数字是不同的 A[0] \u0026lt; A[1] 并且 A[n - 2] \u0026gt; A[n - 1]  假定P是峰值的位置则满足A[P] \u0026gt; A[P-1]且A[P] \u0026gt; A[P+1]，返回数组中任意一个峰值的位置。\n 数组保证至少存在一个峰 如果数组存在多个峰，返回其中任意一个就行 数组至少包含 3 个数   微软面试题\n 解法 // https://www.lintcode.com/problem/find-peak-element/description // // Microsoft // A[P] \u0026gt; A[P-1] \u0026amp;\u0026amp; A[P] \u0026gt; A[P+1] public class FindPeakElement { // 返回的是索引  //  // 数组太大的话，会超时  public int findPeak(int[] A) { // [x,x,x,x,x]  int lo = 1; int hi = A.length - 2; while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (A[mid] \u0026gt; A[mid - 1] \u0026amp;\u0026amp; A[mid] \u0026gt; A[mid + 1]) { return mid; } else if (A[mid] \u0026lt;= A[mid + 1]) { lo = mid + 1; } else if (A[mid] \u0026lt;= A[mid - 1]) { hi = mid - 1; } } return -1; } } "});index.add({'id':129,'href':'/docs/programmer-interview/algorithm/findtheduplicatenumber/','title':"寻找重复数字",'content':"寻找重复数字 // Given an array nums containing n + 1 integers where each integer is between 1 and n (inclusive), // prove that at least one duplicate number must exist. // Assume that there is only one duplicate number, find the duplicate one. // // 1 到 n 的数字，某个数字重复，可能重复次数 \u0026gt; 1 // // Input: [3,1,3,4,2] // Output: 3 // // https://leetcode.com/problems/find-the-duplicate-number/ public class FindtheDuplicateNumber { // ==================================  // 数组不允许修改版本  //  // 剑指 Offer  // ==================================  public int findDuplicate(int[] nums) { int lo = 1; int hi = nums.length - 1; // 数组长 n + 1，但是最大可能值为 n  while (lo \u0026lt;= hi) { // 1 2 3 4 5 6 7 8  // ↑  int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); // 统计 nums 里面位于 1 和 4 中间的数字有多少个  int count = countRange(nums, lo /** 1 */, m/** 4 */); int expectedCount = m - lo + 1; if (expectedCount == 1 /**期望 1 个 */ \u0026amp;\u0026amp; count \u0026gt; 1) { return m; // m == lo, 返回哪个都可以  } if (count \u0026gt; expectedCount) { hi = m; } else { lo = m + 1; } } return -1; } private int countRange(int[] nums, int lo, int hi) { int count = 0; for (int i = 0; i \u0026lt; nums.length; i++) { if (nums[i] \u0026gt;= lo \u0026amp;\u0026amp; nums[i] \u0026lt;= hi) { count++; } } return count; } // ==================================  // 数组允许修改版本  // ==================================  public int findDuplicate0(int[] nums) { for (int i = 0; i \u0026lt; nums.length; i++) { int expectedNum = i + 1; while (nums[i] != expectedNum) { if (nums[i] == nums[nums[i] - 1]) { return nums[i]; } swap(nums, i, nums[i] - 1); } } return -1; } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } } "});index.add({'id':130,'href':'/docs/programmer-interview/algorithm/firstmissingpositive/','title':"第一个缺失的最小正数",'content':"第一个缺失的最小正数 import java.util.ArrayList; import java.util.Collections; import java.util.HashSet; import java.util.List; import java.util.Set; // https://leetcode.com/problems/first-missing-positive/ // Given an unsorted integer array, find the smallest missing positive integer. // // Input: [3,4,-1,1] // Output: 2 // // Input: [7,8,9,11,12] // Output: 1 // public class FirstMissingPositive { // ===============================  // 不使用辅助空间  //  // 最核心的是，遇见哪些数字可以不用管:  // - 负数  // - 大于 nums.length 的数  // ===============================  public int firstMissingPositive(int[] nums) { int i = 0; while (i \u0026lt; nums.length) { int expected = i + 1; if (nums[i] == expected || nums[i] \u0026lt;= 0 || nums[i] \u0026gt; nums.length) { i++; continue; } // 将 nums[i] 放到正确的位置  //  // 1 5 3 4 4  // ↑ (5 应该放到 5 - 1 的位置)  int rightPos = nums[i] - 1; if (nums[rightPos] != nums[i]) { // i 指针没有移动  swap(nums, i, rightPos); continue; } // 如果正确的位置上已经有一个了  // 那么我们试探下一个就行了  i++; } i = 0; while (i \u0026lt; nums.length \u0026amp;\u0026amp; nums[i] == i + 1) { i++; } return i + 1; } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } // ===============================  // 使用了辅助空间 O(n)  // ===============================  public int firstMissingPositive0(int[] nums) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int num: nums) { if (num \u0026gt; 0) { if (set.add(num)) { list.add(num); } } } Collections.sort(list); if (list.isEmpty() || list.get(0).intValue() != 1) { return 1; } for (int i = 0; i \u0026lt; list.size(); i++) { int expected = i + 1; if (list.get(i).intValue() != expected) { return list.get(i - 1).intValue() + 1; } } return list.get(list.size() - 1) + 1; } } "});index.add({'id':131,'href':'/docs/programmer-interview/algorithm/firstuniquenumberindatastream/','title':"数据流的第一个唯一数字",'content':"数据流的第一个唯一数字 import java.util.HashMap; import java.util.LinkedHashMap; import java.util.Map; // https://www.lintcode.com/problem/first-unique-number-in-data-stream/description // // 给一个连续的数据流,写一个函数返回终止数字到达时的第一个唯一数字（包括终止数字）, // 如果在终止数字前无唯一数字或者找不到这个终止数字, 返回 -1. public class FirstUniqueNumberinDataStream { public int firstUniqueNumber(int[] nums, int number) { boolean hasStopNumber = false; HashMap\u0026lt;Integer, Integer\u0026gt; map = new LinkedHashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { int count= map.getOrDefault(nums[i], 0); map.put(nums[i], count + 1); if (nums[i] == number) { hasStopNumber = true; break; } } if (!hasStopNumber) { return -1; } for (Map.Entry\u0026lt;Integer, Integer\u0026gt; entry: map.entrySet()) { if (entry.getValue() == 1) { return entry.getKey().intValue(); } } return -1; } } "});index.add({'id':132,'href':'/docs/programmer-interview/algorithm/insertinterval/','title':"插入区间",'content':"插入区间 import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; import com.zk.algorithm.beans.Interval; // Input: intervals = [[1,2],[3,5],[6,7],[8,10],[12,16]], newInterval = [4,8] // Output: [[1,2],[3,10],[12,16]] // Explanation: Because the new interval [4,8] overlaps with [3,5],[6,7],[8,10]. public class InsertInterval { // 插入一个新的 interval  // 如果有交集，那么合并  public List\u0026lt;Interval\u0026gt; insert(List\u0026lt;Interval\u0026gt; intervals, Interval newInterval) { intervals.add(newInterval); Collections.sort(intervals, new Comparator\u0026lt;Interval\u0026gt;() { public int compare(Interval a, Interval b) { if (a.start \u0026lt; b.start) { return -1; } else if (a.start \u0026gt; b.start) { return 1; } return 0; } }); List\u0026lt;Interval\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); Interval prev = null; for (Interval item: intervals) { if (prev == null || item.start \u0026gt; prev.end) { res.add(item); prev = item; } else if (item.start \u0026lt;= prev.end) { // 这种情况是 OK 的  //  // |______________|  // |_______________|  //  // 但是如果是下面这种情况呢  //  // |______________|  // |___|  prev.end = Math.max(item.end, prev.end); } } return res; } } "});index.add({'id':133,'href':'/docs/programmer-interview/algorithm/intersectionoftwoarrays/','title':"数组交集",'content':"数组交集 方法一 // // Input: nums1 = [4,9,5], nums2 = [9,4,9,8,4] // Output: [9,4] // public class IntersectionofTwoArrays { public int[] intersection(int[] nums1, int[] nums2) { Set\u0026lt;Integer\u0026gt; set1 = toSet(nums1); Set\u0026lt;Integer\u0026gt; set2 = toSet(nums2); return findIntersection(set1, set2); } private int[] findIntersection(Set\u0026lt;Integer\u0026gt; set1, Set\u0026lt;Integer\u0026gt; set2) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int num: set2) { if (set1.contains(num)) { list.add(num); } } int[] res = new int[list.size()]; for (int i = 0; i \u0026lt; list.size(); i++) { res[i] = list.get(i); } return res; } private Set\u0026lt;Integer\u0026gt; toSet(int[] nums) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int num: nums) { set.add(num); } return set; } } 方法二 public class IntersectionofTwoArrays_Solution_1 { public int[] intersection(int[] nums1, int[] nums2) { Arrays.sort(nums1); Arrays.sort(nums2); List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); int i1 = 0, i2 = 0; while (i1 \u0026lt; nums1.length \u0026amp;\u0026amp; i2 \u0026lt; nums2.length) { if (nums1[i1] == nums2[i2]) { int num = nums1[i1]; list.add(num); while (i1 \u0026lt; nums1.length \u0026amp;\u0026amp; nums1[i1] == num) { i1++; } while (i2 \u0026lt; nums2.length \u0026amp;\u0026amp; nums2[i2] == num) { i2++; } } else if (nums1[i1] \u0026lt; nums2[i2]) { i1++; } else { i2++; } } return toIntArray(list); } private int[] toIntArray(List\u0026lt;Integer\u0026gt; list) { int[] arr = new int[list.size()]; for (int i = 0; i \u0026lt; list.size(); i++) { arr[i] = list.get(i); } return arr; } } "});index.add({'id':134,'href':'/docs/programmer-interview/algorithm/ksmallestnuminanarray/','title':"最小的K个数",'content':"最小的K个数 import java.util.ArrayList; // 牛客网 // https://www.nowcoder.com/practice/6a296eb82cf844ca8539b57c23e6e9bf // 最小的 k 个数 // public class KSmallestNumInAnArray { public ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution(int[] input, int k) { int lo = 0; int hi = input.length - 1; while (lo \u0026lt;= hi) { int kth = partition(input, lo, hi); if (kth == k - 1) { ArrayList\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; k; i++) { res.add(input[i]); } return res; } else if (kth \u0026lt; k - 1) { lo = kth + 1; } else { hi = kth - 1; } } return new ArrayList\u0026lt;Integer\u0026gt;(); } private int partition(int[] nums, int lo, int hi) { int left = lo - 1; int pivot = nums[hi]; for (int i = lo; i \u0026lt;= hi - 1; i++) { if (nums[i] \u0026lt;= pivot) { swap(nums, ++left, i); } } swap(nums, left + 1, hi); return left + 1; } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } } "});index.add({'id':135,'href':'/docs/programmer-interview/algorithm/kdiffpairsinanarray/','title':"绝对值差为K的数对数量",'content':"绝对值差为K的数对数量 描述 给定一个整数数组和一个整数k，您需要找到数组中唯一k-diff对的数量。这里k-diff对被定义为整数对(i, j)，其中i和j都是数组中的数字，它们的绝对差是k。\n 对(i,j)和(j,i)计为同一对。 数组的长度不超过10,000。 给定输入中的所有整数都属于以下范围：[ -1e7, 1e7]。  答案 import java.util.Arrays; // https://www.lintcode.com/problem/k-diff-pairs-in-an-array/description // Amazon // // 这个 pair 差的绝对值 == k public class KdiffPairsinanArray { // O(n^2)  public int findPairs(int[] nums, int k) { Arrays.sort(nums); int count = 0; for (int i = 0; i \u0026lt; nums.length - 1; i++) { if (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) { continue; } for (int j = i + 1; j \u0026lt; nums.length; j++) { if (j \u0026gt; i + 1 \u0026amp;\u0026amp; nums[j] == nums[j - 1]) { continue; } int diff = Math.abs(nums[j] - nums[i]); if (diff \u0026gt; k) { break; } else if (diff == k) { count++; } } } return count; } } "});index.add({'id':136,'href':'/docs/programmer-interview/algorithm/kthlargestelement/','title':"第 K 个最大的数字",'content':"第 K 个最大的数字 数组中的第 K 个最大数字 // 数组无序 public class KthLargestElementinanArray { public int findKthLargest(int[] nums, int k) { k = nums.length - k; // 1 2 3 4 5 6  // ↑(第 2 大)  // ↑(partition = 2 的时候，实际上指向的是这里)  int lo = 0; int hi = nums.length - 1; // ==========================  // while (lo \u0026lt; hi)  // nums = [1]，这种情况进入不了循环  //  // ==========================  while (lo \u0026lt;= hi) { int index = partition(nums, lo, hi); if (index == k) { return nums[index]; } else if (index \u0026lt; k) { lo = index + 1; } else { hi = index - 1; } } return -1; } private int partition(int[] nums, int lo, int high) { int i = lo - 1; int pivot = nums[high]; for (int j = lo; j \u0026lt;= high - 1; j++) { if (nums[j] \u0026lt;= pivot) { i++; swap(nums, i, j); } } swap(nums, i + 1, high); return i + 1; } private void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; } } 数据流中的第 K 个最大数字  堆  import java.util.Comparator; import java.util.PriorityQueue; // 1 2 3 4 5 6 7 8 9 10 // ↑ // 第 3 大 (维护 size() = k 的小项堆，大于 k 个就移除最小的) public class KthLargestElementinaStream { private PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;Integer\u0026gt;(new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o1.compareTo(o2); } }); private int k; public KthLargestElementinaStream(int k, int[] nums) { this.k = k; for (int i = 0; i \u0026lt; nums.length; i++) { queue.offer(nums[i]); if (queue.size() \u0026gt; k) { queue.poll(); } } } public int add(int val) { queue.offer(val); if (queue.size() \u0026gt; this.k) { queue.poll(); } return queue.peek(); } } "});index.add({'id':137,'href':'/docs/programmer-interview/algorithm/kthsmallestelement/','title':"第 K 个最小的数字",'content':"第 K 个最小的数字 9 * 9 乘法表中的第 K 个最小数字 // http://exercise.acmcoder.com/online/online_judge_ques?ques_id=3819\u0026amp;konwledgeId=40 // https://leetcode.com/problems/kth-smallest-number-in-multiplication-table/ // // 百度乘法表 // 9 * 9 乘法表 // public class KthSmallestNumberinMultiplicationTable { public int findKthNumber(int m, int n, int k) { int lo = 1; int hi = m * n; while (lo \u0026lt;= hi) { int middle = lo + ((hi - lo) \u0026gt;\u0026gt; 1); int count = countLessOrEqualK(m, n, middle); if (count \u0026lt; k) { lo = middle + 1; } else { hi = middle - 1; } } return lo; } private int countLessOrEqualK(int m, int n, int k) { int c = 0; for (int i = 1; i \u0026lt;= m; i++) { // 1 2 3 4  // 2 4 6 8  // 3 6 9 12  //  // k = 5  // - k \u0026gt;= 第 1 行的 (1 2 3 4) 最后的 4，所以 c += n 个  // - k \u0026lt; 第 2 行的 (2 4 6 8) 最后的 8，所以 c += k / i 个  //  if (k \u0026gt;= n * i) { c += n; } else { c += k / i; } } return c; } } 行或列均有序的矩阵中的第 K 个最小数字 // matrix = [ // [ 1, 5, 9], // [10, 11, 13], // [12, 13, 15] // ], // k = 8,  // return 13. // // 每行、每列都是有序的 public class KthSmallestElementinaSortedMatrix { public int kthSmallest(int[][] matrix, int k) { int lo = matrix[0][0]; int hi = matrix[matrix.length - 1][matrix[0].length - 1]; while (lo \u0026lt;= hi) { int middle = lo + ((hi - lo) \u0026gt;\u0026gt; 1); //  // 小于或者等于 middle 的是 count 个  int count = countLessOrEqual(matrix, middle); if (count \u0026lt; k) { lo = middle + 1; } else if (count \u0026gt; k) { hi = middle - 1; } else if (count == k) { // 1 2 3 4 5 6 7 7 12  // ↑ (mid 是 12， 小于 mid 的是 12 个，尝试降低 mid 的值，然后再去寻找)  // (我们必须保证 mid 是在这个 matrix 里面)  //  // ↑ (我们最终要找的是 7，是位于 mid 左边的数字)  //  // 1 2 3 4 5 6 7 7 7 7 7 7 12  // ↑ ↑  // lo hi  //  // 当 lo 和 hi 相同的时候, 挺难理解的，不好证明，难道是夹逼定理 ?  // lo \u0026lt;= kth \u0026lt; mid \u0026lt;= hi ?  hi = middle - 1; } } return lo; // or hi 因为现在 lo == hi;  } private int countLessOrEqual(int[][] matrix, int target) { // 最后一行的第一个数  int row = matrix.length - 1; int col = 0; int count = 0; while (row \u0026gt;= 0 \u0026amp;\u0026amp; col \u0026lt; matrix[0].length) { if (matrix[row][col] \u0026gt; target) { row--; } else { count += (row + 1); col++; } } return count; } // ========================================  // 下面这种解法不能处理  //  // [1, 2]  // [1, 3]  //  // 寻找第 3 大的数字这种情况  // ========================================  public int kthSmallest0(int[][] matrix, int k) { int lo = matrix[0][0]; int hi = matrix[matrix.length - 1][matrix[0].length - 1]; while (lo \u0026lt;= hi) { int middle = lo + ((hi - lo) \u0026gt;\u0026gt; 1); //  // 小于或者等于 middle 的是 count 个  int count = countLessOrEqual(matrix, middle); if (count \u0026lt; k) { lo = middle + 1; } else if (count \u0026gt; k) { hi = middle - 1; } else if (count == k) { return lessOrEqualThanBinarySearch(matrix, middle); } } return lo; } private int lessOrEqualThanBinarySearch(int[][] matrix, int target) { int lo = 0; int hi = matrix.length - 1; int row = 0; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (matrix[m][0] == target) { return target; } else if (matrix[m][0] \u0026gt; target) { hi = m - 1; } else { if (m == matrix.length - 1 || matrix[m + 1][0] \u0026gt; target) { row = m; break; } lo = m + 1; } } lo = 0; hi = matrix[row].length - 1; int col = 0; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (matrix[row][m] == target) { return target; } else if (matrix[row][m] \u0026gt; target) { hi = m - 1; } else { if (m == matrix[row].length - 1 || matrix[row][m + 1] \u0026gt; target) { col = m; break; } lo = m + 1; } } return matrix[row][col]; } private int countLessOrEqual0(int[][] matrix, int target) { // 最后一行的第一个数  int row = matrix.length - 1; int col = 0; int count = 0; while (row \u0026gt;= 0 \u0026amp;\u0026amp; col \u0026lt; matrix[0].length) { if (matrix[row][col] \u0026gt; target) { row--; } else { count += (row + 1); col++; } } return count; } } "});index.add({'id':138,'href':'/docs/programmer-interview/algorithm/largestnumber/','title':"数组元素所能拼成的最大数字",'content':"数组元素所能拼成的最大数字 import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; public class LargestNumber { public String largestNumber(int[] nums) { List\u0026lt;String\u0026gt; numList = toList(nums); Collections.sort(numList, new Comparator\u0026lt;String\u0026gt;() { @Override public int compare(String a, String b) { return (b + a).compareTo(a + b); } }); final StringBuilder sb = new StringBuilder(); for (String str: numList) { sb.append(str); } String res = sb.toString(); if (allZero(res)) { return \u0026#34;0\u0026#34;; } return res; } private boolean allZero(String str) { for (char c: str.toCharArray()) { if (c != \u0026#39;0\u0026#39;) { return false; } } return true; } private List\u0026lt;String\u0026gt; toList(int[] nums) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int num: nums) { list.add(String.valueOf(num)); } return list; } } "});index.add({'id':139,'href':'/docs/programmer-interview/algorithm/largestrectangleinhistogram/','title':"柱状图中最大的矩形",'content':"柱状图中最大的矩形 描述 题目来源\n给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。求在该柱状图中，能够勾勒出来的矩形的最大面积。\n题解 // https://leetcode.com/problems/largest-rectangle-in-histogram/ 直方图 // __ // __| | // | | | // | | | __ // __ | | |__| | // | |__| | | | | // |__|__|__|__|__|__| // public class LargestRectangleinHistogram { public int largestRectangleArea(int[] heights) { // =================================  // 左边最多延展到哪个索引  // =================================  // 左边比自己大的或相等的  //  // 单调栈找到第一个比自己大的或者小的数字  int[] left = new int[heights.length]; for (int i = 0; i \u0026lt; heights.length; i++) { int j = i; while (j - 1 \u0026gt;= 0 \u0026amp;\u0026amp; heights[j - 1] \u0026gt;= heights[i]) { j--; } left[i] = j; } // =================================  // 右边最多延展到哪个索引  // =================================  int[] right = new int[heights.length]; for (int i = heights.length - 1; i \u0026gt;= 0; i--) { int j = i; while (j + 1 \u0026lt; heights.length \u0026amp;\u0026amp; heights[i] \u0026lt;= heights[j + 1]) { j++; } right[i] = j; } int largest = 0; for (int i = 0; i \u0026lt; heights.length; i++) { largest = Math.max(heights[i] * (right[i] - left[i] + 1), largest); } return largest; } // 2, 1, 5, 6, 2, 3  public int largestRectangleArea0(int[] heights) { // =================================  // 左边最多延展到哪个索引  // =================================  // 左边比自己大的或相等的  //  // 单调栈找到第一个比自己大的或者小的数字  Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // __ // __| | // | | | // | | | __ // __ | | |__| | // | |__| | | | | // |__|__|__|__|__|__| // 0 1 2 3 4 5 // ↑ // 假设当前指向的是这个 // 只要栈里面的比自身高，那么就一直 pop // 所以 2、3 都被 pop 出去了  int[] left = new int[heights.length]; for (int i = 0; i \u0026lt; heights.length; i++) { int j = i; while (!stack.isEmpty() \u0026amp;\u0026amp; heights[i] \u0026lt;= heights[stack.peek()]) { j = stack.pop(); } // 栈空了，说明左边的都大于等于自己  // 栈不空，说明栈里面的 peek() 是小于自己的，那么 peek() + 1 是大于等于自己的  left[i] = stack.isEmpty() ? 0 : stack.peek() + 1; stack.push(i); } // =================================  // 右边最多延展到哪个索引  // =================================  stack.clear(); // 2,1,5,6,2,3  // ↑  // 5, 5  // 5, 4  int[] right = new int[heights.length]; for (int i = heights.length - 1; i \u0026gt;= 0; i--) { int j = i; while (!stack.isEmpty() \u0026amp;\u0026amp; heights[i] \u0026lt;= heights[stack.peek()]) { j = stack.pop(); } // 栈空了，说明右边的都大于等于自己  // 栈不空，说明栈里面的 peek() 是小于自己的，那么 peek() - 1 是大于等于自己的  right[i] = stack.isEmpty() ? heights.length - 1 : stack.peek() - 1; stack.push(i); } int largest = 0; for (int i = 0; i \u0026lt; heights.length; i++) { largest = Math.max(heights[i] * (right[i] - left[i] + 1), largest); } return largest; } } "});index.add({'id':140,'href':'/docs/programmer-interview/algorithm/longestsequence/','title':"最长序列",'content':"最长序列 最长连续递增相差为 1 的序列 import java.util.Arrays; // [100, 4, 200, 1, 3, 2] // // 必须是连续的 1 2 3 4 差值为 1 public class LongestConsecutiveSequence { public int longestConsecutive(int[] nums) { if (nums.length == 0) { return 0; } Arrays.sort(nums); int longestStreak = 1; int currentStreak = 1; for (int i = 1; i \u0026lt; nums.length; i++) { if (nums[i] != nums[i - 1]) { if (nums[i] == nums[i - 1] + 1) { currentStreak += 1; } else { longestStreak = Math.max(longestStreak, currentStreak); currentStreak = 1; } } } return Math.max(longestStreak, currentStreak); } } 连续递增子序列 // Input: [1,3,5,4,7] // Output: 3 // // 连续递增子数组, [1,3,5] public class LongestContinuousIncreasingSubsequence { public int findLengthOfLCIS(int[] nums) { if (nums.length == 0) { return 0; } int[] maxLength = new int[nums.length]; maxLength[0] = 1; for (int i = 1; i \u0026lt; nums.length; i++) { maxLength[i] = nums[i] \u0026gt; nums[i - 1] ? maxLength[i - 1] + 1 : 1; } return findMax(maxLength); } private int findMax(int[] arr) { int max = arr[0]; for (int i = 1; i \u0026lt; arr.length; i++) { if (arr[i] \u0026gt; max) { max = arr[i]; } } return max; } } "});index.add({'id':141,'href':'/docs/programmer-interview/algorithm/majorityelement/','title':"找出过半数的元素",'content':"找出过半数的元素 // Given an array of size n, find the majority element. // The majority element is the element that appears more than ⌊ n/2 ⌋ times. // You may assume that the array is non-empty and the majority element always exist in the array. // public class MajorityElement { public int majorityElement(int[] nums) { int candidate = nums[0]; int count = 1; for (int i = 1; i \u0026lt; nums.length; i++) { // ================  // no candidate  // ================  if (count == 0) { candidate = nums[i]; count = 1; } else if (nums[i] == candidate) { count++; } else { count--; } } return candidate; } } "});index.add({'id':142,'href':'/docs/programmer-interview/algorithm/maxareaofisland/','title':"最大的岛屿",'content':"最大的岛屿 // 时间复杂度 O(row * col)，因为每个小方格访问一次 // public class MaxAreaofIsland { public int maxAreaOfIsland(int[][] grid) { int row = grid.length; int col = grid[0].length; boolean[][] visited = new boolean[row][col]; int max = 0; for (int r = 0; r \u0026lt; row; r++) { for (int c = 0; c \u0026lt; col; c++) { max = Math.max(maxArea(grid, visited, r, c), max); } } return max; } private int maxArea(int[][] grid, boolean[][] visited, int r, int c) { if (r \u0026lt; 0 || r \u0026gt;= grid.length || c \u0026lt; 0 || c \u0026gt;= grid[0].length || grid[r][c] == 0 || visited[r][c]) { return 0; } visited[r][c] = true; return 1 + maxArea(grid, visited, r - 1, c) + maxArea(grid, visited, r + 1, c) + maxArea(grid, visited, r, c - 1) + maxArea(grid, visited, r, c + 1) ; } } "});index.add({'id':143,'href':'/docs/programmer-interview/algorithm/maxpointsonaline/','title':"直线上最多的点数",'content':"直线上最多的点数 描述 题目 给定一个二维平面，平面上有 n 个点，求最多有多少个点在同一条直线上。\n输入: [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]\n输出: 4\n解释:\n^\r|\r| o\r| o o\r| o\r| o o\r+-------------------\u0026gt;\r0 1 2 3 4 5 6\r题解 public class MaxPointsonaLine { public int maxPoints(Point[] points) { if (points.length \u0026lt;= 1) { return points.length; } int max = 0; for (int i = 0; i \u0026lt; points.length - 1; i++) { for (int j = i + 1; j \u0026lt; points.length; j++) { max = Math.max(maxPoint(points[i], points[j], points), max); } } return max; } private int maxPoint(Point p1, Point p2, Point[] points) { int max = 0; for (Point p: points) { if (_eq(p1, p) || _eq(p2, p) || isSameLine(p1, p2, p)) { max++; } } return max; } private boolean _eq(Point p1, Point p2) { return p1.x == p2.x \u0026amp;\u0026amp; p1.y == p2.y; } private boolean isSameLine(Point p1, Point p2, Point p) { // y2 - y1 y - y1  // ------- = ------  // x2 - x1 x - x1  //  if (p1.x == p2.x) { return p.x == p1.x; } if (p1.y == p2.y) { return p.y == p1.y; } if (p.x == p1.x || p.x == p2.x || p.y == p1.y || p.y == p2.y) { return false; } return (p2.y - p1.y) * (p.x - p1.x) == (p2.x - p1.x) * (p.y - p1.y); } } "});index.add({'id':144,'href':'/docs/programmer-interview/algorithm/maximalrectangle/','title':"最大的矩形",'content':"最大的矩形 描述 题目 给定一个仅包含 0 和 1 的二维二进制矩阵，找出只包含 1 的最大矩形，并返回其面积。\n输入:\r[\r[\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;,\u0026quot;0\u0026quot;],\r[\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;],\r[\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;],\r[\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;,\u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;]\r]\r输出: 6\r答案 // Given a matrix: // [ // [1, 1, 0, 0, 1], // [0, 1, 0, 0, 1], // [0, 0, 1, 1, 1], // [0, 0, 1, 1, 1], // [0, 0, 0, 0, 1] // ] // return 6. public class MaximalRectangle { public int maximalRectangle(boolean[][] matrix) { if (matrix == null || matrix.length == 0) { return 0; } int[][] height = new int[matrix.length][matrix[0].length]; height[0][0] = matrix[0][0] ? 1 : 0; for (int i = 1; i \u0026lt; matrix[0].length; i++) { height[0][i] = matrix[0][i] ? 1 : 0; } for (int i = 1; i \u0026lt; matrix.length; i++) { height[i][0] = matrix[i][0] ? height[i - 1][0] + 1 : 0; } int max = largestRectangleArea(height[0]); for (int i = 1; i \u0026lt; matrix.length; i++) { for (int j = 1; j \u0026lt; matrix[0].length; j++) { height[i][j] = matrix[i][j] ? height[i - 1][j] + 1 : 0; } max = Math.max(largestRectangleArea(height[i]), max); } return max; } public int largestRectangleArea(int[] heights) { // =================================  // 左边最多延展到哪个索引  // =================================  int[] left = new int[heights.length]; for (int i = 0; i \u0026lt; heights.length; i++) { int j = i; while (j - 1 \u0026gt;= 0 \u0026amp;\u0026amp; heights[j - 1] \u0026gt;= heights[i]) { j--; } left[i] = j; } // =================================  // 右边最多延展到哪个索引  // =================================  int[] right = new int[heights.length]; for (int i = heights.length - 1; i \u0026gt;= 0; i--) { int j = i; while (j + 1 \u0026lt; heights.length \u0026amp;\u0026amp; heights[i] \u0026lt;= heights[j + 1]) { j++; } right[i] = j; } int largest = 0; for (int i = 0; i \u0026lt; heights.length; i++) { largest = Math.max(heights[i] * (right[i] - left[i] + 1), largest); } return largest; } } "});index.add({'id':145,'href':'/docs/programmer-interview/algorithm/maximumaveragesubarray/','title':"子数组的最大平均值",'content':"子数组的最大平均值 描述 原题 给定一个由 n 个整数组成的数组，找到给定长度 k 的连续子数组，该子数组具有最大平均值。你需要输出最大平均值。\n答案  前缀和  // https://www.lintcode.com/problem/maximum-average-subarray/description // 给定一个由 n 个整数组成的数组，找到给定长度 k 的连续子数组，该子数组具有最大平均值。你需要输出最大平均值。 // 注意这个长度 k 是固定的 // public class MaximumAverageSubarray { public double findMaxAverage(int[] nums, int k) { int[] prefixSum = prefixSum(nums); int maxSum = 0; for (int i = k; i \u0026lt; prefixSum.length; i++) { maxSum = Math.max(maxSum, prefixSum[i] - prefixSum[i - k]); } return maxSum * 1.0 / k; } private int[] prefixSum(int[] nums) { int[] prefixSum = new int[nums.length + 1]; for (int i = 1; i \u0026lt;= nums.length; i++) { prefixSum[i] = prefixSum[i - 1] + nums[i - 1]; } return prefixSum; } } "});index.add({'id':146,'href':'/docs/programmer-interview/algorithm/maximumproductsubarray/','title':"连续子数组最大乘积",'content':"连续子数组最大乘积 // 连续子数组最大乘积 public class MaximumProductSubarray { public int maxProduct(int[] nums) { int min = nums[0]; int max = nums[0]; int ans = nums[0]; for (int i = 1; i \u0026lt; nums.length; i++) { // ==============================  // max 值基于更新后的 min 值进行了计算  // ==============================  int A = nums[i] * min; int B = nums[i] * max; min = min(A, B, nums[i]); max = max(A, B, nums[i]); if (max \u0026gt; ans) { ans = max; } } return ans; } private int min(int...arr) { int m = arr[0]; for (int i = 1; i \u0026lt; arr.length; i++) { if (arr[i] \u0026lt; m) { m = arr[i]; } } return m; } private int max(int...arr) { int m = arr[0]; for (int i = 1; i \u0026lt; arr.length; i++) { if (arr[i] \u0026gt; m) { m = arr[i]; } } return m; } } "});index.add({'id':147,'href':'/docs/programmer-interview/algorithm/maximumsizesubarraysumequalsk/','title':"最大子数组之和为 K",'content':"最大子数组之和为 K  微软\n 描述 给一个数组nums和目标值k，找到数组中最长的子数组，使其中的元素和为k。如果没有，则返回0。\n 数组之和保证在32位有符号整型数的范围内\n 题解  前缀和 + Map  import java.util.HashMap; import java.util.Map; // https://www.lintcode.com/problem/maximum-size-subarray-sum-equals-k/description // Facebook // // 微软面试题 // longest consecutive sequence of numbers that sum to K // https://www.1point3acres.com/bbs/thread-541121-1-1.html // // 给一个数组nums和目标值k，找到数组中最长的子数组，使其中的元素和为k。如果没有，则返回0。 // // ↓ ↓ (k = 17) // 2 [ 3 5 6 3 ] 8 public class MaximumSizeSubarraySumEqualsk { public int maxSubArrayLen(int[] nums, int k) { int[] prefixSum = prefixSum(nums); // 感觉就是跟 two sum 似的  // 借助 map 直接让复杂度降低了  Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // 左边的边界  map.put(0, 0); int ans = 0; for (int i = 1; i \u0026lt; prefixSum.length; i++) { int target = prefixSum[i] - k; if (map.containsKey(target)) { ans = Math.max(ans, i - map.get(target)); } if (!map.containsKey(prefixSum[i])) { map.put(prefixSum[i], i); } } return ans; } private int[] prefixSum(int[] nums) { int[] prefixs = new int[nums.length + 1]; for (int i = 0; i \u0026lt; nums.length; i++) { prefixs[i + 1] = prefixs[i] + nums[i]; } return prefixs; } } "});index.add({'id':148,'href':'/docs/programmer-interview/algorithm/maximumsubmatrix/','title':"最大子矩阵",'content':"最大子矩阵 描述 原题 给出一个大小为 n x n 的矩阵，里面元素为 正整数 和 负整数 ，找到具有最大和的子矩阵。\n输入:\rmatrix = [\r[1,3,-1],\r[2,3,-2],\r[-1,-2,-3]\r]\r输出: 9\r解释:\r具有最大和的子矩阵是:\r[\r[1,3],\r[2,3]\r]\r题解  二维数组的前缀和  // https://www.lintcode.com/problem/maximum-submatrix/description // // Given an n x n matrix of positive and negative integers, // find the submatrix with the largest possible sum. // public class MaximumSubmatrix { public int maxSubmatrix(int[][] matrix) { if (matrix == null || matrix.length == 0 || matrix[0].length == 0) { return 0; } int[][] prefixSum = prefixSum(matrix); int max = Integer.MIN_VALUE; for (int i = 1; i \u0026lt;= matrix.length; i++) { for (int j = i; j \u0026lt;= matrix.length; j++) { int currMax = sumColumn(i, j, 1, prefixSum); max = Math.max(currMax, max); for (int col = 1; col \u0026lt; matrix[0].length; col++) { if (currMax \u0026lt; 0) { currMax = sumColumn(i, j, col + 1, prefixSum); } else { currMax = currMax + sumColumn(i, j, col + 1, prefixSum); } max = Math.max(currMax, max); } } } return max; } private int sumColumn(int i, int j, int col, int[][] prefixSum) { return prefixSum[j][col] - prefixSum[i - 1][col] - (prefixSum[j][col - 1] - prefixSum[i - 1][col - 1]); } private int[][] prefixSum(int[][] matrix) { int[][] prefixSum = new int[matrix.length + 1][matrix[0].length + 1]; for (int i = 1; i \u0026lt;= matrix.length; i++) { for (int j = 1; j \u0026lt;= matrix[0].length; j++) { prefixSum[i][j] = prefixSum[i - 1][j] + prefixSum[i][j - 1] + matrix[i - 1][j - 1] - prefixSum[i - 1][j - 1]; } } return prefixSum; } } "});index.add({'id':149,'href':'/docs/programmer-interview/algorithm/maximumswap/','title':"最大的交换",'content':"最大的交换 描述 原题 给定一个非负整数, 你可以选择交换它的两个数位. 返回你能获得的最大的合法的数.\n 给定的数字在 [0, 10^8] 范围内\n 输入: 2736\r输出: 7236\r解释: 交换数字2和数字7.\r题解 // https://www.lintcode.com/problem/maximum-swap/description // // 给定一个非负整数, 你可以选择交换它的两个数位. 返回你能获得的最大的合法的数. // // 输入: 2736 // 输出: 7236 // 解释: 交换数字2和数字7. // // 给定的数字在 [0, 10^8] 范围内 public class MaximumSwap { public static void main(String...args) { MaximumSwap maximumSwap = new MaximumSwap(); System.out.println(maximumSwap.maximumSwap0(98368)); System.out.println(maximumSwap.maximumSwap0(1993)); } public int maximumSwap0(int num) { char[] arr = String.valueOf(num).toCharArray(); int[] max = new int[arr.length]; max[arr.length - 1] = arr.length - 1; for (int i = arr.length - 2; i \u0026gt;= 0; i--) { char prevNum = arr[max[i + 1]]; char singleNum = arr[i]; if (singleNum \u0026gt; prevNum) { max[i] = i; } else { max[i] = max[i + 1]; } } print(max); for (int i = 0; i \u0026lt; arr.length; i++) { // 索引不同  // 并且数字也要不同  if (i != max[i] \u0026amp;\u0026amp; arr[i] != arr[max[i]]) { swap0(arr, i, max[i]); return Integer.parseInt(new String(arr)); } } return num; } public static final void print(int[] arr) { StringBuilder sb = new StringBuilder(\u0026#34;[\u0026#34;); for (int i = 0; i \u0026lt; arr.length; i++) { sb.append(arr[i]); if (i != arr.length - 1) { sb.append(\u0026#39;,\u0026#39;); } } sb.append(\u0026#34;]\u0026#34;); System.out.println(sb.toString()); } private void swap0(char[] arr, int i, int j) { char t = arr[i]; arr[i] = arr[j]; arr[j] = t; } // 1 0000 0000  public int maximumSwap(int num) { char[] bucket = String.valueOf(num).toCharArray(); int[] intBucket = new int[bucket.length]; for (int i = 0; i \u0026lt; bucket.length; i++) { intBucket[i] = bucket[i] - \u0026#39;0\u0026#39;; } for (int i = 0; i \u0026lt; intBucket.length - 1; i++) { int candidateI = i; int candidateJ = i; // 5656626 =\u0026gt; 6656625  // =\u0026gt; 6556626  //  // 98386 =\u0026gt; 98836  for (int j = i + 1; j \u0026lt; intBucket.length; j++) { if (intBucket[j] \u0026gt;= intBucket[candidateJ]) { candidateJ = j; } } if (candidateI != candidateJ \u0026amp;\u0026amp; intBucket[candidateI] != intBucket[candidateJ]) { swap(intBucket, candidateI, candidateJ); return toInt(intBucket); } } return num; } private int toInt(int[] intBucket) { final StringBuilder sb = new StringBuilder(); for (int i = 0; i \u0026lt; intBucket.length; i++) { sb.append(intBucket[i]); } return Integer.parseInt(sb.toString()); } private void swap(int[] array, int i, int j) { int tmp = array[i]; array[i] = array[j]; array[j] = tmp; } } "});index.add({'id':150,'href':'/docs/programmer-interview/algorithm/medianoftwosortedarrays/','title':"两个有序数组合并后的中位数",'content':"两个有序数组合并后的中位数  微软\n 题解 public class MedianofTwoSortedArrays { public double findMedianSortedArrays(int[] nums1, int[] nums2) { int n = nums1.length + nums2.length; if (n % 2 == 1) { return findKth(nums1, nums2, 0, nums1.length - 1, 0, nums2.length - 1, n / 2 + 1); } else { double a = findKth(nums1, nums2, 0, nums1.length - 1, 0, nums2.length - 1, n / 2); double b = findKth(nums1, nums2, 0, nums1.length - 1, 0, nums2.length - 1, n / 2 + 1); return (a + b) / 2.0; } } private double findKth(int[] nums1, int[] nums2, int l1, int h1, int l2, int h2, int k) { if (l1 \u0026gt; h1) { return nums2[l2 + k - 1]; } else if (l2 \u0026gt; h2) { return nums1[l1 + k - 1]; } int m1 = l1 + ((h1 - l1) \u0026gt;\u0026gt; 1); int m2 = l2 + ((h2 - l2) \u0026gt;\u0026gt; 1); if (nums1[m1] \u0026lt;= nums2[m2]) { if (k \u0026lt;= (m1 - l1) + (m2 - l2) + 1) { return findKth(nums1, nums2, l1, h1, l2, m2 - 1, k); } else { return findKth(nums1, nums2, m1 + 1, h1, l2, h2, k - (m1 - l1 + 1)); } } else { if (k \u0026lt;= (m1 - l1) + (m2 - l2) + 1) { return findKth(nums1, nums2, l1, m1 - 1, l2, h2, k); } else { return findKth(nums1, nums2, l1, h1, m2 + 1, h2, k - (m2 - l2 + 1)); } } } } 其他网友答案 (有注释) // O(log(m + n)) public class MedianofTwoSortedArrays_Reference { // using divide and conquer idea, each time find the mid of both arrays  public double findMedianSortedArrays(int A[], int m, int B[], int n) { /* A[0, 1, 2, ..., n-1, n] */ /* A[0, 1, 2, ..., m-1, m] */ int k = (m + n + 1) / 2; double v = (double) FindKth(A, 0, m - 1, B, 0, n - 1, k); if ((m + n) % 2 == 0) { int k2 = k + 1; double v2 = (double) FindKth(A, 0, m - 1, B, 0, n - 1, k2); v = (v + v2) / 2; } return v; } // find the kth element int the two sorted arrays  // let us say: A[aMid] \u0026lt;= B[bMid], x: mid len of a, y: mid len of b, then wen can know  //  // (1) there will be at least (x + 1 + y) elements before bMid  // (2) there will be at least (m - x - 1 + n - y) = m + n - (x + y +1) elements after aMid  // therefore  // if k \u0026lt;= x + y + 1, find the kth element in a and b, but unconsidering bMid and its suffix  // if k \u0026gt; x + y + 1, find the k - (x + 1) th element in a and b, but unconsidering aMid and its prefix  //  // A: [X, X, X, X, X, X, X, X, X, X, X]  // ↑ ↑  // aL aR  //  // B: [X, X, X, X, X, X, X, X, X, X, X]  // ↑ ↑  // bL bR  int FindKth(int A[], int aL, int aR, int B[], int bL, int bR, int k /** 第 k 个的意思 */) { if (aL \u0026gt; aR) return B[bL + k - 1]; if (bL \u0026gt; bR) return A[aL + k - 1]; int aMid = (aL + aR) / 2; int bMid = (bL + bR) / 2; // =====================  // 排在 B[bMid] 前面的元素，至少有 A 前半部分，B 前半部分，这两部分  // =====================  if (A[aMid] \u0026lt;= B[bMid]) { // ========================  // K 位于这两个前半部分，那么肯定可以砍掉 B 的最后半部分  // ========================  //  // 这个 1 代表的是 kth，而非 indexK，实际上可以这样写：  // k 的索引位于某个区间  //  // (kIndex = k - 1) \u0026lt;= (aMid - aL) + (bMid - bL)  if (k \u0026lt;= (aMid - aL) + (bMid - bL) + 1) return FindKth(A, aL, aR, B, bL, bMid - 1, k); else // ========================  // K 不位于这两个前半部分，肯定可以砍掉 A 的前半部分  // ========================  return FindKth(A, aMid + 1, aR, B, bL, bR, k - (aMid - aL) - 1); } else { // A[aMid] \u0026gt; B[bMid]  if (k \u0026lt;= (aMid - aL) + (bMid - bL) + 1) return FindKth(A, aL, aMid - 1, B, bL, bR, k); else return FindKth(A, aL, aR, B, bMid + 1, bR, k - (bMid - bL) - 1); } } } "});index.add({'id':151,'href':'/docs/programmer-interview/algorithm/mergeintervals/','title':"合并区间",'content':"合并区间 描述 原题 给出一个区间的集合，请合并所有重叠的区间。\n输入: [[1,3],[2,6],[8,10],[15,18]]\r输出: [[1,6],[8,10],[15,18]]\r解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6].\r题解 // Input: [[1,3],[2,6],[8,10],[15,18]] // Output: [[1,6],[8,10],[15,18]] // Explanation: Since intervals [1,3] and [2,6] overlaps, merge them into [1,6]. // // 56: https://leetcode.com/problems/merge-intervals/ public class MergeIntervals { public List\u0026lt;Interval\u0026gt; merge(List\u0026lt;Interval\u0026gt; intervals) { if (intervals == null || intervals.isEmpty()) { return Collections.emptyList(); } Collections.sort(intervals, new Comparator\u0026lt;Interval\u0026gt;() { @Override public int compare(Interval a, Interval b) { if (a.start \u0026lt; b.start) { return -1; } else if (a.start \u0026gt; b.start) { return 1; } else { if (a.end \u0026lt; b.end) { return -1; } else if (a.end \u0026gt; b.end) { return 1; } else { return 0; } } } }); List\u0026lt;Interval\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); Interval prev = null; for (Interval interval: intervals) { if (prev == null || interval.start \u0026gt; prev.end) { prev = interval; res.add(interval); } else { prev.end = Math.max(interval.end, prev.end); } } return res; } } "});index.add({'id':152,'href':'/docs/programmer-interview/algorithm/mergeksortedarrays/','title':"合并 K 个有序数组",'content':"合并 K 个有序数组 题解  优先级队列  import java.util.PriorityQueue; // https://www.lintcode.com/problem/merge-k-sorted-arrays/description // public class MergeKSortedArrays { public int[] mergekSortedArrays(int[][] arrays) { PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(); int n = 0; for (int i = 0; i \u0026lt; arrays.length; i++) { for (int j = 0; j \u0026lt; arrays[i].length; j++) { n++; queue.offer(arrays[i][j]); } } int[] sortedArr = new int[n]; int index = 0; while (!queue.isEmpty()) { sortedArr[index++] = queue.poll(); } return sortedArr; } } "});index.add({'id':153,'href':'/docs/programmer-interview/algorithm/minimumsizesubarraysum/','title':"长度最小的子数组",'content':"长度最小的子数组 描述 题目 给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。\n输入：s = 7, nums = [2,3,1,2,4,3]\r输出：2\r解释：子数组 [4,3] 是该条件下的长度最小的子数组。\r题解 // Given an array of n positive integers and a positive integer s, // find the minimal length of a contiguous subarray of which the sum ≥ s. If there isn\u0026#39;t one, return 0 instead. // // Input: s = 7, nums = [2,3,1,2,4,3] // Output: 2 // Explanation: the subarray [4,3] has the minimal length under the problem constraint. // // 必须得连续 // // 时间复杂度 O(n^2) public class MinimumSizeSubarraySum { public int minSubArrayLen(int s, int[] nums) { int[] prefixSum = prefixSum(nums); int min = nums.length + 1; for (int i = 0; i \u0026lt; nums.length; i++) { for (int j = i + 1; j \u0026lt;= nums.length; j++) { if (prefixSum[j] - prefixSum[i] \u0026gt;= s) { min = Math.min(j - i, min); } } } return min == (nums.length + 1) ? 0 : min; } private int[] prefixSum(int[] nums) { int[] prefixSum = new int[nums.length + 1]; for (int i = 0; i \u0026lt; nums.length; i++) { prefixSum[i + 1] = prefixSum[i] + nums[i]; } return prefixSum; } } "});index.add({'id':154,'href':'/docs/programmer-interview/algorithm/movingaveragefromdatastream/','title':"数据流滑动窗口平均值",'content':"数据流滑动窗口平均值 描述 原题 给出一串整数流和窗口大小，计算滑动窗口中所有整数的平均值。\n题解 import java.util.LinkedList; import java.util.Queue; // https://www.lintcode.com/problem/moving-average-from-data-stream/description // 给出一串整数流和窗口大小，计算滑动窗口中所有整数的平均值。 // // MovingAverage m = new MovingAverage(3); // m.next(1) = 1 // 返回 1.00000 // m.next(10) = (1 + 10) / 2 // 返回 5.50000 // m.next(3) = (1 + 10 + 3) / 3 // 返回 4.66667 // m.next(5) = (10 + 3 + 5) / 3 // 返回 6.00000 // public class MovingAveragefromDataStream { private final int size; private final Queue\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); private double sum; /* * @param size: An integer */ public MovingAveragefromDataStream(int size) { this.size = size; } /* * @param val: An integer * @return: */ public double next(int val) { if (this.size == 0) { return 0; } sum += val; queue.add(val); if (queue.size() \u0026gt; this.size) { Integer removed = queue.remove(); sum -= removed.intValue(); } return sum / queue.size(); } } "});index.add({'id':155,'href':'/docs/programmer-interview/algorithm/nextgreaterelementii/','title':"下一个更大元素 II",'content':"下一个更大元素 II 描述 原题 给定一个循环数组（最后一个元素的下一个元素是数组的第一个元素），输出每个元素的下一个更大元素。数字 x 的下一个更大的元素是按数组遍历顺序，这个数字之后的第一个比它更大的数，这意味着你应该循环地搜索它的下一个更大的数。如果不存在，则输出 -1。\n输入: [1,2,1]\r输出: [2,-1,2]\r解释: 第一个 1 的下一个更大的数是 2；\r数字 2 找不到下一个更大的数； 第二个 1 的下一个最大的数需要循环搜索，结果也是 2。\r题解 // 循环数组 // Input: [1,2,1] // Output: [2,-1,2] public class NextGreaterElementII { // O(n^2)  public int[] nextGreaterElements(int[] nums) { int[] greater = new int[nums.length]; for (int i = 0; i \u0026lt; nums.length; i++) { greater[i] = findNextGreat(nums, i); } return greater; } private int findNextGreat(int[] nums, int i) { int great = nums[i]; int j = i + 1; while (j \u0026lt; nums.length) { if (nums[j] \u0026gt; great) { return nums[j]; } j++; } j = 0; while (j \u0026lt; i) { if (nums[j] \u0026gt; great) { return nums[j]; } j++; } return -1; } } 另外一种解法  单调递增栈  import java.util.Arrays; import java.util.Stack; public class NextGreaterElementII_Solution_1 { public int[] nextGreaterElements(int[] nums) { if (nums == null || nums.length == 0) return new int[0]; Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); // store the index  int n = nums.length; int[] res = new int[n]; Arrays.fill(res, -1); // =========================  // stack j  // ↓ ↓  // xxxxxxxxx yyyyyyyy  // =========================  for (int i = 0; i \u0026lt; 2 * n; i++) { int j = i % n; // ====================================================================  // 单调递增栈  //  // 只要栈里面的存储的比它小，那么栈里面的那个数的 next great element 就是这个 nums[j]  // ====================================================================  while (!stack.isEmpty() \u0026amp;\u0026amp; nums[j] \u0026gt; nums[stack.peek()]) { int index = stack.pop(); if (res[index] != -1) continue; res[index] = nums[j]; } stack.push(j); } return res; } } "});index.add({'id':156,'href':'/docs/programmer-interview/algorithm/nextpermutation/','title':"下一个排列",'content':"下一个排列  微软\n 描述 原题 实现获取下一个排列的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列。\n如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。\n必须原地修改，只允许使用额外常数空间。\n以下是一些例子，输入位于左侧列，其相应输出位于右侧列。\n1,2,3 → 1,3,2\r3,2,1 → 1,2,3\r1,1,5 → 1,5,1\r题解 public class NextPermutation { // 1 5 8 4 7 6 5 3 1  // ↑  // ↑(5 \u0026gt; 4 最后一个大于它的)  // 1 5 8 5 7 6 4 3 1 (交换)  // ↑(reverse)↑  // 1 5 8 5 1 3 4 6 7  //  // 找到第一个不符合从右到左升序对的数字 i = 4  // 找到第一个刚刚大于 nums[i] 的数字 j = 5  // swap(i, j)  // reverse(i + 1)  //  // ↓ (第一个不符合的数字 3，如果相等，比如多个 3 还要再往前找)  // 7 3 6 4 2  // ↑ (第一个刚刚大于 3 的数字 4)  //  // 7 4 6 3 2 (交换)  //  // 7 4 2 3 6 (4 后面的数字也交换)  public void nextPermutation(int[] nums) { int i = nums.length - 1; while (i \u0026gt; 0) { // ===========================  // 这个地方是 \u0026gt;=  // [1,1,2,2,5,2]  // ↑  // i  // ===========================  if (nums[i - 1] \u0026gt;= nums[i]) { i--; } else { break; } } // 指向前一个  i--; if (i \u0026gt;= 0) { int j = i; // ===========================  // 这个地方是 \u0026gt; 必须大于，不能等于  // [1,1,2,2,5,2]  // ↑ ↑  // i j  //  // 如果允许等于的话，那么 2 最终 j 还会指向 2  // reverse(i, j) 没什么用  // ===========================  while (j + 1 \u0026lt; nums.length \u0026amp;\u0026amp; nums[j + 1] \u0026gt; nums[i]) { j++; } swap(nums, i, j); } reverse(nums, i + 1, nums.length - 1); } private void reverse(int[] nums, int i, int j) { while (i \u0026lt; j) { swap(nums, i, j); i++; j--; } } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } } "});index.add({'id':157,'href':'/docs/programmer-interview/algorithm/numberofairplanesinthesky/','title':"数飞机",'content':"数飞机 描述 原题 给出飞机的起飞和降落时间的列表，用序列 interval 表示. 请计算出天上同时最多有多少架飞机？\n 如果多架飞机降落和起飞在同一时刻，我们认为降落有优先权。\n 输入: [(1, 10), (2, 3), (5, 8), (4, 7)]\r输出: 3\r解释: 第一架飞机在1时刻起飞, 10时刻降落.\r第二架飞机在2时刻起飞, 3时刻降落.\r第三架飞机在5时刻起飞, 8时刻降落.\r第四架飞机在4时刻起飞, 7时刻降落.\r在5时刻到6时刻之间, 天空中有三架飞机.\r题解 import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; import com.zk.algorithm.beans.Interval; // https://www.lintcode.com/problem/number-of-airplanes-in-the-sky/description // Amazon // // Input: [(1, 10), (2, 3), (5, 8), (4, 7)] // Output: 3 // Explanation: // The first airplane takes off at 1 and lands at 10. // The second ariplane takes off at 2 and lands at 3. // The third ariplane takes off at 5 and lands at 8. // The forth ariplane takes off at 4 and lands at 7. // During 5 to 6, there are three airplanes in the sky. // Example 2:  // Input: [(1, 2), (2, 3), (3, 4)] // Output: 1 // Explanation: Landing happen before taking off. // // 天空同时有几个飞机 // // 降落优先于起飞 public class NumberofAirplanesintheSky { public int countOfAirplanes(List\u0026lt;Interval\u0026gt; airplanes) { List\u0026lt;int[]\u0026gt; times = new ArrayList\u0026lt;\u0026gt;(airplanes.size() * 2); for (Interval interval: airplanes) { times.add(new int[]{ interval.start, 0 }); times.add(new int[]{ interval.end, 1 }); } Collections.sort(times, new Comparator\u0026lt;int[]\u0026gt;() { @Override public int compare(int[] a, int[] b) { if (a[0] \u0026lt; b[0]) { return -1; } else if (a[0] \u0026gt; b[0]) { return 1; } else { // 降落优先于起飞  // 降落排在前面  //  if (a[1] \u0026lt; b[1]) { return 1; } else if (a[1] \u0026gt; b[1]) { return -1; } return 0; } } }); int max = 0; int count = 0; for (int[] t: times) { if (t[1] == 0) { // 起飞  count++; } else { // 降落  count--; } max = Math.max(max, count); } return max; } } "});index.add({'id':158,'href':'/docs/programmer-interview/algorithm/partitionequalsubsetsum/','title':"分割等和子集",'content':"分割等和子集 描述 原题 给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n注意:\n 每个数组中的元素不会超过 100 数组的大小不会超过 200  输入: [1, 5, 11, 5]\r输出: true\r解释: 数组可以分割成 [1, 5, 5] 和 [11].\r题解 import java.util.Arrays; // https://leetcode.com/problems/partition-equal-subset-sum/ // // 0 1 背包问题 // 从 nums.length 个数中找出若干个数，使其和 == sum /2 // // 416. Two subset public class PartitionEqualSubsetSum { public boolean canPartition(int[] nums) { int sum = 0; for (int num : nums) { sum += num; } if ((sum \u0026amp; 1) == 1) { return false; } // ======================  // sum 变为一半  // ======================  sum /= 2; int n = nums.length; // dp[i][j]  boolean[][] dp = new boolean[n + 1][sum + 1]; for (int i = 0; i \u0026lt; dp.length; i++) { Arrays.fill(dp[i], false); } // ======================  // 0 个物品，选取值等于 0 的  // ======================  dp[0][0] = true; // ======================  // 1 个物品、2 个物品、3 个物品...，选取值等于 0 的，全部为 true，我们不选就行了  // ======================  for (int i = 1; i \u0026lt; n + 1; i++) { dp[i][0] = true; } // ======================  // 0 个物品选取，值等于 j 的，我们全部选择也不够  // ======================  for (int j = 1; j \u0026lt; sum + 1; j++) { dp[0][j] = false; } for (int i = 1; i \u0026lt; n + 1; i++) { for (int j = 1; j \u0026lt; sum + 1; j++) { // 如果 i - 1 个物品已经选择够 j 了，那么 i 个物品当然也可以选择够  if (dp[i - 1][j]) { dp[i][j] = dp[i - 1][j]; } else if (j \u0026gt;= nums[i - 1]) { // 选上 nums[i - 1]，也就是说最后一个，因为 i - 1 代表最后一个，i: 0 ~ n - 1  // 然后看之前的是否是 true 或者 false  dp[i][j] = dp[i - 1][j - nums[i - 1]]; } } } return dp[n][sum]; } } "});index.add({'id':159,'href':'/docs/programmer-interview/algorithm/productofarrayexceptself/','title':"数组除了自身的乘积",'content':" 描述 原题 给定 n 个整数的数组 nums，其中 n \u0026gt; 1，返回一个数组输出，使得 output[i] 等于 nums 的所有除了nums[i] 的元素的乘积。\n输入: [1,2,3,4]\r输出: [24,12,8,6]\r解释:\r2*3*4=24\r1*3*4=12\r1*2*4=8\r1*2*3=6\r题解 // 给定n个整数的数组nums，其中n\u0026gt; 1，返回一个数组输出，使得output [i]等于nums的所有除了nums [i]的元素的乘积。 // 在没有除和O(n)时间内解决 // https://www.lintcode.com/problem/product-of-array-except-self/description // 输入: [1,2,3,4] // 输出: [24,12,8,6] // 解释: // 2*3*4=24 // 1*3*4=12 // 1*2*4=8 // 1*2*3=6 // // 输入: [2,3,8] // 输出: [24,16,6] // 解释: // 3*8=24 // 2*8=16 // 2*3=6 public class ProductofArrayExceptSelf { public int[] productExceptSelf(int[] nums) { //  // 2, 3, 8  //  // 2, 6, 1  // 1, 24, 8  //  // 24, 16, 6  int[] leftProduct = new int[nums.length]; leftProduct[0] = nums[0]; leftProduct[nums.length - 1] = 1; for (int i = 1; i \u0026lt; nums.length - 1; i++) { leftProduct[i] = leftProduct[i - 1] * nums[i]; } // 2  // 0 1  // 1 * 2  int[] rightProduct = new int[nums.length]; rightProduct[nums.length - 1] = nums[nums.length - 1]; rightProduct[0] = 1; for (int i = nums.length - 2; i \u0026gt;= 1; i--) { rightProduct[i] = rightProduct[i + 1] * nums[i]; } int[] res = new int[nums.length]; for (int i = 0; i \u0026lt; nums.length; i++) { res[i] = (i \u0026gt;= 1 ? leftProduct[i - 1] : 1) * (i \u0026lt; nums.length - 1 ? rightProduct[i + 1] : 1); } return res; } } "});index.add({'id':160,'href':'/docs/programmer-interview/algorithm/queuereconstructionbyheight/','title':"根据身高重建队列",'content':"根据身高重建队列 原题 假设有打乱顺序的一群人站成一个队列。 每个人由一个整数对(h, k)表示，其中h是这个人的身高，k是排在这个人前面且身高大于或等于h的人数。 编写一个算法来重建这个队列。\n注意：\n 总人数少于1100人。  示例\n输入:\r[[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]]\r输出:\r[[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]]\r题解 import java.util.ArrayList; import java.util.Arrays; import java.util.Comparator; import java.util.List; // https://leetcode.com/problems/queue-reconstruction-by-height/ public class QueueReconstructionbyHeight { // [[7,0], [7,1], [6,1], [5,2], [5,0], [4,4]]  //  // [[7,0]]  // [[7,0], [7,1]]  // [[7,0], [6,1], [7,1]]  // [[7,0], [6,1], [5,2], [7,1]]  // [[5,0], [7,0], [6,1], [5,2], [7,1]]  // [[5,0], [7,0], [6,1], [5,2], [4,4], [7,1]]  public int[][] reconstructQueue(int[][] people) { // 个高的排在最前面，个矮的排在后面  Arrays.sort(people, new Comparator\u0026lt;int[]\u0026gt;() { @Override public int compare(int[] a, int[] b) { if (a[0] \u0026lt; b[0]) { return 1; } else if (a[0] \u0026gt; b[0]) { return -1; } if (a[1] \u0026lt; b[1]) { return -1; } else if (a[1] \u0026gt; b[1]) { return 1; } return 0; } }); List\u0026lt;int[]\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(people.length); for (int i = 0; i \u0026lt; people.length; i++) { // 个头矮的去前面寻找自己的位置，这样不会对每个人前面有几个人高于自己的这个 k 产生影响  // 所以必须是个头矮的去寻找位置  list.add(people[i][1], people[i]); } return list.toArray(people); } } "});index.add({'id':161,'href':'/docs/programmer-interview/algorithm/ratelimiter/','title':"限流器",'content':"限流器  基于 Token Bucket 的限流算法 基于 Redis 的限流算法  public class RateLimiter { public static long now() { return System.currentTimeMillis(); } // ==================  // - Token Bucket  // - Redis  // ==================  // =========================  // Token Bucket 算法  // 每过 RATE / PER 时间，就加  // =========================  public static class TokenBucketRateLimiter { private long lastCheck = now(); private static final int RATE = 5; // 5 requests  private static final int PER = 8; // 8 seconds  private int allowance = RATE; public boolean overhead(String key) { long current = now(); long timePassed = current - lastCheck; lastCheck = timePassed; // ======================  // 过去的这段时间内可以增加多少个 allowance  // ======================  allowance += timePassed * (RATE / PER); if (allowance \u0026gt; RATE) { allowance = RATE; } // ======================  // 是否能够减去 1.0 个  // ======================  if (allowance \u0026lt; 1.0) { return true; } else { allowance -= 1.0; return false; } } } // =========================  // Redis RateLimiter 算法  // 系统要限定用户的某个行为在指定的时间里只能允许发生 N 次  //  // 每一个行为到来时，都维护一次时间窗口。  // 将时间窗口外的记录全部清理掉，只保留窗口内的记录。  // zset 集合中只有 score 值非常重要，value 值没有特别的意义，  // 只需要保证它是唯一的就可以了  //  // 因为这几个连续的 Redis 操作都是针对同一个 key 的，  // 使用 pipeline 可以显著提升 Redis 存取效率。  // 但这种方案也有缺点，因为它要记录时间窗口内所有的行为记录，  // 如果这个量很大，比如限定 60s 内操作不得超过 100w 次这样的参数，  // 它是不适合做这样的限流的，因为会消耗大量的存储空间。  // =========================  public static class RedisRateLimiter { private static final int MAX_COUNT = 5; // 5 requests  private static final int PERIOD = 60; // 60 s  private Jedis jedis = new Jedis(); public boolean overhead(String key) { long nowTs = now(); Pipeline pipe = jedis.pipelined(); pipe.multi(); pipe.zadd(key, nowTs, \u0026#34;\u0026#34; + nowTs); pipe.zremrangeByScore(key, 0, nowTs - PERIOD * 1000); Response\u0026lt;Long\u0026gt; count = pipe.zcard(key); pipe.expire(key, PERIOD + 1); // 防止这个 key 一直存在  pipe.exec(); pipe.close(); return count.get() \u0026lt;= MAX_COUNT; } } static class Jedis { Pipeline pipelined() { return null; } } static class Pipeline { void exec() {} void close() {} void multi() {} void zadd(String key, long time, String value) {} void zremrangeByScore(String key, long begin, long end) {}; void expire(String key, long timeout) {} Response\u0026lt;Long\u0026gt; zcard(String key) { return new Response\u0026lt;Long\u0026gt;(); } } static class Response\u0026lt;T\u0026gt; { int get() { return 0; } } } "});index.add({'id':162,'href':'/docs/programmer-interview/algorithm/removeduplicatesfromsortedarray/','title':"删除排序数组中的重复项",'content':"删除排序数组中的重复项 描述 原题 给定一个排序数组，你需要在 原地 删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。\n不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。\n 示例 1:\n给定数组 nums = [1,1,2], 函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。 你不需要考虑数组中超出新长度后面的元素。\r题解 // 排序数组 // 移除重复数字 public class RemoveDuplicatesfromSortedArray { public int removeDuplicates(int[] nums) { // [1,1,1,1,2,2,3,4,5,5]  //  int noDuplicated = 0; for (int i = 1; i \u0026lt; nums.length; i++) { if (nums[i] == nums[i - 1]) { continue; } else { nums[++noDuplicated] = nums[i]; } } return nums.length == 0 ? 0 : noDuplicated + 1; } } "});index.add({'id':163,'href':'/docs/programmer-interview/algorithm/kth-of-two-sorted-array/','title':"两个有序数组第 K 大的数",'content':"两个有序数组第 K 大的数 O(n) 解法 class Main { static int kth(int arr1[], int arr2[], int m, int n, int k) { int[] sorted1 = new int[m + n]; int i = 0, j = 0, d = 0; while (i \u0026lt; m \u0026amp;\u0026amp; j \u0026lt; n) { if (arr1[i] \u0026lt; arr2[j]) sorted1[d++] = arr1[i++]; else sorted1[d++] = arr2[j++]; } while (i \u0026lt; m) sorted1[d++] = arr1[i++]; while (j \u0026lt; n) sorted1[d++] = arr2[j++]; return sorted1[k - 1]; } // main function  public static void main (String[] args) { int arr1[] = {2, 3, 6, 7, 9}; int arr2[] = {1, 4, 8, 10}; int k = 5; System.out.print(kth(arr1, arr2, 5, 4, k)); } } O(log K) 解法 // Java Program to find kth element from two sorted arrays class GFG { static int kth(int arr1[], int arr2[], int m, int n, int k, int st1, int st2) { // In case we have reached end of array 1  if (st1 == m) { return arr2[st2 + k - 1]; } // In case we have reached end of array 2  if (st2 == n) { return arr1[st1 + k - 1]; } // k should never reach 0 or exceed sizes  // of arrays  if (k == 0 || k \u0026gt; (m - st1) + (n - st2)) { return -1; } // Compare first elements of arrays and return  if (k == 1) { return (arr1[st1] \u0026lt; arr2[st2]) ? arr1[st1] : arr2[st2]; } int curr = k / 2; // Size of array 1 is less than k / 2  if (curr - 1 \u0026gt;= m - st1) { // Last element of array 1 is not kth  // We can directly return the (k - m)th  // element in array 2  if (arr1[m - 1] \u0026lt; arr2[st2 + curr - 1]) { return arr2[st2 + (k - (m - st1) - 1)]; } else { return kth(arr1, arr2, m, n, k - curr, st1, st2 + curr); } } // Size of array 2 is less than k / 2  if (curr - 1 \u0026gt;= n - st2) { if (arr2[n - 1] \u0026lt; arr1[st1 + curr - 1]) { return arr1[st1 + (k - (n - st2) - 1)]; } else { return kth(arr1, arr2, m, n, k - curr, st1 + curr, st2); } } else // Normal comparison, move starting index  // of one array k / 2 to the right  if (arr1[curr + st1 - 1] \u0026lt; arr2[curr + st2 - 1]) { return kth(arr1, arr2, m, n, k - curr, st1 + curr, st2); } else { return kth(arr1, arr2, m, n, k - curr, st1, st2 + curr); } } // Driver code  public static void main(String[] args) { int arr1[] = {2, 3, 6, 7, 9}; int arr2[] = {1, 4, 8, 10}; int k = 5; int st1 = 0, st2 = 0; System.out.println(kth(arr1, arr2, 5, 4, k, st1, st2)); } } 参考  K-th Element of Two Sorted Arrays  "});index.add({'id':164,'href':'/docs/programmer-interview/algorithm/removeelement/','title':"移除元素",'content':"移除元素 描述 原题 给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。\n不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。\n元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1:\n给定 nums = [3,2,2,3], val = 3,\r函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。\r你不需要考虑数组中超出新长度后面的元素。\r题解 public class RemoveElement { // [1,1,1,2,2]  public int removeElement(int[] nums, int val) { int valid = -1; for (int i = 0; i \u0026lt; nums.length; i++) { if (nums[i] != val) { nums[++valid] = nums[i]; } } return nums.length == 0 ? 0 : valid + 1; } } "});index.add({'id':165,'href':'/docs/programmer-interview/algorithm/reversepairs/','title':"逆序对",'content':"逆序对 描述 数组的逆序对个数\n题解 // https://www.lintcode.com/problem/reverse-pairs/description // // Input: A = [2, 4, 1, 3, 5] // Output: 3 // Explanation: // (2, 1), (4, 1), (4, 3) are reverse pairs // // 逆序对个数 public class ReversePairs { public long reversePairs(int[] A) { return mergeSort(A, 0, A.length - 1); } private long mergeSort(int[] A, int lo, int hi) { int sum = 0; if (lo \u0026lt; hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); // =====================  // 注意：这个地方是都加了一遍  // =====================  sum += mergeSort(A, lo, mid); sum += mergeSort(A, mid + 1, hi); sum += merge(A, lo, mid, hi); } return sum; } private int merge(int[] A, int lo, int mid, int hi) { int[] temp = new int[hi - lo + 1]; int tempIndex = 0; int loIndex = lo; int hiIndex = mid + 1; int count = 0; while (loIndex \u0026lt;= mid \u0026amp;\u0026amp; hiIndex \u0026lt;= hi) { // left | right  // 3 5 6 | 2  //  // 3 比 2 大的话，那么 3 5 6 都比 2 大  if (A[loIndex] \u0026gt; A[hiIndex]) { count += mid - loIndex + 1; temp[tempIndex++] = A[hiIndex++]; } else { temp[tempIndex++] = A[loIndex++]; } } while (loIndex \u0026lt;= mid) { temp[tempIndex++] = A[loIndex++]; } while (hiIndex \u0026lt;= hi) { temp[tempIndex++] = A[hiIndex++]; } tempIndex = 0; while (tempIndex \u0026lt; temp.length) { A[lo + tempIndex] = temp[tempIndex]; tempIndex++; } return count; } } "});index.add({'id':166,'href':'/docs/programmer-interview/algorithm/rotateimage/','title':"旋转图像",'content':"旋转图像 描述 原题 给定一个 n × n 的二维矩阵表示一个图像。\n将图像顺时针旋转 90 度。\n说明：\n你必须在原地旋转图像，这意味着你需要直接修改输入的二维矩阵。请不要使用另一个矩阵来旋转图像。\n题解 // Given input matrix = // [ // [1,2,3], // [4,5,6], // [7,8,9] // ],  // rotate the input matrix in-place such that it becomes: // [ // [7,4,1], // [8,5,2], // [9,6,3] // ] public class RotateImage { public void rotate(int[][] matrix) { int n = matrix.length; for (int i = 0; i \u0026lt; n / 2; i++) { // ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓  // ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓  //  // 【j 走到哪里停】? (无需走到9，这个时候就已经完了. j \u0026lt; (n - 1) - i，小于号，而非小于等于)  //  // ↓ ↓  // 7, 8, 9  // 12, 13, 14  // 17, 18, 19  for (int j = i; j \u0026lt; n - 1 - i; j++) { // ----------------------------------------\u0026gt;  // | (i,j)  // | X  // | X (n-1-j, i)  // |  // |  // |  // |  // |  // | X (j,n-1-i)  // | X  // | (n-1-i, n-1-j)  // |  // |  // |  // |  // V  //  // (i,j)  // |  // (j,n-1-i)  // |  // (n-1-i,n-1-j)  // |  // (n-1-j,i)  //  int tmp = matrix[i][j]; matrix[i][j] = matrix[n - 1 - j][i]; matrix[n - 1 - j][i] = matrix[n - 1 - i][n - 1 - j]; matrix[n - 1 - i][n - 1 - j] = matrix[j][n - 1 - i]; matrix[j][n - 1 - i] = tmp; } } } } "});index.add({'id':167,'href':'/docs/programmer-interview/algorithm/searcha2dmatrix/','title':"二维数组中的查找",'content':"二维数组中的查找 描述  剑指offer  在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n题解 // 这个算法适用于行有序、列有序的情况 public class Searcha2DMatrix { public boolean searchMatrix(int[][] matrix, int target) { if (matrix.length == 0) { return false; } int m = matrix.length; int n = matrix[0].length; // 第一行最后一个数字  //  int r = 0; int c = n - 1; while (r \u0026lt; m \u0026amp;\u0026amp; c \u0026gt;= 0) { int val = matrix[r][c]; if (target \u0026gt; val) { r++; } else if (target \u0026lt; val) { c--; } else { return true; } } return false; } } "});index.add({'id':168,'href':'/docs/programmer-interview/algorithm/searchforarange/','title':"搜索区间",'content':"搜索区间 描述 原题 给定一个包含 n 个整数的排序数组，找出给定目标值 target 的起始和结束位置。\n如果目标值不在数组中，则返回 [-1, -1]\n题解 // LintCode // https://www.lintcode.com/problem/search-for-a-range/leaderboard // public class SearchforaRange { public int[] searchRange(int[] A, int target) { if (A == null || A.length == 0) { return new int[]{ -1, -1 }; } int start = searchStartIndex(A, target); int end = searchEndIndex(A, target); return new int[]{ start, end }; } // target 开始的地方  private int searchStartIndex(int[] A, int target) { int lo = 0; int hi = A.length - 1; while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (A[mid] \u0026gt; target) { hi = mid - 1; } else if (A[mid] \u0026lt; target) { lo = mid + 1; } else { // ↓ (mid)  // 4 4 4 5 6 7  //  // ↓ (mid)  // 3 4 4 5 6 7  if (mid == 0 || mid - 1 \u0026gt;= 0 \u0026amp;\u0026amp; A[mid - 1] \u0026lt; target) { return mid; } else { hi = mid - 1; } } } return -1; } private int searchEndIndex(int[] A, int target) { int lo = 0; int hi = A.length - 1; while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (A[mid] \u0026gt; target) { hi = mid - 1; } else if (A[mid] \u0026lt; target) { lo = mid + 1; } else { if (mid == A.length - 1 || mid + 1 \u0026lt;= A.length - 1 \u0026amp;\u0026amp; A[mid + 1] \u0026gt; target) { return mid; } else { lo = mid + 1; } } } return -1; } } "});index.add({'id':169,'href':'/docs/programmer-interview/algorithm/searchinrotatedsortedarray/','title':"搜索旋转排序数组",'content':"搜索旋转排序数组 Search in Rotated Sorted Array 和 Search in Rotated Sorted Array II\n数组无重复数字 // 么有重复数字 public class SearchinRotatedSortedArray { // https://leetcode.com/problems/search-in-rotated-sorted-array/discuss/14472/Java-AC-Solution-using-once-binary-search  // 使用一次二分搜索  //  // 核心思想: 确定 m 位于哪一边，然后确定 target 是不是位于有序的一边  //  public int search(int[] nums, int target) { int lo = 0; int hi = nums.length - 1; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } // 6 7 1 2 3 4 5  // ↑ ↑ ↑ ↑ ↑  //  // 1 2 3 4 5 6 7  // ↑ ↑ ↑ ↑ ↑ ↑ ↑  //  // 右侧有序  // target 是不是位于有序右侧空间  if (nums[m] \u0026lt;= nums[hi]) { if (target \u0026gt; nums[m] \u0026amp;\u0026amp; target \u0026lt;= nums[hi]) { lo = m + 1; } else { hi = m - 1; } } else { // 6 7 1 2 3 4 5  // ↑ ↑  //  // target 是否位于左侧有序空间内  if (target \u0026gt;= nums[lo] \u0026amp;\u0026amp; target \u0026lt; nums[m]) { hi = m - 1; } else { lo = m + 1; } } } return -1; } // ================================  // 下面这个方案先查找 min 值  // ================================  public int search0(int[] nums, int target) { // ================================  // 忘记这一步骤了  // ================================  if (nums.length == 0) { return -1; } int minIndex = searchMin(nums); int index = -1; if ((index = binarySearch(nums, minIndex, nums.length - 1, target)) != -1) { return index; } return binarySearch(nums, 0, minIndex - 1, target); } private int searchMin(int[] nums) { int lo = 0; // always point to 前半部分  int hi = nums.length - 1; // always point to 后半部分  if (nums[lo] \u0026gt; nums[hi]) { while (lo \u0026lt; hi) { if (hi - lo == 1) { return hi; } int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] \u0026lt; nums[0]) { // middle 位于后半部分  hi = m; } else if (nums[m] \u0026gt; nums[nums.length - 1]) { // middle 位于前半部分  // min 在后半部分  //  // ==================================  // 注意这个地方， lo = m，而不是 lo = m + 1  // m + 1 有可能越界，跑到后半部分  // ==================================  lo = m; } } } return 0; } private int binarySearch(int[] nums, int lo, int hi, int target) { while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } else if (nums[m] \u0026gt; target) { hi = m - 1; } else { lo = m + 1; } } return -1; } } 有重复数字 // (i.e., [0,0,1,2,2,5,6] might become [2,5,6,0,0,1,2]). // 包含重复元素 // public class SearchinRotatedSortedArray2 { // https://leetcode.com/problems/search-in-rotated-sorted-array-ii/discuss/28202/Neat-JAVA-solution-using-binary-search  // 使用一次二分搜索  //  // 核心思想: 确定 m 位于哪一边，然后确定 target 是不是位于有序的一边  //  public boolean search(int[] nums, int target) { int lo = 0; int hi = nums.length - 1; while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return true; } // 0 0 0 1 1 1  // 1 1 0 0 0 1  //  // 0 1 1 2 2  // 2 0 1 2 2  if (nums[m] == nums[hi] \u0026amp;\u0026amp; nums[lo] == nums[hi]) { // 我们的指针无法确定哪一边是有序的  // 移动 lo++ 或者 hi-- 都可以帮助减少一个重复的元素  // 所以最坏情况是 O(n)  hi--; continue; } // 6 7 1 2 3 4 5  // ↑ ↑ ↑ ↑ ↑  //  // 1 2 3 4 5 6 7  // ↑ ↑ ↑ ↑ ↑ ↑ ↑  //  // 右侧有序  // target 是不是位于有序右侧空间  if (nums[m] \u0026lt;= nums[hi]) { if (target \u0026gt; nums[m] \u0026amp;\u0026amp; target \u0026lt;= nums[hi]) { lo = m + 1; } else { hi = m - 1; } } else { // 6 7 1 2 3 4 5  // ↑ ↑  //  // target 是否位于左侧有序空间内  if (target \u0026gt;= nums[lo] \u0026amp;\u0026amp; target \u0026lt; nums[m]) { hi = m - 1; } else { lo = m + 1; } } } return false; } public boolean search0(int[] nums, int target) { if (nums.length == 0) { return false; } // ==========================  // 1. we find the array\u0026#39;s rotate pivot  //  // 4 5 6 0 0 0 1 2 3  // ↑  // minIndex  // ==========================  int minIndex = searchMin(nums); // ==========================  // 2. search the second half by using binary search  //  // 4 5 6 0 0 0 1 2 3  // ↑ ↑  // ==========================  if (binarySearch(nums, minIndex, nums.length - 1, target) != -1) { return true; } // ==========================  // 2. search the first half by using binary search  //  // 4 5 6 0 0 0 1 2 3  // ↑ ↑  // ==========================  return binarySearch(nums, 0, minIndex - 1, target) != -1; } private int searchMin(int[] nums) { int lo = 0; // always point to the first half  int hi = nums.length - 1; // always point to the second half  // ==========================  // 1 2 3 4 5 6 7  // ↑ ↑  // ==========================  if (nums[lo] \u0026gt;= nums[hi]) { while (lo \u0026lt; hi) { // ==========================  // 4 5 6 1 2 3  // ↑ ↑  // lo hi  // ==========================  if (hi - lo == 1) { return hi; } int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); // ==========================  // we has no way to know the min located in which half  // ==========================  if (nums[m] == nums[lo] \u0026amp;\u0026amp; nums[m] == nums[hi]) { return findMin(nums, lo, hi); } else if (nums[m] \u0026lt; nums[lo]) { hi = m; } else if (nums[m] \u0026gt; nums[hi]) { lo = m; } } } return 0; } private int findMin(int[] nums, int lo, int hi) { int min = hi; // ==========================  // 1 3 1 1 1  // ↑  // minIndex (we need return this)  // ==========================  for (int i = hi - 1; i \u0026gt;= lo; i--) { if (nums[i] \u0026lt;= nums[min]) { min = i; } else { break; } } return min; } private int binarySearch(int[] nums, int lo, int hi, int target) { while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } else if (nums[m] \u0026gt; target) { hi = m - 1; } else { lo = m + 1; } } return -1; } } "});index.add({'id':170,'href':'/docs/programmer-interview/algorithm/slidingwindowmaximum/','title':"滑动窗口最大值",'content':"滑动窗口最大值 描述 原题 给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。\n返回滑动窗口中的最大值。\n输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3\r输出: [3,3,5,5,6,7] 解释: 滑动窗口的位置 最大值\r--------------- -----\r[1 3 -1] -3 5 3 6 7 3\r1 [3 -1 -3] 5 3 6 7 3\r1 3 [-1 -3 5] 3 6 7 5\r1 3 -1 [-3 5 3] 6 7 5\r1 3 -1 -3 [5 3 6] 7 6\r1 3 -1 -3 5 [3 6 7] 7\r题解 import java.util.Deque; import java.util.LinkedList; // 滑动窗口最大值 // k always valid // public class SlidingWindowMaximum { public int[] maxSlidingWindow(int[] nums, int k) { if (nums.length == 0) { return new int[]{}; } // [1,3,-1,-3,5,3,6,7]  // ↑(1)  // ↑(3)  // ↑(3)  // ↑(3)  // ↑(5)  //  // Queue 是一个 FIFO 队列，并不是一个 Deque  // Queue 中存放的是索引  //  // ==============================  // Deque 中维护的是索引值  //  // [1,2,3]  // ↑  // 最大元素  //  // - 如果超出范围了，那么移除 head 元素  // - 如果大于等于末尾元素，那么移除 last 元素  // ==============================  Deque\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); int[] result = new int[nums.length - k + 1]; int index = 0; for (int i = 0; i \u0026lt; nums.length; i++) { // 超出范围了，从 head 移除元素  while (outOfRange(queue, i, k)) { queue.poll(); } // 比队列中最后一个元素大，那么移除最后一个元素  while (biggerThanLastElementInQueue(queue, nums, nums[i])) { queue.pollLast(); } queue.offer(i); if (i + 1 \u0026gt;= k) { result[index++] = nums[queue.peek()]; } } return result; } private boolean outOfRange(Deque\u0026lt;Integer\u0026gt; queue, int i, int k) { return !queue.isEmpty() \u0026amp;\u0026amp; (i - queue.peek() \u0026gt;= k); } private boolean biggerThanLastElementInQueue(Deque\u0026lt;Integer\u0026gt; queue, int[] nums, int curr) { return !queue.isEmpty() \u0026amp;\u0026amp; curr \u0026gt;= nums[queue.peekLast()]; } } "});index.add({'id':171,'href':'/docs/programmer-interview/algorithm/slidingwindowmedian/','title':"滑动窗口中位数",'content':"滑动窗口中位数 描述 原题 中位数是有序序列最中间的那个数。如果序列的大小是偶数，则没有最中间的数；此时中位数是最中间的两个数的平均数。\n例如：\n [2,3,4]，中位数是 3 [2,3]，中位数是 (2 + 3) / 2 = 2.5  给你一个数组 nums，有一个大小为 k 的窗口从最左端滑动到最右端。窗口中有 k 个数，每次窗口向右移动 1 位。你的任务是找出每次窗口移动后得到的新窗口中元素的中位数，并输出由它们组成的数组。\n示例：\n给出 nums = [1,3,-1,-3,5,3,6,7]，以及 k = 3。\n窗口位置 中位数\r--------------- -----\r[1 3 -1] -3 5 3 6 7 1\r1 [3 -1 -3] 5 3 6 7 -1\r1 3 [-1 -3 5] 3 6 7 -1\r1 3 -1 [-3 5 3] 6 7 3\r1 3 -1 -3 [5 3 6] 7 5\r1 3 -1 -3 5 [3 6 7] 6\r因此，返回该滑动窗口的中位数数组 [1,-1,-1,3,5,6]。\n题解 暂未找到一个好记易理解的方法\n"});index.add({'id':172,'href':'/docs/programmer-interview/algorithm/sortcolors/','title':"颜色分类",'content':"颜色分类 描述 原题 给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。\n此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。\n题解 public class SortColors { // 0,0,1,1,2,2  //  // 2 0 1 0 1 1 2  public void sortColors(int[] nums) { // 000001111122222  // ↑ ↑  // l r  //  int left = 0; int right = nums.length - 1; int i = 0; while (i \u0026lt;= right) { if (nums[i] == 0) { // =========================  // 遇到 0 就换到左边  //  // 0 0 0 0 1 1 1 1 0 2  // ↑ ↑  // left i  //  // =========================  swap(nums, left++, i++); } else if (nums[i] == 1) { i++; } else { // =========================  // right 指向的是第一个非 2 的位置  //  // 遇到 2 就换到右边，但是换过来的不一定是 1 还是 0，所以 i 不能动  //  // 1 1 1 1 1 2 2 0  // ↑ ↑  // i right  //  // =========================  swap(nums, i, right--); } } } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } } "});index.add({'id':173,'href':'/docs/programmer-interview/algorithm/spiralmatrix/','title':"螺旋矩阵",'content':"螺旋矩阵 Spiral Matrix 和 Spiral Matrix II\n返回螺旋顺序 给定一个包含 m x n 个元素的矩阵（m 行, n 列），请按照顺时针螺旋顺序，返回矩阵中的所有元素。\nimport java.util.ArrayList; import java.util.Collections; import java.util.List; public class SpiralMatrix { public List\u0026lt;Integer\u0026gt; spiralOrder(int[][] matrix) { if (matrix == null || matrix.length == 0) { return Collections.emptyList(); } List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); int left = 0; int right = matrix[0].length - 1; int top = 0; int bottom = matrix.length - 1; while (true) { for (int c = left; c \u0026lt;= right; c++) { res.add(matrix[top][c]); } // top 不能超过 bottom  if (++top \u0026gt; bottom) { break; } for (int r = top; r \u0026lt;= bottom; r++) { res.add(matrix[r][right]); } // right 不能低于 left  if (--right \u0026lt; left) { break; } for (int c = right; c \u0026gt;= left; c--) { res.add(matrix[bottom][c]); } // bottom 不能低于 top  if (--bottom \u0026lt; top) { break; } for (int r = bottom; r \u0026gt;= top; r--) { res.add(matrix[r][left]); } // left 不能大于 right  if (++left \u0026gt; right) { break; } } return res; } } 生成螺旋矩阵 给定一个正整数 n，生成一个包含 1 到 n^2 所有元素，且元素按顺时针顺序螺旋排列的正方形矩阵。\n// 给定一个数n, 生成一个包含1~n^2 的螺旋矩阵 // (螺旋由外向内顺时针旋转, 可参照样例) // // 输入: 3 // 输出: // [ // [ 1, 2, 3 ], // [ 8, 9, 4 ], // [ 7, 6, 5 ] // ] public class SpiralMatrixII { public int[][] generateMatrix(int n) { int[][] array = new int[n][n]; int top = 0; int bottom = n - 1; int left = 0; int right = n - 1; int count = 1; while (true) { for (int i = left; i \u0026lt;= right; i++) { array[top][i] = count++; } if (++top \u0026gt; bottom) { break; } for (int i = top; i \u0026lt;= bottom; i++) { array[i][right] = count++; } if (--right \u0026lt; left) { break; } for (int i = right; i \u0026gt;= left; i--) { array[bottom][i] = count++; } if (--bottom \u0026lt; top) { break; } for (int i = bottom; i \u0026gt;= top; i--) { array[i][left] = count++; } if (++left \u0026gt; right) { break; } } return array; } } "});index.add({'id':174,'href':'/docs/programmer-interview/algorithm/splitarraylargestsum/','title':"分割数组的最大值",'content':"分割数组的最大值 描述 原题 给定一个非负整数数组和一个整数 m，你需要将这个数组分成 m 个非空的连续子数组。设计一个算法使得这 m 个子数组各自和的最大值最小。\n注意: 数组长度 n 满足以下条件:\n 1 ≤ n ≤ 1000 1 ≤ m ≤ min(50, n)  输入:\rnums = [7,2,5,10,8]\rm = 2\r输出:\r18\r解释:\r一共有四种方法将nums分割为2个子数组。\r其中最好的方式是将其分为[7,2,5] 和 [10,8]，\r因为此时这两个子数组各自的和的最大值为18，在所有情况中最小。\r题解 // 数组包含非负整数，以及一个 整数 m // 将数组分成 m 个连续的 subarray // 写一个算法，来使得所有的这些 subarrays 的最大和最小 // // [7,2,5,10,8] // // https://leetcode.com/problems/split-array-largest-sum/discuss/89817/Clear-Explanation%3A-8ms-Binary-Search-Java public class SplitArrayLargestSum { // [.......]  //  // max: 所有数相加  // min: Math.max(array) 最大的那个数  //  // 做一个二分搜索  public int splitArray(int[] nums, int m) { long sum = 0; int max = 0; for (int num: nums) { max = Math.max(max, num); sum += num; } // ============================  // 答案最少是这个最大的那个数，或者是所有数之和  // ============================  return (int) binary(nums, m, sum, max /** 最低的最大值是 max */); } private long binary(int[] nums, int m /** m 份 */, long high /** 最大值 */, long low /** 最小值 */) { long mid = 0; while (low \u0026lt; high) { mid = (high + low) / 2; // 每一份切成比 mid 刚刚小的值，切成 m 份可以做到  if (valid(nums, m, mid)) { high = mid; } else { // 切割不成，需要大于 m 份，证明 mid 选择小了  low = mid + 1; } } return high; } // 尽可能地去切分  // 每一份都切分为刚好比 max 小，如果可以做到，那么证明我们的最大值 max 小了，  // 因为每一份尽可能的去切分，依然可以做到，所以切成 m 份是不可能的  //  // 做不到，那么证明 mid 选大了  private boolean valid(int[] nums, int m /** m 份 */, long max /** 最大值 */) { int cur = 0; int count = 1; for (int num: nums) { cur += num; if (cur \u0026gt; max) { cur = num; count++; /** 大于 max 的子数组份数 \u0026gt; m 组 */ if (count \u0026gt; m) { return false; } } } return true; } } "});index.add({'id':175,'href':'/docs/programmer-interview/algorithm/sqrt/','title':"X 的平方根",'content':"X 的平方根 描述 原题 实现 int sqrt(int x) 函数。\n计算并返回 x 的平方根，其中 x 是非负整数。\n由于返回类型是整数，结果只保留整数的部分，小数部分将被舍去。\n题解 public class Sqrt { public int mySqrt(int x) { // 注意边界条件  if (x == 0) { return 0; } int lo = 1; int hi = x / 2; int ans = lo; while (lo \u0026lt;= hi) { int mid = lo + ((hi - lo) \u0026gt;\u0026gt; 1); // 注意这里  // 防止溢出  if (mid \u0026gt; x / mid) { hi = mid - 1; } else if (mid \u0026lt;= x / mid) { ans = mid; lo = mid + 1; } } return ans; } } "});index.add({'id':176,'href':'/docs/programmer-interview/algorithm/subarrayproductlessthank/','title':"乘积小于K的子数组",'content':"乘积小于K的子数组 描述 原题 给定一个正整数数组 nums。\n找出该数组内乘积小于 k 的连续的子数组的个数。\n示例 1:\n输入: nums = [10,5,2,6], k = 100\r输出: 8\r解释: 8个乘积小于100的子数组分别为: [10], [5], [2], [6], [10,5], [5,2], [2,6], [5,2,6]。\r需要注意的是 [10,5,2] 并不是乘积小于100的子数组。\r题解 // Input: nums = [10, 5, 2, 6], k = 100 // Output: 8 // Explanation: The 8 subarrays that have product less than 100 are: [10], [5], [2], [6], [10, 5], [5, 2], [2, 6], [5, 2, 6]. // Note that [10, 5, 2] is not included as the product of 100 is not strictly less than k. // // 0 \u0026lt; nums[i] \u0026lt; 1000 只包含正数 // // ============================== // 滑动窗口 // // 子数组乘积小于 k // [X, X, X, X, X, X, X, X] // ↑ ↑ // ============================== public class SubarrayProductLessThanK { public int numSubarrayProductLessThanK(int[] nums, int k) { if (k \u0026lt;= 1) { // 数组最小元素是 1，而且 less than 必须严格小于，因此 k = 0 的时候，直接返回 0 就行了  return 0; } int ans = 0; int left = 0; int product = 1; for (int right = 0; right \u0026lt; nums.length; right++) { product *= nums[right]; while (product \u0026gt;= k) { if (left \u0026gt;= nums.length) { return ans; } product /= nums[left++]; } // 1 2 3 4  // ↑  // left  // ↑  // right  //  // 多一个 right，那么就多 right - left + 1 个候选数组  ans += (right - left + 1); } return ans; } } "});index.add({'id':177,'href':'/docs/programmer-interview/algorithm/subarraysumequalsk/','title':"和为K的子数组",'content':"和为K的子数组 描述 给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。\n示例 1 :\n输入:nums = [1,1,1], k = 2\r输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。\r题解 // Given an array of integers and an integer k, you need to find the total number of continuous subarrays whose sum equals to k. // // 和为 k 的连续子数组，有多少组 public class SubarraySumEqualsK { public int subarraySum(int[] nums, int k) { int[] preSum = calcPreSum(nums); int count = 0; for (int i = 0; i \u0026lt; preSum.length - 1; i++) { for (int j = i + 1; j \u0026lt; preSum.length; j++) { // =======================  // [___|_________|___]  // ↑ k ↑  // i j  // =======================  if (preSum[j] - preSum[i] == k) { count++; } } } return count; } // 计算前缀子数组  // 非降序列  private int[] calcPreSum(int[] nums) { int[] preSum = new int[nums.length + 1]; for (int i = 1; i \u0026lt; preSum.length; i++) { preSum[i] = nums[i - 1] + preSum[i - 1]; } return preSum; } } "});index.add({'id':178,'href':'/docs/programmer-interview/algorithm/subarrayswithkdifferentintegers/','title':"K 个不同整数的子数组",'content':"K 个不同整数的子数组 描述 给定一个正整数数组 A，如果 A 的某个子数组中不同整数的个数恰好为 K，则称 A 的这个连续、不一定独立的子数组为好子数组。\n（例如，[1,2,3,1,2] 中有 3 个不同的整数：1，2，以及 3。）\n返回 A 中好子数组的数目。\n示例 1：\n输入：A = [1,2,1,2,3], K = 2\r输出：7\r解释：恰好由 2 个不同整数组成的子数组：[1,2], [2,1], [1,2], [2,3], [1,2,1], [2,1,2], [1,2,1,2].\r题解 import java.util.HashMap; import java.util.Map; // Input: A = [1,2,1,2,3], K = 2 // Output: 7 // Explanation: Subarrays formed with exactly 2 different integers: [1,2], [2,1], [1,2], [2,3], [1,2,1], [2,1,2], [1,2,1,2]. // // 子数组里面有 k 个不同的数字 // // 解法参考了 // https://leetcode.com/problems/subarrays-with-k-different-integers/discuss/235235/C%2B%2BJava-with-picture-prefixed-sliding-window // public class SubarrayswithKDifferentIntegers { public int subarraysWithKDistinct(int[] A, int K) { int ans = 0; int left = 0; Map\u0026lt;Integer /** 数字 */, Integer /** 这个数字出现的次数 */\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); //  // 前 prefix 个数，代表的就是有多少个重复的，也出现在当前这个窗口中  //  // 以下面这个为例，前 prefix = 2 个数也出现在后面的 [1, 2, 3] 窗口中  //  // [1, 2, 1, 2, 3]  // l r  //  // 这个 l 和 r 维护的区间刚好是 k 个，添加上前面 prefix 个也是可以的  int prefix = 0; // ===================  // 始终维护的是一个最紧凑的窗口，刚刚好能放下 k 个不同的数字  // ===================  for (int right = 0; right \u0026lt; A.length; right++) { map.put(A[right], map.getOrDefault(A[right], 0) + 1); // ===================  // 始终维护的是一个最紧凑的窗口，刚刚好能放下 k 个不同的数字  //  // [X, X, X, X, X, X, X]  // ↑ ↑  // 移动  // ===================  int size = map.size(); if (size \u0026gt; K) { map.remove(A[left++]); prefix = 0; } // [1, 2, 1, 2, 3]  // ↑ ↑  //  // left  // ↓  // [1, 2, 1, 2, 3]  //  // left  // ↓  // [1, 2, 1, 2, 3]  //  // [1, 2, 1, 2, 3]  //  // prefix = 2  while (map.get(A[left]) \u0026gt; 1) { map.put(A[left], map.get(A[left]) - 1); prefix++; left++; } if (map.size() == K) { ans += prefix + 1; } } return ans; } } "});index.add({'id':179,'href':'/docs/programmer-interview/algorithm/sum/','title':"2、3、4个数之和",'content':"2、3、4个数之和 从 nums 数组中找到数字相加的结果符合要求的几个数字。\n两数之和 import java.util.HashMap; import java.util.Map; // 返回索引 // // O(N) public class TwoSum { public int[] twoSum(int[] nums, int target) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { if (map.containsKey(target - nums[i])) { return new int[]{ map.get(target - nums[i]), i }; } map.put(nums[i], i); } return new int[]{}; } } 三个数之和 import java.util.List; import java.util.LinkedList; import java.util.ArrayList; import java.util.Arrays; public class ThreeSum { // ========================  // 方法一: 排序 + 二分搜索 O(n^2)  // ========================  public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] num) { Arrays.sort(num); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; num.length-2; i++) { if (i == 0 || (i \u0026gt; 0 \u0026amp;\u0026amp; num[i] != num[i-1])) { int lo = i + 1, hi = num.length - 1, sum = 0 - num[i]; while (lo \u0026lt; hi) { if (num[lo] + num[hi] == sum) { res.add(Arrays.asList(num[i], num[lo], num[hi])); while (lo \u0026lt; hi \u0026amp;\u0026amp; num[lo] == num[lo+1]) lo++; while (lo \u0026lt; hi \u0026amp;\u0026amp; num[hi] == num[hi-1]) hi--; lo++; hi--; } else if (num[lo] + num[hi] \u0026lt; sum) { lo++; } else { hi--; } } } } return res; } // ========================  // 方法二: 排序 + 二分搜索 O(n^2 * logn)  // ========================  public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum0(int[] nums) { Arrays.sort(nums); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length - 2; i++) { if (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) { continue; } List\u0026lt;int[]\u0026gt; twoSumList = twoSum(nums, i + 1, nums.length - 1, 0 - nums[i]); if (!twoSumList.isEmpty()) { for (int[] twoSumArray: twoSumList) { res.add(new ArrayList\u0026lt;\u0026gt;(Arrays.asList( nums[i], twoSumArray[0], twoSumArray[1] ))); } } } return res; } private List\u0026lt;int[]\u0026gt; twoSum(int[] nums, int lo, int hi, int target) { List\u0026lt;int[]\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = lo; i \u0026lt;= hi - 1; i++) { if (i \u0026gt; lo \u0026amp;\u0026amp; nums[i] == nums[i - 1]) { continue; } // target = a + b  int a = nums[i]; int bIndex = binarySearch(nums, i + 1, hi, target - a); if (bIndex != -1) { list.add(new int[]{ a, nums[bIndex] }); } } return list; } // 二分搜索  // =============================  // 用 HashSet 可以将查询复杂度降低到 O(1)  // =============================  private int binarySearch(int[] nums, int lo, int hi, int target) { while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } else if (nums[m] \u0026gt; target) { hi = m - 1; } else { lo = m + 1; } } return -1; } } 最接近 target 的三个数之和 import java.util.Arrays; // 3 个数字和最接近 target public class Sum3Closest { public int threeSumClosest(int[] nums, int target) { // O(nlogn)  Arrays.sort(nums); int min = nums[0] + nums[1] + nums[nums.length - 1]; // O(n^2)  for (int i = 0; i \u0026lt; nums.length - 2; i++) { int lo = i + 1; int hi = nums.length - 1; // =============================  // 这个地方不能是等于号  // =============================  while (lo \u0026lt; hi) { int sum = nums[i] + nums[lo] + nums[hi]; if (sum == target) { return sum; } else if (sum \u0026gt; target) { hi--; } else { lo++; } if (Math.abs(sum - target) \u0026lt; Math.abs(min - target)) { min = sum; } } } return min; } } 四个数之和 import java.util.ArrayList; import java.util.Arrays; import java.util.List; public class Sum4 { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; fourSum(int[] nums, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); Arrays.sort(nums); for (int i = 0; i \u0026lt; nums.length - 3; i++) { if (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) { continue; } for (int j = i + 1; j \u0026lt; nums.length - 2; j++) { if (j \u0026gt; (i + 1) \u0026amp;\u0026amp; nums[j] == nums[j - 1]) { continue; } for (int k = j + 1; k \u0026lt; nums.length - 1; k++) { if (k \u0026gt; (j + 1) \u0026amp;\u0026amp; nums[k] == nums[k - 1]) { continue; } int index = binarySearch(nums, k + 1, nums.length - 1, target - nums[i] - nums[j] - nums[k]); if (index != -1) { res.add(new ArrayList(Arrays.asList( nums[i], nums[j], nums[k], nums[index] ))); } } } } return res; } // =============================  // 用 HashSet 可以将查询复杂度降低到 O(1)  // =============================  private int binarySearch(int[] nums, int lo, int hi, int target) { while (lo \u0026lt;= hi) { int m = lo + ((hi - lo) \u0026gt;\u0026gt; 1); if (nums[m] == target) { return m; } else if (nums[m] \u0026gt; target) { hi = m - 1; } else { lo = m + 1; } } return -1; } } 四个数之和第二种解法 import java.util.HashMap; import java.util.Map; // A、B、C、D 是 4 个数组 // // 计算多少对 A[i] + B[j] + C[k] + D[l] = 0 public class Sum4_2 { public int fourSumCount(int[] A, int[] B, int[] C, int[] D) { int[] E = sumAAndB(A, B); Map\u0026lt;Integer, Integer\u0026gt; map = toMap(E); int count = 0; for (int i = 0; i \u0026lt; C.length; i++) { for (int j = 0; j \u0026lt; D.length; j++) { int target = -(C[i] + D[j]); if (map.containsKey(target)) { // 我们得到的就是有几个  //  count += map.get(target); } } } return count; } private Map\u0026lt;Integer, Integer\u0026gt; toMap(int[] A) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int num: A) { if (map.containsKey(num)) { map.put(num, map.get(num) + 1); } else { map.put(num, 1); } } return map; } private int[] sumAAndB(int[] A, int[] B) { int[] E = new int[A.length * B.length]; for (int i = 0; i \u0026lt; A.length; i++) { for (int j = 0; j \u0026lt; B.length; j++) { E[i * A.length + j] = A[i] + B[j]; } } return E; } } "});index.add({'id':180,'href':'/docs/programmer-interview/algorithm/sumofsubarrayminimums/','title':"子数组的最小值之和",'content':"子数组的最小值之和 描述 原题 给定一个整数数组 A，找到 min(B) 的总和，其中 B 的范围为 A 的每个（连续）子数组。\n由于答案可能很大，因此返回答案模 10^9 + 7。\n示例：\n输入：[3,1,2,4]\r输出：17\r解释：\r子数组为 [3]，[1]，[2]，[4]，[3,1]，[1,2]，[2,4]，[3,1,2]，[1,2,4]，[3,1,2,4]。 最小值为 3，1，2，4，1，1，2，1，1，1，和为 17。\r题解 import java.util.Stack; // // 连续子数组里面的最小值 min 相加的和 // [3,1,2,4]: // // [3][1][2][4][3,1][1,2][2,4][3,1,2][1,2,4][3,1,2,4] // public class SumofSubarrayMinimums { public int sumSubarrayMins(int[] A) { Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;Integer\u0026gt;(); // ==========================================  // 最左侧，第一个比 A[i] 小的值  // ==========================================  int[] leftMin = new int[A.length]; for (int i = 0; i \u0026lt; A.length; i++) { // ↓  // 这两个地方至少要有一个添加一个等于号, 否则就会遗漏  while (!stack.isEmpty() \u0026amp;\u0026amp; A[i] \u0026lt;= A[stack.peek()]) { stack.pop(); } leftMin[i] = stack.isEmpty() ? -1 : stack.peek(); stack.push(i); } // ==========================================  // 最右侧，第一个比 A[i] 小的值  // ==========================================  int[] rightMin = new int[A.length]; stack.clear(); for (int i = A.length - 1; i \u0026gt;= 0; i--) { // ↓  // 这两个地方至少要有一个添加一个等于号  while (!stack.isEmpty() \u0026amp;\u0026amp; A[i] \u0026lt; A[stack.peek()]) { stack.pop(); } rightMin[i] = stack.isEmpty() ? A.length : stack.peek(); stack.push(i); } long sum = 0; for (int i = 0; i \u0026lt; A.length; i++) { // =======================  // 这两段区间内的 subarray 有多少个 A[i] (min)  // =======================  sum += (A[i] * (i - leftMin[i]) * (rightMin[i] - i)); } return (int)(sum % (1e9 + 7)); } } "});index.add({'id':181,'href':'/docs/programmer-interview/algorithm/thirdmaximumnumber/','title':"第三大的数",'content':"第三大的数 描述 给定一个非空数组，返回此数组中第三大的数。如果不存在，则返回数组中最大的数。要求算法时间复杂度必须是 O(n)。\n示例 1:\n输入: [3, 2, 1]\r输出: 1\r解释: 第三大的数是 1.\r题解 // 返回第三大，如果不存在，返回最大的 // // Input: [3, 2, 1] // Output: 1 // Explanation: The third maximum is 1. // // Input: [2, 2, 3, 1] // Output: 1 // Explanation: Note that the third maximum here means the third maximum distinct number. // Both numbers with value 2 are both considered as second maximum. // // https://leetcode.com/problems/third-maximum-number/ public class ThirdMaximumNumber { public int thirdMax(int[] nums) { Integer maximumA = null; Integer maximumB = null; Integer maximumC = null; for (int num: nums) { if (maximumA == null || num == maximumA.intValue()) { maximumA = num; continue; } else if (num \u0026gt; maximumA.intValue()) { // A: 3, num: 5  maximumC = maximumB; maximumB = maximumA; maximumA = num; } else if (num \u0026lt; maximumA.intValue()) { if (maximumB == null || num == maximumB.intValue()) { maximumB = num; continue; } else if (num \u0026gt; maximumB) { maximumC = maximumB; maximumB = num; } else { if (maximumC == null || num == maximumC.intValue()) { maximumC = num; continue; } else { maximumC = Math.max(num, maximumC); } } } } return maximumC == null ? maximumA.intValue() : maximumC.intValue(); } } "});index.add({'id':182,'href':'/docs/programmer-interview/algorithm/topklargestnumbers/','title':"前K大数",'content':"前K大数 描述 原题 在一个数组中找到前K大的数\n题解 // https://www.lintcode.com/problem/top-k-largest-numbers/description // // Input: [3, 10, 1000, -99, 4, 100] and k = 3 // Output: [1000, 100, 10] import java.util.*; public class TopkLargestNumbers { public int[] topk(int[] nums, int k) { PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o1.compareTo(o2); } }); for (int num: nums) { queue.offer(num); if (queue.size() \u0026gt; k) { queue.poll(); } } // addFirst();  // addLast();  // removeFirst();  // removeLast();  int[] res = new int[queue.size()]; for (int i = queue.size() - 1; i \u0026gt;= 0; i--) { res[i] = queue.poll().intValue(); } // Error: int[] res = new int[list.size()];  //  // Integer[] res = new Integer[list.size()];  // list.toArray(res);  return res; } } "});index.add({'id':183,'href':'/docs/programmer-interview/algorithm/validmountainarray/','title':"有效的山脉数组",'content':"有效的山脉数组 原题 给定一个整数数组 A，如果它是有效的山脉数组就返回 true，否则返回 false。\n让我们回顾一下，如果 A 满足下述条件，那么它是一个山脉数组：\n A.length \u0026gt;= 3 在 0 \u0026lt; i \u0026lt; A.length - 1 条件下，存在 i 使得：  A[0] \u0026lt; A[1] \u0026lt; ... A[i-1] \u0026lt; A[i] A[i] \u0026gt; A[i+1] \u0026gt; ... \u0026gt; A[A.length - 1]    题解 // https://leetcode.com/problems/valid-mountain-array/ // // (1) A.length \u0026gt;= 3 // (2) There exists some i with 0 \u0026lt; i \u0026lt; A.length - 1 such that: // A[0] \u0026lt; A[1] \u0026lt; ... A[i-1] \u0026lt; A[i] // A[i] \u0026gt; A[i+1] \u0026gt; ... \u0026gt; A[B.length - 1] // public class ValidMountainArray { public boolean validMountainArray(int[] A) { if (A == null || A.length \u0026lt; 3) { return false; } int stopIndex = -1; for (int i = 1; i \u0026lt; A.length; i++) { if (A[i - 1] \u0026lt; A[i]) { continue; } else { stopIndex = i - 1; break; } } if (stopIndex == 0 /** 5 (stop) 1 2 3 4 */ || stopIndex == -1 /** 1 2 3 4 5, never stop */) { return false; } for (int i = stopIndex; i \u0026lt; A.length - 1; i++) { if (A[i] \u0026gt; A[i + 1]) { continue; } else { return false; } } return true; } } "});index.add({'id':184,'href':'/docs/programmer-interview/algorithm/validsudoku/','title':"有效的数独",'content':"有效的数独 描述 原题 判断一个 9x9 的数独是否有效。只需要根据以下规则，验证已经填入的数字是否有效即可。\n 数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。  题解 import java.util.HashMap; import java.util.HashSet; import java.util.Map; import java.util.Set; // https://leetcode.com/problems/valid-sudoku/ // // public class ValidSudoku { public boolean isValidSudoku(char[][] board) { int m = board.length; int n = board[0].length; Map\u0026lt;Integer, Set\u0026lt;Integer\u0026gt;\u0026gt; rowMap = new HashMap\u0026lt;\u0026gt;(); Map\u0026lt;Integer, Set\u0026lt;Integer\u0026gt;\u0026gt; colMap = new HashMap\u0026lt;\u0026gt;(); // 1 2 3  // 4 5 6  // 7 8 9  Map\u0026lt;Integer, Set\u0026lt;Integer\u0026gt;\u0026gt; smallBoxMap = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { char c = board[i][j]; if (c == \u0026#39;.\u0026#39;) { continue; } int num = c - \u0026#39;0\u0026#39;; Set\u0026lt;Integer\u0026gt; rowSet = getOrCreate(rowMap, i + 1); if (!rowSet.add(num)) { return false; } Set\u0026lt;Integer\u0026gt; colSet = getOrCreate(colMap, j + 1); if (!colSet.add(num)) { return false; } int boxPos = 3 * (i / 3) + ((j / 3) % 3 + 1); Set\u0026lt;Integer\u0026gt; smallBoxSet = getOrCreate(smallBoxMap, boxPos); if (!smallBoxSet.add(num)) { return false; } } } return true; } private Set\u0026lt;Integer\u0026gt; getOrCreate(Map\u0026lt;Integer, Set\u0026lt;Integer\u0026gt;\u0026gt; map, int key) { if (map.containsKey(key)) { return map.get(key); } Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); map.put(key, set); return set; } } "});index.add({'id':185,'href':'/docs/programmer-interview/algorithm/validtrianglenumber/','title':"有效三角形的个数",'content':"有效三角形的个数 描述 原题 给定一个包含非负整数的数组，你的任务是统计其中可以组成三角形三条边的三元组个数。\n示例 1:\n输入: [2,2,3,4]\r输出: 3\r解释:\r有效的组合是: 2,3,4 (使用第一个 2)\r2,3,4 (使用第二个 2)\r2,2,3\r题解 import java.util.Arrays; // 给定一个包含非负整数的数组，你的任务是计算从数组中选出的可以制作三角形的三元组数目，如果我们把它们作为三角形的边长。 // 非负 // // https://www.lintcode.com/problem/valid-triangle-number/description // public class ValidTriangleNumber { // ERROR: O(n ^ 2)，错误解法  public int triangleNumber(int[] nums) { // a + b \u0026gt; c  // c \u0026lt; a + b  //  // 1 2 3 4  Arrays.sort(nums); int count = 0; // 2 2 3 4  // ↑  // 2, 2, 4  // 2, 3, 4  // 2, 3, 4  // 2, 2, 3  for (int i = 0; i \u0026lt; nums.length - 2; i++) { int lo = i + 1; int hi = nums.length - 1; // 0 1 2 3 4 5 6 7  // ↑ ↑  // ↑ ↑  while (lo \u0026lt; hi) { // 最短两边 \u0026gt; 最长一边  // 2 2 3 4  // ↑ (2 + 2 不大于 4, 这个时候增加 lo 和减少 hi 都可能成立，所以无法确定)  // ======================  // ERROR  // ======================  if (nums[lo] + nums[i] \u0026gt; nums[hi]) { count += (hi - lo); lo++; } else { hi--; } } } return count; } // O(n ^ 2) 正确解法  public int triangleNumber0(int[] nums) { // a + b \u0026gt; c  // c \u0026lt; a + b  //  // 1 2 3 4  Arrays.sort(nums); int count = 0; // nums[i] 代表的就是最长一边  for (int i = nums.length - 1; i \u0026gt;= 2; i--) { int lo = 0; int hi = i - 1; // 0 1 2 3 4 5 6 7  // ↑ ↑  // ↑ ↑  while (lo \u0026lt; hi) { // 最短两边 \u0026gt; 最长一边  if (nums[lo] + nums[hi] \u0026gt; nums[i]) { count += (hi - lo); hi--; } else { lo++; } } } return count; } // O(n ^ 3)  public int triangleNumber1(int[] nums) { // a + b \u0026gt; c  // c \u0026lt; a + b  //  // 1 2 3 4  Arrays.sort(nums); int count = 0; for (int i = 0; i \u0026lt; nums.length - 2; i++) { for (int j = i + 1; j \u0026lt; nums.length - 1; j++) { for (int k = j + 1; k \u0026lt; nums.length; k++) { if (nums[i] + nums[j] \u0026gt; nums[k]) { count++; } } } } return count; } } "});index.add({'id':186,'href':'/docs/programmer-interview/algorithm/combinationsum/','title':"组合总和",'content':"组合总和 下属题目，所有数字均是正整数\nCombination Sum 原题 一个数组，从这个数组中找出所有相加等于 target 的元素的组合，数组中的每一个数字可以被多次重复选取。\n 数组无重复数字。\n import java.util.ArrayList; import java.util.List; // Input: candidates = [2,3,5], target = 8, // A solution set is: // [ // [2,2,2,2], // [2,3,3], // [3,5] // ] // // candidates 里面没有重复数字 // 每一个 candidate 可以重复使用多次 // // 时间复杂度可以转为，见这个帖子分析 // https://leetcode.com/problems/combination-sum/discuss/16634/if-asked-to-discuss-the-time-complexity-of-your-solution-what-would-you-say // // 每个元素有 ceil(target / element) 个 // 展开后的数组个数为 n\u0026#39; = ceil(target / element) * n // 最后时间复杂度为 O(k * 2 ^ n\u0026#39;) // public class CombinationSum { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); helper(candidates, res, new ArrayList\u0026lt;Integer\u0026gt;(), 0, 0, target); return res; } private void helper(int[] candidates, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, List\u0026lt;Integer\u0026gt; cur, int curSum, int index, int target) { if (curSum \u0026gt; target) { return; } if (curSum == target) { res.add(new ArrayList\u0026lt;\u0026gt;(cur)); return; } for (int i = index; i \u0026lt; candidates.length; i++) { cur.add(candidates[i]); // 之所以不是 i + 1 的原因是因为这道题的数字: 可重复使用  helper(candidates, res, cur, curSum + candidates[i], i, target); cur.remove(cur.size() - 1); } } } Combination Sum 2 原题 这次增加了限制条件：\n 数组可能包含重复数字 数组中的每个数字只能被选取一次，不能多次重复使用  import java.util.ArrayList; import java.util.Arrays; import java.util.List; // Input: candidates = [10,1,2,7,6,1,5], target = 8, // A solution set is: // [ // [1, 7], // [1, 2, 5], // [2, 6], // [1, 1, 6] // ] // // candidates 可能有重复数字 // 所有数字只让用 1 次 // // 时间复杂度: // 假设一个 solution 平均需要 k 个数字才能解决，那么 // 时间复杂度为 O(k * 2 ^ n)，我们需要 O(k) 时间来拷贝结果，总共 2^n 次方的选择方式 // // 我们从 n 个数选择 1 个，有多少组 // 我们从 n 个数选择 2 个，有多少组 // 我们从 n 个数选择 3 个，有多少组 // // C(n, 0) + C(n, 1) + C(n, 2) + ... + C(n, n) = 2 ^ n 一共这么多可能的解决方案 // // 0, 0, 0, 0, 0, 0, 0, 0 public class CombinationSum2 { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum2(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); Arrays.sort(candidates); helper(candidates, res, new ArrayList\u0026lt;Integer\u0026gt;(), 0, 0, target); return res; } private void helper(int[] candidates, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, List\u0026lt;Integer\u0026gt; cur, int curSum, int index, int target) { if (curSum \u0026gt; target) { return; } // 第一种情况:  //  // 这种情况会多跳过，相当于多层递归之间有交叉的部分  //  // if (index - 1 \u0026gt;= 0 \u0026amp;\u0026amp; candidates[index - 1] == candidates[index]) {  // return;  // }  // ============================  // 结束判断优先于越界判断  // ============================  if (curSum == target) { res.add(new ArrayList\u0026lt;\u0026gt;(cur)); return; } // ============================  // 越界判断低于结束判断  // ============================  if (index \u0026gt;= candidates.length) { return; } for (int i = index; i \u0026lt; candidates.length; i++) { // 以这个为例  // 1 1 6 7  //  // index = 0, i = 0 A  // index = 1, i = 1 B [如果使用了第一种情况，那么相当与 B 轮和 A 轮连在一起了，B 轮的 1 == A 轮的 1，所以 return 了]  // index = 2, i = 2  // index = 3, i = 3  // index = 2, i = 3  // index = 1, i = 2  // index = 3, i = 3  // index = 1, i = 3  //  // ↓ ↓  // index = 0, i = 1 [1, 1, 6, 7] 跳过了，两个 1 使用了 1 个。也就是说这是单就这一轮的限制，这是平级的限制，没有 A 轮和 B 轮之间的交叉限制  // index = 2, i = 2  // index = 3, i = 3  // index = 2, i = 3  //  // index = 0, i = 2  // index = 3, i = 3  //  // index = 0, i = 3  if (i \u0026gt; index \u0026amp;\u0026amp; candidates[i] == candidates[i - 1]) { continue; } cur.add(candidates[i]); helper(candidates, res, cur, curSum + candidates[i], i + 1, target); cur.remove(cur.size() - 1); } } } Combination Sum 3 找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。\n 这次还限定了可使用元素的个数  import java.util.ArrayList; import java.util.List; // Input: k = 3, n = 9 // Output: [[1,2,6], [1,3,5], [2,3,4]] // // 3 个数相加等于 9 // // 时间复杂度 C(n, k) * O(k) public class CombinationSum3 { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum3(int k, int n) { int[] array = buildArray(9); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;(); helper(res, array, new ArrayList\u0026lt;\u0026gt;(), k, n, 0, 0); return res; } private void helper(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, int[] nums, List\u0026lt;Integer\u0026gt; cur, int k, int sum, int curSum, int begin) { if (cur.size() == k \u0026amp;\u0026amp; curSum == sum) { res.add(new ArrayList\u0026lt;\u0026gt;(cur)); return; } if (cur.size() \u0026gt; k || curSum \u0026gt; sum) { return; } for (int i = begin; i \u0026lt; nums.length; i++) { cur.add(nums[i]); helper(res, nums, cur, k, sum, nums[i] + curSum, i + 1); cur.remove(cur.size() - 1); } } private int[] buildArray(int n) { int[] array = new int[n]; for (int i = 1; i \u0026lt;= n; i++) { array[i - 1] = i; } return array; } } "});index.add({'id':187,'href':'/docs/programmer-interview/algorithm/combinations/','title':"组合",'content':"组合 描述 原题 给定两个整数 n 和 k，返回 1 \u0026hellip; n 中所有可能的 k 个数的组合。\n示例:\n输入: n = 4, k = 2\r输出:\r[\r[2,4],\r[3,4],\r[2,3],\r[1,2],\r[1,3],\r[1,4],\r]\r题解 import java.util.ArrayList; import java.util.List; // Input: n = 4, k = 2 // Output: // [ // [2,4], // [3,4], // [2,3], // [1,2], // [1,3], // [1,4], // ] // // 时间复杂度看起来像是 C(n, k)，从 n 个数里面选择 k 个 // 所以总的时间复杂度为 C(n, k) * O(k) public class Combinations { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); int[] array = buildArray(n); helper(res, new ArrayList\u0026lt;Integer\u0026gt;(), array, k, 0); return res; } private void helper(List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res, List\u0026lt;Integer\u0026gt; cur, int[] array, int k, int begin) { // Base Case  // 得到之后，需要花费 O(K) 来拷贝结果到 res 里面  //  if (cur.size() == k) { res.add(new ArrayList\u0026lt;\u0026gt;(cur)); return; } for (int i = begin; i \u0026lt; array.length; i++) { cur.add(array[i]); helper(res, cur, array, k, i + 1); cur.remove(cur.size() - 1); } } private int[] buildArray(int n) { int[] array = new int[n]; for (int i = 1; i \u0026lt;= n; i++) { array[i - 1] = i; } return array; } } "});index.add({'id':188,'href':'/docs/programmer-interview/algorithm/differentwaystoaddparentheses/','title':"为运算表达式设计优先级",'content':"为运算表达式设计优先级 描述 原题 给定一个含有数字和运算符的字符串，为表达式添加括号，改变其运算优先级以求出不同的结果。你需要给出所有可能的组合的结果。有效的运算符号包含 +, - 以及 * 。\n输入: \u0026quot;2-1-1\u0026quot;\r输出: [0, 2]\r解释: ((2-1)-1) = 0 (2-(1-1)) = 2\r 头条面试题\n 题解 import java.util.LinkedList; import java.util.List; // 头条面试题 // // https://leetcode.com/problems/different-ways-to-add-parentheses/ // Input: \u0026#34;2*3-4*5\u0026#34; // Output: [-34, -14, -10, -10, 10] // Explanation: // (2*(3-(4*5))) = -34 // ((2*3)-(4*5)) = -14 // ((2*(3-4))*5) = -10 // (2*((3-4)*5)) = -10 // (((2*3)-4)*5) = 10 // // 只有 +、- 和 * // // 有几种不同的结果 public class DifferentWaysToAddParentheses { public List\u0026lt;Integer\u0026gt; diffWaysToCompute(String input) { List\u0026lt;Integer\u0026gt; ret = new LinkedList\u0026lt;Integer\u0026gt;(); for (int i = 0; i \u0026lt; input.length(); i++) { if (input.charAt(i) == \u0026#39;-\u0026#39; || input.charAt(i) == \u0026#39;*\u0026#39; || input.charAt(i) == \u0026#39;+\u0026#39; ) { String part1 = input.substring(0, i); String part2 = input.substring(i + 1); List\u0026lt;Integer\u0026gt; part1Ret = diffWaysToCompute(part1); List\u0026lt;Integer\u0026gt; part2Ret = diffWaysToCompute(part2); for (Integer p1:part1Ret) { for (Integer p2:part2Ret) { int c = 0; switch (input.charAt(i)) { case \u0026#39;+\u0026#39;: c = p1 + p2; break; case \u0026#39;-\u0026#39;: c = p1 - p2; break; case \u0026#39;*\u0026#39;: c = p1 * p2; break; } ret.add(c); } } } } if (ret.size() == 0) { // input 是一个单一数字  ret.add(Integer.valueOf(input)); } return ret; } } "});index.add({'id':189,'href':'/docs/programmer-interview/algorithm/game24/','title':"快算 24",'content':"快算 24 描述 原题 你有 4 张写有 1 到 9 数字的牌。你需要判断是否能通过 *，/，+，-，(，) 的运算得到 24。\n题解 import java.util.ArrayList; // Input: [4, 1, 8, 7] // Output: True // Explanation: (8-4) * (7-1) = 24 // // https://leetcode.com/articles/24-game/ public class Game24 { public boolean judgePoint24(int[] nums) { ArrayList A = new ArrayList\u0026lt;Double\u0026gt;(); for (int v: nums) { A.add((double) v); } return solve(A); } private boolean solve(ArrayList\u0026lt;Double\u0026gt; nums) { if (nums.size() == 0) return false; if (nums.size() == 1) return Math.abs(nums.get(0) - 24) \u0026lt; 1e-6; // ================================  // 挑选两个数  // ================================  for (int i = 0; i \u0026lt; nums.size(); i++) { for (int j = 0; j \u0026lt; nums.size(); j++) { if (i != j) { // ====================================  // 任意取出两个数 nums[i] 和 nums[j]  //  // 把剩余的那没有用到的数字放到 nums2 里面  // ====================================  ArrayList\u0026lt;Double\u0026gt; nums2 = new ArrayList\u0026lt;Double\u0026gt;(); for (int k = 0; k \u0026lt; nums.size(); k++) if (k != i \u0026amp;\u0026amp; k != j) { nums2.add(nums.get(k)); } // 运算符号 4 种  // 对取出来的 nums[i] 和 nums[j] 进行不同的四则运算  // 将所得结果重新放入到 nums2 里面  // 然后递归 nums2 这个新的数组  //  for (int k = 0; k \u0026lt; 4; k++) { if (k \u0026lt; 2 \u0026amp;\u0026amp; j \u0026gt; i) continue; if (k == 0) nums2.add(nums.get(i) + nums.get(j)); if (k == 1) nums2.add(nums.get(i) * nums.get(j)); if (k == 2) nums2.add(nums.get(i) - nums.get(j)); if (k == 3) { if (nums.get(j) != 0) { nums2.add(nums.get(i) / nums.get(j)); } else { continue; } } if (solve(nums2)) return true; nums2.remove(nums2.size() - 1); } } } } return false; } } "});index.add({'id':190,'href':'/docs/programmer-interview/algorithm/generateparentheses/','title':"生成括号对",'content':"生成括号对 描述 原题 数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。\n输入：n = 3\r输出：[\r\u0026quot;((()))\u0026quot;,\r\u0026quot;(()())\u0026quot;,\r\u0026quot;(())()\u0026quot;,\r\u0026quot;()(())\u0026quot;,\r\u0026quot;()()()\u0026quot;\r]\r题解 import java.util.LinkedList; import java.util.List; // 时间复杂度，总的括号对个数是 Catalan number 个 // O(C(2n, n) / (n + 1)) // public class GenerateParentheses { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); helper(res, \u0026#34;\u0026#34;, 0, 0, n); return res; } private void helper(List\u0026lt;String\u0026gt; res, String cur, int open, int close, int n) { if (open == n \u0026amp;\u0026amp; close == n) { res.add(cur); return; } if (open \u0026lt; n) { helper(res, cur + \u0026#34;(\u0026#34;, open + 1, close, n); } if (close \u0026lt; open) { helper(res, cur + \u0026#34;)\u0026#34;, open, close + 1, n); } } } "});index.add({'id':191,'href':'/docs/programmer-interview/algorithm/lettercasepermutation/','title':"字母大小写全排列",'content':"字母大小写全排列 描述 给定一个字符串S，通过将字符串S中的每个字母转变大小写，我们可以获得一个新的字符串。返回所有可能得到的字符串集合。\n题解 import java.util.ArrayList; import java.util.List; // Examples: // Input: S = \u0026#34;a1b2\u0026#34; // Output: [\u0026#34;a1b2\u0026#34;, \u0026#34;a1B2\u0026#34;, \u0026#34;A1b2\u0026#34;, \u0026#34;A1B2\u0026#34;]  // Input: S = \u0026#34;3z4\u0026#34; // Output: [\u0026#34;3z4\u0026#34;, \u0026#34;3Z4\u0026#34;]  // Input: S = \u0026#34;12345\u0026#34; // Output: [\u0026#34;12345\u0026#34;] // // 假设字符串中包含 k 个需要转的字符 // 那么总共有 2 ^ k 个状态，base case 需要拷贝，那么就是 O(n) // 所以复杂度为 O(n * 2^k) public class LetterCasePermutation { public List\u0026lt;String\u0026gt; letterCasePermutation(String S) { List\u0026lt;String\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); helper(res, S.toCharArray(), 0); return res; } private void helper(List\u0026lt;String\u0026gt; res, char[] array, int index) { if (index == array.length) { res.add(new String(array)); return; } // 这个地方不用 for 循环了  //  // a 1 b 2  // ↑  // ↑  // ↑  // ↑  // a 1 b 2  // ↑  // a 1 b 2  // ↑  // a 1 b 2  // ↑  if ((array[index] \u0026gt;= \u0026#39;a\u0026#39; \u0026amp;\u0026amp; array[index] \u0026lt;= \u0026#39;z\u0026#39;) || (array[index] \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; array[index] \u0026lt;= \u0026#39;Z\u0026#39;)) { array[index] = toUpperCase(array[index]); helper(res, array, index + 1); array[index] = toLowerCase(array[index]); helper(res, array, index + 1); } else { helper(res, array, index + 1); } } private char toLowerCase(char c) { return c \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; c \u0026lt;= \u0026#39;Z\u0026#39; ? (char)(c + 32) : c; } private char toUpperCase(char c) { return c \u0026gt;= \u0026#39;a\u0026#39; \u0026amp;\u0026amp; c \u0026lt;= \u0026#39;z\u0026#39; ? (char)(c - 32) : c; } } "});index.add({'id':192,'href':'/docs/programmer-interview/algorithm/lettercombinationsofaphonenumber/','title':"电话号码的字母组合",'content':"电话号码的字母组合 描述 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。\n给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。\n题解 import java.util.HashMap; import java.util.LinkedList; import java.util.List; import java.util.Map; // 时间复杂度为 O(3 ^ N * 4 ^ M) 次方个 // // 每一个数有 3 种选法的有 N 个 // 每一个数有 4 种选法的有 M 个 // public class LetterCombinationsofaPhoneNumber { static Map\u0026lt;Character, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); static { map.put(\u0026#39;2\u0026#39;, \u0026#34;abc\u0026#34;); map.put(\u0026#39;3\u0026#39;, \u0026#34;def\u0026#34;); map.put(\u0026#39;4\u0026#39;, \u0026#34;ghi\u0026#34;); map.put(\u0026#39;5\u0026#39;, \u0026#34;jkl\u0026#34;); map.put(\u0026#39;6\u0026#39;, \u0026#34;mno\u0026#34;); map.put(\u0026#39;7\u0026#39;, \u0026#34;pqrs\u0026#34;); map.put(\u0026#39;8\u0026#39;, \u0026#34;tuv\u0026#34;); map.put(\u0026#39;9\u0026#39;, \u0026#34;wxyz\u0026#34;); } public List\u0026lt;String\u0026gt; letterCombinations(String digits) { List\u0026lt;String\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); if (digits == null || digits.length() == 0) { return res; } helper(res, digits, 0, \u0026#34;\u0026#34;); return res; } private void helper(List\u0026lt;String\u0026gt; res, String digits, int index, String cur) { if (index == digits.length()) { res.add(cur); } else { String chs = map.get(digits.charAt(index)); for (char c: chs.toCharArray()) { helper(res, digits, index + 1, cur + c); } } } } "});index.add({'id':193,'href':'/docs/programmer-interview/algorithm/nqueues/','title':"N 皇后",'content':"N 皇后 描述 原题 n 皇后问题研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。\n题解 import java.util.ArrayList; import java.util.Arrays; import java.util.LinkedList; import java.util.List; public class NQueues { public List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; solveNQueens(int n) { char[][] grid = new char[n][n]; for (int i = 0; i \u0026lt; n; i++) { Arrays.fill(grid[i], \u0026#39;.\u0026#39;); } List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; res = new ArrayList\u0026lt;List\u0026lt;String\u0026gt;\u0026gt;(); helper(res, grid, 0, n); return res; } // 方法是对于固定的列 colIndex，依次试探第 0 1 2 ... n 行，然后看是否 OK，如果 OK，试探 colIndex + 1 列  // colIndex == n 的时候，就可以返回了  private void helper(List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; res, char[][] grid, int colIndex /** 对于固定的列 */, int n) { if (colIndex == n) { res.add(construct(grid)); return; } for (int r = 0; r \u0026lt; n; r++) { // 对于第 r 行，试探着放第 0、1、2、...、n 列，然后看是否 valid，  // valid 则用 \u0026#39;Q\u0026#39; 替代，然后继续放  if (isValid(grid, r, colIndex, n)) { grid[r][colIndex] = \u0026#39;Q\u0026#39;; helper(res, grid, colIndex + 1, n); grid[r][colIndex] = \u0026#39;.\u0026#39;; } } } private boolean isValid(char[][] grid, int r, int c, int n) { for (int i = 0; i \u0026lt; n; i++) { // 行  if (grid[r][i] == \u0026#39;Q\u0026#39;) { return false; } // 列  if (grid[i][c] == \u0026#39;Q\u0026#39;) { return false; } } return isDiagonalValid(grid, r, c, n); // 斜坡  } private boolean isDiagonalValid(char[][] grid, int r, int c, int n) { for (int i = r + 1, j = c + 1; i \u0026lt; n \u0026amp;\u0026amp; j \u0026lt; n; i++, j++) { if (grid[i][j] == \u0026#39;Q\u0026#39;) { return false; } } for (int i = r - 1, j = c - 1; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0; i--, j--) { if (grid[i][j] == \u0026#39;Q\u0026#39;) { return false; } } for (int i = r + 1, j = c - 1; i \u0026lt; n \u0026amp;\u0026amp; j \u0026gt;= 0; i++, j--) { if (grid[i][j] == \u0026#39;Q\u0026#39;) { return false; } } for (int i = r - 1, j = c + 1; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; n; i--, j++) { if (grid[i][j] == \u0026#39;Q\u0026#39;) { return false; } } return true; } private List\u0026lt;String\u0026gt; construct(char[][] board) { List\u0026lt;String\u0026gt; res = new LinkedList\u0026lt;String\u0026gt;(); for(int i = 0; i \u0026lt; board.length; i++) { String s = new String(board[i]); res.add(s); } return res; } } "});index.add({'id':194,'href':'/docs/programmer-interview/front-end/css-center/','title':"CSS 垂直居中",'content':"CSS 垂直居中 有固定的高度和宽度 主要是依靠 absolute 属性置于距离左上角 50% 的位置，然后再利用 margin 调整位置。\n.parent { position: relative; } .child { width: 300px; height: 100px; padding: 20px; position: absolute; top: 50%; left: 50%; margin: -70px 0 0 -170px; } 效果如下  假如不添加 margin   无固定的高度和宽度 使用 transform 属性：\n.parent { position: relative; } .child { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); } 使用 flexbox 布局 .container { display: flex; justify-content: center; align-items: center; } 单行文本水平垂直居中 transform .container { position: absolute; top: 50%; left: 50%; transform: translateX(-50%) translateY(-50%); } Flexbox .container { display: flex; align-items: center; justify-content: center; } table-cell .parent { width: 100%; height: 100%; display: table; text-align: center; } .parent \u0026gt; .child { display: table-cell; vertical-align: middle; } absolute .container { position: relative; text-align: center; } .container \u0026gt; p { position: absolute; top: 50%; left: 0; right: 0; margin-top: -9px; } line-height (不推荐) .parent { height: 200px; width: 400px; text-align: center; } .parent \u0026gt; .child { line-height: 200px; } 多行文本水平垂直居中 参考  CSS center text (horizontally and vertically) inside a div block  "});index.add({'id':195,'href':'/docs/tutorial/git/git-ignore/','title':"Git .gitignore 文件",'content':"Git .gitignore 文件 Git 可以使用 .gitignore 文件来对工作区的某个目录、某个文件等设置忽略，忽略后这些文件的状态变化，将不会被记录在 git 中，也不会被 push 到远程服务器上。\n如果想要忽略项目里面的某些文件夹，比如 build/、target/、node_modules/ 等文件夹，不 push 到服务器上，就需要在相应的目录中添加一个 .gitignore 文件，并在里面将这些文件夹的名字给加上。\n.gitignore 的作用范围 作用范围：.gitignore 文件所处的目录及其子目录。\n如何查看哪些文件被忽略了 git status --ignored # 或 git check-ignore -v example.jpg .gitignore 文件语法  # 开始的行代表注释 *：代表任意多个字符，?：代表一个字符，[abc] 代表可选字符范围等 **：匹配任意数量的目录 名称以 / 开头：只忽略此目录下的文件，对于子目录中的文件不忽略 名称以 / 结尾：忽略整个目录，同名文件不忽略；否则同名文件和目录都被忽略 名称以 ! 开头：代表不忽略这个文件  示例 # 任何目录下面的 .DS_Store 文件都会被忽略 .DS_Store # 忽略整个目录 node_modules/ logs/ # 忽略所有以 log 结尾的文件，但是 example.log 不被忽略 *.log !example.log # 忽略 abc 文件夹下面的以 log 结尾的文件，注意：子目录不会被忽略 abc/*.log # 忽略 abc 文件夹以及所有子目录下面的以 log 结尾的文件 abc/**/*.log 快速生成一份 .gitingore 忽略文件 如果你不知道应该忽略哪些文件，那么请访问 gitignore.io 网站，通过输入你的框架名称、IDE 名称、开发语言等来生成一份 .gitignore 文件。\n参考  How to Use a .gitignore File  扫描下面二维码，在手机端阅读：\n"});index.add({'id':196,'href':'/docs/programmer-interview/java/java-gc/','title':"Java 垃圾回收器",'content':"Java 垃圾回收器 判断对象是否可回收 如何判断一个对象属于垃圾对象呢？\n引用计数法 对于一个对象 A，只要有任意一个对象引用了 A，则 A 的计数器加 1，当引用失效的时候，引用计数器就减 1。如果 A 的应用计数器为 0，则对象 A 就不可能再被使用。\n 缺点：无法处理循环引用的问题。\n 可达性分析算法 通过一系列的称为 GC Roots 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的循环称为引用链。当一个对象到 GC Roots 没有任何引用链的时候，则证明此对象是不可达的，因此它们会被判定为可回收对象。\n可以作为 GC Roots 的对象：\n 类静态属性中引用的对象 常量引用的对象 虚拟机栈中引用的对象 本地方法栈中引用的对象  垃圾回收算法 标记-清除算法 从每个 GC Roots 对象出发，依次标记有引用关系的对象，最后将没有被标记的对象清除。\n 缺点：带来大量空间碎片，导致需要分配一个较大连续空间时，容易触发 GC。\n 标记-整理(标记-压缩)算法 从每个 GC Roots 对象出发，标记存活的对象，然后将存活的对象整理到内存空间的一端，形成连续的已使用空间，最后将已使用空间外的部分全部清理掉，消除空间碎片问题。\n标记-复制算法 为了能够并行的标记和整理，将整个空间分为两块，每次只激活一块，垃圾回收只需把存活的对象复制到另一块未激活的空间上，将未激活空间标记为已激活，将已激活空间标记为未激活，然后清除原空间中的原对象。\n分代收集算法 垃圾收集器一般根据对象存活周期的不同，将内存划分为几块，根据每块内存空间的特点，使用不同的回收算法，提供回收效率。\n分区算法 将整个堆空间划分为连续的不同小空间，每一个小空间独立使用，独立回收。\n 优点：可以控制一次回收多少个小区间。\n HotSpot 虚拟机垃圾收集器 Serial 新生代 Serial 收集器采用复制算法，使用单线程进行垃圾回收，回收时 Java 应用程序中的线程都需要暂停 (Stop-The-World)，以等待回收完成。使用 -XX:+UseSerialGC 可以指定新生代采用 Serial 收集器，老年代采用 Serial Old 收集器。\nParNew 新生代 ParNew 将 Serial 收集器多线程化，在并发能力强的 CPU 上，产生的停顿时间短于串行回收器。开启 ParNew 回收器：\n -XX:+UseParNewGC：新生代 ParNew，老年代采用 Serial Old -XX:+UseConcMarkSweepGC：新生代 ParNew，老年代采用 CMS  -XX:ParallelGCThreads 可以指定并行回收的线程数，这个线程数的默认值是：\nthreadsNum = 0; if (CORE_OF_CPU \u0026lt; 8) { threadsNum = CORE_OF_CPU; } else { threadsNum = 3 + 5 * CORE_OF_CPU / 8; } Parallel 新生代 Parallel 采用复制算法，多线程、独占式，它与 ParNew 的不同之处：\n 关注系统的吞吐量 支持自适应 GC 调节  以下参数启用 Parallel 回收器：\n -XX:+UseParallelGC：新生代 ParallelGC，老年代：Serial Old -XX:+UseParallelOldGC：新生代 ParallelGC，老年代 ParallelOldGC  用于控制吞吐量的两个重要参数：\n -XX:MaxGCPauseMills: 设置最大垃圾收集停顿时间。 -XX:GCTimeRatio: 设置吞吐量大小，范围 0 ~ 100。假设这个值是 n，那么默认不超过 1 / (1 + n) 的时间百分比用于垃圾收集，n 默认为 99。  用于控制自适应调节 GC 的参数：\n -XX:+UseAdaptiveSizePolicy: 新生代、eden 和 survivor 的比例会动态调整。  Serial Old 老年代串行收集器 Serial Old 采用标记-整理算法，也使用单线程进行垃圾回收。使用如下参数开启 Serial Old 回收器：\n -XX:+UseSerialGC：新生代、老年代都使用 Serial 回收器 (老年代用的是 Serial Old) -XX:+UseParNewGC：新生代采用 ParNew，老年代采用 Serial Old -XX:+UseParallelGC：新生代采用 ParallelGC，老年代采用 Serial Old  Parallel Old 老年代 Parallel Old 回收器采用标记-整理算法，多线程进行垃圾回收。使用 -XX:+UseParallelOldGC 可以在新生代采用 Parallel，老年代采用 Parallel Old 收集器。参数 -XX:ParallelGCThreads 可以用于设置垃圾回收时的线程数量。\nCMS CMS 是一个基于标记-清除的算法，启用 CMS 的参数是 -XX:+UseConcMarkSweepGC，默认启动的工作线程数是 (ParallelGCThreads + 3) / 4。 CMS 不会等到堆内存饱和的时候才进行垃圾回收，而是当老年代的堆内存使用率达到某个阈值 -XX:CMSInitiatingOccupancyFraction 的时候便开始进行回收。CMS 基于标记-清除算法，因此执行垃圾回收完毕之后，会出现大量内存碎片，造成如果需要将内存分配给较大的对象，则必须被迫进行一次垃圾回收，以换取连续的内存空间。未解决这个问题，可以使用 -XX:+UseCMSCompactAtFullCollection 开关，使得 CMS 垃圾收集完毕之后，进行一次内存碎片整理；-XX:CMSFullGCsBeforeCompaction 参数可以用于设定进行多少次 CMS 回收后，执行一次内存压缩。\n CMS 的代价：应用程序消耗更多的 CPU。\n G1 使用 -XX:+UseG1GC 可以打开 G1 收集器开关。参数 -XX:MaxGCPauseMills 可以调整最大停顿时间，另外一个参数 -XX:ParallelGCThreads 可以设置并行回收时，GC 的工作线程数量。\n G1 引入的目的是为了缩短处理超大堆的停顿时间。\n ZGC FullGC 什么情况下会触发 FullGC ?\nSystem.gc() 默认情况下(即未开启 -XX:+DisableExplictGC 参数的情况下)，调用 System.gc() 会显示触发 FullGC，同时对新生代和老年代进行回收。\n对象何时进入老年代 老年对象达到年龄 新生代的对象，每经历一次 GC，年龄加 1，当年龄的最大值最多达到 MaxTenuringThreshold (默认值 15) 的情况下，就可以晋升到老年代。\n 对象的实际晋升年龄是根据 survivor 区的使用情况动态计算的。\n 大对象 新生代空间无法容纳大对象，则会直接晋升到老年代。\n 参数 PretenureSizeThreshold 可以设置对象直接晋升到老年代的阈值，单位是字节，不过只对 Serial 和 ParNew 收集器有效，默认值为 0，即不指定最大晋升大小。\n 参考  《深入理解 Java 虚拟机》 《实战 Java 虚拟机》 《码出高效：Java 开发手册》  "});index.add({'id':197,'href':'/docs/tutorial/unix-optimize/avg-load/','title':"平均负载",'content':"平均负载  作者：赵坤\n uptime 命令 了解负载情况：\n$ uptime 22:39:37 up 2:47, 1 user, load average: 1.44, 1.12, 0.79 含义：\n# 当前时间 22:39:37 # 系统运行多久了 up 2:47 # 当前有几个用户登录 1 user # 过去 1 分钟、5 分钟、15 分钟的平均负载 load average: 1.44, 1.12, 0.79 平均负载的含义  平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。它不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程。\n  可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。  $ ps -efl F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 0 80 0 - 42420 - 19:51 ? 00:00:11 /sbin/init splash 1 S root 2 0 0 80 0 - 0 - 19:51 ? 00:00:00 [kthreadd] 1 I root 3 2 0 60 -20 - 0 - 19:51 ? 00:00:00 [rcu_gp] 1 I root 4 2 0 60 -20 - 0 - 19:51 ? 00:00:00 [rcu_par_gp] 1 D root 10529 2 0 80 0 - 0 - 21:46 ? 00:00:04 [kworker/u16:1+events_unbound] 4 R zk 15513 14502 0 80 0 - 5029 - 22:48 pts/1 00:00:00 ps -efl 平均负载多少算合理  平均负载最理想的情况是等于 CPU 个数\n 查看 CPU 个数：\n$ grep \u0026#39;model name\u0026#39; /proc/cpuinfo | wc -l 4  在生产环境中，当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。\n "});index.add({'id':198,'href':'/docs/programmer-interview/java/multi-thread/','title':"并发 - 多线程的实现方式",'content':"多线程的实现方式 Java多线程实现方式主要有四种：\n 继承 Thread 类 实现 Runnable 接口 实现 Callable 接口 使用线程池  继承 Thread 类 class MyThread extends Thread { @Override public void run() { System.out.println(\u0026#34;MyThread.run()\u0026#34;); } } MyThread myThread1 = new MyThread(); MyThread myThread2 = new MyThread(); myThread1.start(); myThread2.start(); 实现 Runnable 接口 class MyTask implements Runnable { @Override public void run() { System.out.println(\u0026#34;MyTask running....\u0026#34;); } } new Thread(new MyTask()).start(); 实现 Callable 接口 Callable 接口可以在任务执行完之后获取结果：\npublic class MyCallable implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return String.valueOf(System.currentTimeMillis()); } } ExecutorService service = Executors.newSingleThreadExecutor(); MyCallable task = new MyCallable(20); Future\u0026lt;String\u0026gt; future = service.submit(task); String time = future.get(); System.out.println(time); 线程池 Executors 是一个辅助创建各种线程池的工具类：\nExecutor executor = Executors.newSingleThreadExecutor(); executor.execute(() -\u0026gt; System.out.println(\u0026#34;Hello World\u0026#34;)); 提交任务并获取返回值：\nExecutorService executorService = Executors.newFixedThreadPool(10); Future\u0026lt;String\u0026gt; future = executorService.submit(() -\u0026gt; \u0026#34;Hello World\u0026#34;); // some operations String result = future.get(); "});index.add({'id':199,'href':'/docs/books/ddia/ddia-chapter1/','title':"设计数据密集型应用程序 - 可靠 \u0026 可扩展 \u0026 可维护",'content':"设计数据密集型应用程序 - 可靠 \u0026amp; 可扩展 \u0026amp; 可维护  笔记来自于 《Designing Data-Intensive Applications》 的第一章\n 何为数据密集型应用程序 很多应用程序都需要用到如下和数据打交道的系统:\n 数据库 缓存 搜索数据 \u0026amp; 索引 流处理 批量处理  设计这样的应用程序需要考虑很多因素，在此重点关注:\n 可靠性: 系统持续工作 可扩展: 能维持系统负载 (Load) 的增长 可维护: 多人维护  Twitter 的负载  2012 年 Tweet 平均产生的速率: 4.6k/s，峰值速率可以达到 12k/s. 用户浏览首页的这个 API 请求平均: 300k/s.  Twitter 主要的挑战在于，每个用户可以关注很多人，每个人可以被很多人关注。实现这种系统通常有两种方式:\n(1) 用户发布 Tweet 直接写入到大的 Tweet 表中即可。而用户浏览首页，需要首先查找用户关注的所有人，找到这些人发布的所有 Tweet，然后(按照时间)合并这些 Tweet:\nSELECT tweets.*, users.* FROM tweets JOIN users ON tweets.sender_id = users.id JOIN follows ON follows.followee_id = users.id WHERE follows.follower_id = current_user (2) 每个用户的 Timeline 都维护一个缓存。用户发布 Tweet 时，查找关注这个用户的所有人，然后将这条 Tweet 插入到这些人的 Timeline 中。这样用户浏览 Timeline 的请求，几乎不需要耗费什么代价，因为结果已经放置好了。\n方法 (2) 的缺点是将 46k/s 写转为 345k/s 写，因为发布一条帖子，平均需要插入到 75 个关注者的 Timeline 中，更别提那些大 V 有千万的关注者，在几秒内写入这么多数据可以说是非常具有挑战性的。所以，对于 Twitter 这样的系统，每个用户的关注者的分布是一个非常核心的影响可扩展性的负载因子。你的应用程序或许有其它不同的特征参数，但是你运用上述分析原则也能分析出影响你的系统性能的关键因子。\nTwitter 现在的系统采用的是方法 (1) 和方法 (2) 混合在一起的方法，同时使用。\nHadoop 的吞吐量 Hadoop 的吞吐量是指每秒钟能处理的记录，但有时候响应时间(响应时间: 客户端衡量的；延迟: 一个请求等待多长时间被处理)是一个更好的衡量指标。你不要把响应时间看做是一个单一的值，而应该将其视为一系列可以测量的值的分布。\n这偶尔高出来的响应时间可能是因为后台进程的上下文切换、TCP 包的重传、垃圾收集器的 STW、缺页中断、服务器机架的物理震动或其他原因。\n另外，你可能看过许多人提到平均响应时间，这并不是一个很好的衡量系统的指标，它并没有告诉你一个普通用户实际上感受到了多少 delay。更好的方法是使用 percentiles，将所有的响应时间由最快到最慢进行排序，取中位数这个数字，你可以很直观地了解到你的用户通常需要等待多长时间，这个中位数也称之为 50th percentile，有时候缩写为 p50。根据此种指标，你可以找出你的系统在极端情况下表现的有多差劲，比如改为 95th、99th、99.9th 等等。\n响应时间的 High percentiles，也称之为tail latencies，是一个很重要的指标，因为它直接影响用户的体验。例如，亚马逊在内部设置了 99.9th 的指标，即使只影响 1/1000 的请求，这是因为发起请求最慢的用户通常是在账户里面拥有最多数据的用户，他们购买的产品也会更多，它们是最有价值的那批用户，如何保证这批用户欢快的购物体验是极其重要的，亚马逊也注意到响应时间每增加 100ms，销售额就会降低 1%，另外一项调查表明每增加 1秒钟的响应时间，用户的购物满意度降低 16%。\n如果某个服务依赖了多个其他服务，那么被依赖的服务只要有一个服务的响应时间变慢，就会有相当一部分用户觉得整个系统变慢了，这称之为尾部延迟增强 (tail latency amplification)。\n监控响应时间的手段有很多种，比如可以使用滚动窗口实时统计过去 10 分钟之内的所有请求的响应时间的中位数，以及不同的 percentiles 指标。比较自然的实现方式是使用一个 List 每分钟排序一次找出这些指标，效率更为高效的实现方式可以采用 forward decay、t-digest、HdrHistogram 方法。\n如何应对可扩展性 设计一个每秒钟需要处理 10W 请求，每个请求 1KB 的系统，和设计一个每分钟需要处理 3 个请求，每个请求 2GB 大小的系统，其设计应该是完全不同的。\n 银弹并不存在。\n 可维护性 三个系统设计原则:\n Operability: Make it easy for operations teams to keep the system running smoothly. Simplicity: 新加入工程师能够比较容易地理解系统. Evolvability: 工程师能够在未来能够比较容易地改变系统.  "});index.add({'id':200,'href':'/docs/tutorial/unix-optimize/context-switch/','title':"CPU 上下文切换",'content':"CPU 上下文切换  作者：赵坤\n CPU 上下文  CPU 上下文是 CPU 在运行任何任务前，必须的依赖环境。在每个任务运行前，CPU 需要知道任务从哪里加载、又从哪里开始运行，所以这些环境通常包括 CPU 寄存器和程序计数器等。\n 查看系统上下文切换情况 可以使用 vmstat 查询：\n# 每隔 5 秒查询一次 $ vmstat 5 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 256 170532 136656 3361432 0 0 38 53 189 557 6 2 92 0 0 0 0 256 170060 136668 3362284 0 0 0 62 441 785 2 1 97 0 0 0 0 256 170320 136676 3362360 0 0 0 13 706 1002 3 1 97 0 0  cs：每秒上下文切换的次数\n 查看进程上下文切换情况 使用 pidstat 查看：\n$ sudo apt install sysstat $ pidstat -w 5 Linux 5.4.0-42-generic (zk) 2020年08月31日 _x86_64_\t(4 CPU) 23时26分46秒 UID PID cswch/s nvcswch/s Command 23时26分51秒 0 1 2.19 0.00 systemd 23时26分51秒 0 9 0.20 0.00 ksoftirqd/0 23时26分51秒 0 10 17.13 0.00 rcu_sched 23时26分51秒 0 11 0.20 0.00 migration/0  cswch，表示每秒自愿上下文切换（voluntary context switches）的次数，是指进程无法获取所需资源，导致的上下文切换。I/O、内存资源不足，容易发生。 nvcswch，表示每秒非自愿上下文切换（non voluntary context switches）的次数，指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。进程数量多，容易发生。  "});index.add({'id':201,'href':'/docs/programmer-interview/java/synchronized/','title':"并发 - synchronized",'content':"synchronized 锁的对象是谁 当你使用 synchronized 关键字的时候，JVM 底层使用 Monitor 锁来实现同步。而锁的对象可以分为：\n 如果 synchronized 的是普通方法，那么锁是当前实例 如果 synchronized 的是静态方法，那么锁是当前类的 Class 如果 synchronized 的是同步块，那么锁是括号里面的对象  synchronized 同步块 底层基于 monitorenter 和 monitorexit 这一对指令实现的。\npublic void foo(Object lock) { synchronized (lock) { lock.hashCode(); } } 上面的Java代码将编译为下面的字节码：\npublic void foo(java.lang.Object); Code: 0: aload_1 1: dup 2: astore_2 3: monitorenter 4: aload_1 5: invokevirtual java/lang/Object.hashCode:()I 8: pop 9: aload_2 10: monitorexit 11: goto 19 14: astore_3 15: aload_2 16: monitorexit 17: aload_3 18: athrow 19: return Exception table: from to target type 4 11 14 any 14 17 14 any synchronized 方法 方法标记为 ACC_SYNCHRONIZED，同样需要进行 monitorenter 操作。\npublic synchronized void foo(Object lock) { lock.hashCode(); } 上面的 Java 代码将编译为下面的字节码：\npublic synchronized void foo(java.lang.Object); descriptor: (Ljava/lang/Object;)V flags: (0x0021) ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=1, locals=2, args_size=2 0: aload_1 1: invokevirtual java/lang/Object.hashCode:()I 4: pop 5: return synzhronized 性能改进 在 Java 6 之前，Monitor 的实现完全是依靠操作系统内部的互斥锁，因为需要进行用户态到内核态的切换，所以同步操作是一个无差别的重量级操作。\n现代的（Oracle）JDK 中，JVM 对此进行了非常大地改进，提供了三种不同的 Monitor 实现，也就是常说的三种不同的锁：偏斜锁（Biased Locking）、轻量级锁和重量级锁，大大改进了其性能。\n所谓锁的升级、降级，就是 JVM 优化 synchronized 运行的机制，当 JVM 检测到不同的竞争状况时，会自动切换到适合的锁实现，这种切换就是锁的升级、降级。\n对象头 对于 HotSpot JVM，Java 对象保存在内存中时，由对象头、实例数据、对齐填充字节组成。对象头由 **Mark Word、指向类的指针、数组长度(只有数组对象才有)**组成。\nMark Word 记录了对象和锁有关的信息，当这个对象被 synchronized 关键字当成同步锁时，围绕这个锁的一系列操作都和Mark Word 有关。Mark Word 在 32 位 JVM 中的长度是 32 bit，在 64 位 JVM 中长度是 64 bit。Mark Word 在不同的锁状态下存储的内容不同，在 32 位JVM中是这么存的：\n偏斜锁 当没有竞争出现时，默认会使用偏斜锁。JVM 会利用 CAS 操作（compare and swap），在对象头上的 Mark Word 部分设置线程 ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁。这样做的假设是基于在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以降低无竞争开销。\n轻量级锁 如果有另外的线程试图锁定某个已经被偏斜过的对象，JVM 就需要撤销（revoke）偏斜锁，并切换到轻量级锁实现。轻量级锁依赖 CAS 操作 Mark Word 来试图获取锁，如果重试成功，就使用普通的轻量级锁；否则，进一步升级为重量级锁。\n重量级锁 轻量级锁 CAS 获取锁失败，会升级为重量级锁。\n锁降级 当 JVM 进入安全点（SafePoint）的时候，会检查是否有闲置的 Monitor，然后试图进行降级。HotSpot JVM 的 Stop-the-World 机制称为 safepoint，在此期间，所有线程（不含 JNI 线程）会被挂起。\n可重入与公平性 synchronized 是非公平锁，可以重入。\n"});index.add({'id':202,'href':'/docs/programmer-interview/front-end/mobile-responsive/','title':"移动端响应式布局",'content':"移动端响应式布局 1px 的坑  1px 的坑：CSS 中的 1px 并不是固定的大小，它是一个跟设备大小有关系的单位。PC 端的 5px 单位看到的视觉效果并不等同于移动端看到的 5px 的效果。\n 1 CSS 像素与屏幕物理像素的换算公式：\n1 CSS 像素 = 物理像素 / 分辨率 rem rem 是一种相对于根字体大小的相对单位。根字体就是 \u0026lt;html\u0026gt; 元素的字体，改变了 \u0026lt;html\u0026gt; 字体的大小，那么整个页面上基于 rem 的大小都会改变。一般初始值是 16px。\n这种方案需要监听屏幕窗口大小的变化，然后动态地改变 \u0026lt;html\u0026gt; 的 font-size，这个 font-size 一变化，整个页面的其他元素的大小也会跟着变化，从而达到适配的效果。\nfunction refreshRem() { var docEl = doc.documentElement; var width = docEl.getBoundingClientRect().width; var rem = width / 10; docEl.style.fontSize = rem + \u0026#39;px\u0026#39;; flexible.rem = win.rem = rem; } win.addEventListener(\u0026#39;resize\u0026#39;, refreshRem); 有了这个函数，再使用 CSS 预编译插件 px2rem 将 CSS 中定义的 px 转为 rem 单位即可。\nvh + vw vw 是相对于视窗宽度的单位，vh 是相对于视窗高度的单位，有了这两个单位，就不需要我们动态监听了。\n使用插件 postcss-px-to-viewport 将 px 转为 viewport 单位。\n媒体查询 缺点：不同手机的宽度不同，缩放比无法完全确定。\n"});index.add({'id':203,'href':'/docs/books/ddia/ddia-chapter2/','title':"设计数据密集型应用程序 - 数据模型 \u0026 查询语言",'content':"设计数据密集型应用程序 - 数据模型 \u0026amp; 查询语言  笔记来自于 《Designing Data-Intensive Applications》 的第二章\n LinkedIn 的简历 简历是一种 self-contained 文档，采用 JSON 的表达方式应该会更为合适。\nJSON 示例如下:\n{ \u0026#34;user_id\u0026#34;: 251, \u0026#34;first_name\u0026#34;: \u0026#34;Bill\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Gates\u0026#34;, \u0026#34;summary\u0026#34;: \u0026#34;Co-chair of the Bill \u0026amp; Melinda Gates... Active blogger.\u0026#34;, \u0026#34;region_id\u0026#34;: \u0026#34;us:91\u0026#34;, \u0026#34;industry_id\u0026#34;: 131, \u0026#34;photo_url\u0026#34;: \u0026#34;/p/7/000/253/05b/308dd6e.jpg\u0026#34;, \u0026#34;positions\u0026#34;: [ {\u0026#34;job_title\u0026#34;: \u0026#34;Co-chair\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Bill \u0026amp; Melinda Gates Foundation\u0026#34;}, {\u0026#34;job_title\u0026#34;: \u0026#34;Co-founder, Chairman\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Microsoft\u0026#34;} ], \u0026#34;education\u0026#34;: [ {\u0026#34;school_name\u0026#34;: \u0026#34;Harvard University\u0026#34;, \u0026#34;start\u0026#34;: 1973, \u0026#34;end\u0026#34;: 1975}, {\u0026#34;school_name\u0026#34;: \u0026#34;Lakeside School, Seattle\u0026#34;, \u0026#34;start\u0026#34;: null, \u0026#34;end\u0026#34;: null} ], \u0026#34;contact_info\u0026#34;: { \u0026#34;blog\u0026#34;: \u0026#34;http://thegatesnotes.com\u0026#34;, \u0026#34;twitter\u0026#34;: \u0026#34;http://twitter.com/BillGates\u0026#34; } } 在简历中，存在很多一对多的关系，这种关系构成了树的结构，采用 JSON 可以简化这种表达:\n在简历中也需要多对多的关系:\n扩展简历的多对多关系:\n"});index.add({'id':204,'href':'/docs/programmer-interview/front-end/position/','title':"position 属性",'content':"position 属性 作用：决定一个元素放在页面的哪个位置。\nstatic 这是默认值。含义：不要以任何特殊的方式摆放这个元素的位置。\nrelative 这个属性同 static 的表现一致。如果你加了其它的属性比如 top、right、bottom、left 属性，那么就会导致它相应的偏离自己原来的默认位置。\nfixed 让元素相对于浏览器的窗口摆放位置。这个元素不会随着页面的滚动而滚动。\nabsolute 让元素相对于离自己最近的 position 属性的值是非 static 的祖先元素摆放位置。如果实在找不到 position 属性是非 static 的祖先，那么就会相对于 body 元素摆放位置，随着页面的滚动而滚动。\nsticky sticky 是粘性定位 (动态定位)。它依据滚动的位置动态地在 fixed 定位和 relative 定位之间切换。\n参考  CSS position explained  "});index.add({'id':205,'href':'/docs/programmer-interview/java/threadlocal/','title':"并发 - ThreadLocal",'content':"ThreadLocal 作用 有一个比喻：\n学生需要在签字墙签字，锁 相当于只有一个签字笔，学生们需要争抢这个签字笔；而 ThreadLocal 相当于给每个学生发了一个签字笔，每人一个，效率大大提升。\n底层原理 假如你自己需要实现 ThreadLocal\u0026lt;T\u0026gt; 相关的 API，请问你会怎么做？\n 你可能会使用 ConcurrentHashMap\u0026lt;Thread, T\u0026gt;，以 Thread.currentThread() 作为 key，这完全可以。但是缺点明显：1. 处理并发问题；2. 必须有指针指向 Thraed 和 这个对象，即使 Thraed 已经结束了，可以被 GC 了。 那我们改为  Collections.synchronizedMap(new WeakHashMap\u0026lt;Thread, T\u0026gt;()) 怎样？ 可以解决 GC 问题，但是多线程问题仍然没有解决。  Java 实现的想法，没有用 \u0026lt;Thread, T\u0026gt; ，而是大概如下：\nnew WeakHashMap\u0026lt;ThreadLocal,T\u0026gt;() 而事实上，在每个 Thread 内部也的确有这么一个 Map 指针：\npublic class Thread implements Runnable { ThreadLocal.ThreadLocalMap threadLocals = null; } 虽然 ThreadLocalMap 并不是一个 WeakHashMap，但是它的设计类似 WeakHashMap，它的 Key 是由 Weak Reference 引用的。\n再看 ThreadLocal.get() 方法：\npublic T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } ThreadLocal.setInitialValue() 的实现：\nprivate T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } 假设你定义了如下几个 ThreadLocal：\nThreadLocal\u0026lt;SimpleDateFormat\u0026gt; threadLocalA = new ThreadLocal\u0026lt;\u0026gt;(); ThreadLocal\u0026lt;Random\u0026gt; threadLocalB = new ThreadLocal\u0026lt;\u0026gt;(); ThreadLocal\u0026lt;Buffer\u0026gt; threadLocalC = new ThreadLocal\u0026lt;\u0026gt;(); 那么 Thread.currentThread() 指向了一个 ThreadLocalMap，这个 Map 存储了 3 个 ThreadLocal：\n使用场景  你想要在多线程环境下访问某个非线程安全的对象（例如 SimpleDateFormat），但是你又想避免通过添加 synchronized 来进行同步，此刻可以考虑使用 ThreadLocal 来给每个线程都搞一份找个对象的实例。 很多框架使用 ThreadLocal 来存储与当前线程相关的上下文信息 Context，这样从 A 方法调用到 B 方法的时候，不用手动通过参数传入当前的 Context 信息，直接从 ThreadLocal 中读取即可。  内存泄露 ThreadLocal 已经被赋值为 null 了，但是由于 Thread 没有运行完，一直强引用着 ThreadLocalMap，那么这个 Map 无法被 GC，导致这个 Map 所引用的 value 无法被回收（key 是可以被回收的，因为 key 是弱引用），从而出现内存泄露。\nThreadLocalMap 中的每一项如下所示，即 key 是 WeakReference，而 v 也就是 value 是强引用：\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } 为什么 value 不也弄成弱引用？ 如果弄成弱引用，那么每次 GC 都有可能被清空，那么就无法自始至终地保存这一个 value 对象了，每次获取到的有可能都是一个全新的对象。\n推荐：每次使用完 ThreadLocal，都调用 ThreadLocal.remove() 方法清除数据。\nobjectThreadLocal.set(userInfo); try { // ... } finally { objectThreadLocal.remove(); } "});index.add({'id':206,'href':'/docs/books/ddia/ddia-chapter3/','title':"设计数据密集型应用程序 - 存储和读取",'content':"设计数据密集型应用程序 - 存储和读取  笔记来自于 《Designing Data-Intensive Applications》 的第三章\n 精心选取的索引可以提升查询的速度，但是也会影响写入的速度。很多数据库系统内部会采用一种 append-only log file 文件，来记录更新了什么数据。\nHash 索引 使用 in-memory hash map 对只进行追加写入的文件进行索引:\n如上述讨论，我们只对文件追加，但是如何防止文件大到超出磁盘空间呢？一种可行的办法是，将 log 文件切分为 segments (当一个 segment 文件达到某个大小的时候，就关闭它，然后开始往新的 segment 文件中写入)，我们可以在这些 compaction 中进行 compaction (去除对 key 的重复的历史更新，只保留最近一次的更新即可)。\n事实上，在执行 compaction (可以让 segment 文件不至于太大) 的时候，我们还可以同时 merge segments 到新的 segment 文件中，可以使用一个后台线程来执行这些操作。在执行操作的同时，我们依然可以使用旧的 segment 文件继续对外提供 read 和 write 服务。当 merge 完毕后，我们再切换到新的 segment 文件上，然后将旧的 segment 文件删除即可。\n现在每一个 segment 文件都拥有了自己的 in-memory hash table，存储了 key 到文件偏移量的映射关系。根据 key 查找值的过程，我们首先检查最近的 segment 的 hash map，如果 key 不在里面，我们就查找第二个 segment，以此类推。merge 操作本身会保证 segment 文件不至于太多，所以我们也无须查看太多的 hash map。当然在实际实现中，还是有很多问题需要考虑:\n 文件格式: CSV 文件格式并不是最合适的，一般采用二进制格式的文件，先存储字符串的长度，紧跟着存储字符串本身。 删除记录: 如果想要删除一个键值对，那么需要对数据文件添加一个特殊的删除记录 (有时候称之为 tombstone). 当执行 merge 操作的时候，这个标记位告诉 merging 进程删除掉这个 key. 崩溃恢复: 如果数据库重启了，那么 in-memory hash map 里面的数据也就丢失了。当然你可以每次在数据库重启的时候，重新读取每一个 segment 文件，重新建立建设关系，但似乎花费的时间有点长。Bitcask 采用的策略是在磁盘上存储每一个 segment 文件 hash map 的 snapshot，这样重新恢复到磁盘的时候会更快一些。 部分写入: 数据库可能在写入到一半的时候崩溃。Bitcask 引入了校验码，针对这种写入可能出现问题的记录可以删除掉或者忽略掉。 并发控制: 写是追加写，因此一般的实现方法是只有一个写线程。segment 文件一旦写入，就不会再做任何修改，因此并发读是没有问题的。  只能进行追加写入的 log 文件是否有点浪费？实时证明，这样的限制有很多好处:\n Appending 和 segment merging 是顺序写操作，比随机写要快很多，尤其是在旋转驱动的磁盘上。某种程度上，及时是 SSD 磁盘，也要快一些。 顺序写或者 immutable 使得并发控制和崩溃恢复容易了许多，你不用担心某个文件会出现既存储了一部分旧文件的内容，同时又存储了一些新文件的内容这种情况。 归并旧的 segment 文件，避免了数据文件分片的问题。  然而，Hash table 索引也有限制:\n Tash table 必须装载到内存中，基于磁盘的 hash map 需要太多的随机 I/O。 支持范围查询并不容易，例如你不能很轻松地查找到位于 kitty00000 和 kitty99999 之间的所有 key。  接下来，我们就讨论不受这种限制的索引结构。\nSSTables 和 LSM-Trees 现在，我们要求存储到 segment 文件中的 key-value 对要根据 key 排好序，这种格式的文件称之为 Sorted String Table (SSTable)。我们还要求在每一个归并好的 segment 文件中，每一个 key 仅允许出现一次 (当然，compaction 进程本身已经保证了这一点)。SSTable 相对于基于哈希索引的 segment log 文件有巨大优势:\n 归并 segment 文件变得简单和高效，即使文件的超出了内存的大小。我们可以采用归并排序来突破内存的限制，只需要对比每个文件的第一个 key，然后将 lowest 的 key 拷贝到输出文件中，然后重复这个过程即可:  每一个 segment 包含的是某段时间内的所有写入数据库的值，这也意味着一个 segment 文件里的所有 values 一定比另外一个 segment 文件里的所有 values 在时间线上要更晚一些 (假设我们只是合并相邻的 segment 文件)。当多个文件都有某个 key 的时候，仅需要保存最近一次写入的，放弃其它时间段写入的旧值即可。\n 假设你要查找 handiwork 关联的值，但是你并不知道它的值在 segment 文件中的具体的偏移量。然而，你知道 handbag 和 handsome 的偏移量，因为所有的 key 是排好序的，因此你知道 handiwork 位于这两个 key 之间。因此你可以从 handbag 这个 key 开始顺序扫描，直至找到这个 key。  你仍然需要在内存维护一个有关 key 偏移量的表，然而你不需要维护所有的，只需要维护特定的一些 (sparse) 即可，一个几 KB 大小的 segment 文件维护一个 key 的偏移量即可，因为线性扫描几 KB 速度是很快的。\n 既然读请求无论如何都需要线性扫描一段范围的 key，那干脆就将这段记录归位一组，形成一个 block，在写入磁盘之前进行压缩。这样，sparse in-memory index 的每一项指向的都是压缩后的 block 的入口，不但可以节省磁盘空间，也能减少 I/O 带宽消耗。  构建与维护 SSTables  当有 write 操作的时候，将其添加到 in-memory balanced tree 数据结构中 (例如，红黑树)，这种 in-memory tree 有时候称之为 memtable. 当 memtable 增长到超过某个阈值 (比如几 MB) 的时候，将其作为一个 SSTable 写入到磁盘上。接下来的 write 请求可以转到新的 memtable 上进行。 当有 read 请求的时候，首先尝试在 memtable 中根据 key 查找，然后在最近一次生成的存储在磁盘上的 segment 文件中查找，然后再下一个 segment 文件中查找。 在这整个过程中，后台运行着一个进行归并和 compaction 的进程，用来 combine segment 文件、丢弃已经删除的值等操作。  上述机制运行地非常好，它只有一个问题: 当数据库崩溃的时候，最近一次的 write 写 (即已经写入到 memtable 中，但是没有写入到磁盘中的记录) 将会丢失。为了避免出现这个问题，我们可以创建一个独立的 log 文件用来记录每次新写入的记录。这个 log 文件无须有序，它唯一的目的就是为了恢复数据库。每当 memtable 写入到 SSTable 文件中后，与其相关联的 log 文件就可以丢弃了。\nLSM-tree 与 SSTable 这里提到的算法是 LevelDB 和 RocksDB 中提到的，Cassandra 和 HBase 中也使用了相似的存储引擎算法，它们应该都是受到 Google 的 Bigtable 论文 (引入了 SSTable 和 memtable 的概念) 所启发的。\n这种索引结构一开始称作 Log-Structured Merge-Tree ，在此基础上构建的存储引擎称之为 LSM 存储引擎。Elasticsearch 和 Solr 使用 Lucene 作为其索引引擎，其底层同样采用了相似的技术来存储它的 term dictionary。\n优化  LSM 查找不在数据库中的 key 的时候会很慢，一般采用 Bloom filters 来优化。 SSTable 如何 compact 和 merge 的策略也有很多种，最常见的是 size-tiered 和 leveled compaction。 LevelDB 和 RocksDB 采用的是 leveled compaction，HBase 采用的是 size-tiered，Cassandra 两者都支持。Size-tiered compaction，新的以及比较小的 SSTable 会被合并到旧的、比较大的 SSTable 的后面；Leveled compaction，key range 会被切为比较小的 SSTable 文件，旧的数据会被移动到其它层，这是为了让 compaction 增量处理、并且使用较少的磁盘空间。  采用 LSM-tree 之后，也支持范围查询了，因此磁盘写入是顺序的，因此写的吞吐量也是非常高的。\nB-Trees B-tree 是最常见的索引结构，目前很多关系型甚至非关系型数据库底层存储索引采用的都是 B-tree 索引。\nB-tree 存储的 key-value 对是有序的，因此支持非常高效地范围查询。B-tree 将数据库切分为固定大小的 block 或 page，通常是 4KB 大小 (有时候更大)，每次读或者写都是以一个 page 为单位的。这种设计更多的考虑到了硬件的特性，因为磁盘就是以固定大小的 block 设计的。\n每一个 page 可以通过一个地址或者一个 location，来指向到这个 page，一个 page 也可以通过指针指向另外一个 page，此处说的指针不是位于内存中，而是位于磁盘上。我们就根据 page 直接的这些引用可以构建一颗 page 树:\n这棵树的的 root 节点是一个 page，这个 page 包含了几个 key 以及指向孩子节点 page 的 reference，每一个孩子又以同样的方式构建一段连续的 key。最终会进入到一个包含 key 和这个 key 关联的 value 的 page 。\n一个 page 指向孩子 page 的 reference 的数量在 B-tree 里称之为 branching factor，在实际实现中，这个值取决于存储 reference 需要多少空间，以及一段连续 key 的范围是多大，一般而言这个数字是几百。\n如果想要更新某个 key 的值，那么需要首先找到这个 key 所在的 leaf page，然后更新值，然后写回到磁盘上。如果你想要添加一个新的 key，你需要找到容纳这个 key 的这段 range，然后添加这个 range 所在的 page 上，如果这个 page 的空间不足了，那么需要将其拆为两半，父 page 需要根据新的子视图的变化而随之更新。\nB-tree 树的算法保证了树本身一直是平衡的: 一个有 n 个 key 的 B-tree 总是有 O(logn) 的深度，多数数据库用 3 层或 4 层的深度就可以容纳整个数据库本身，所以在查找数据额时候，无须追踪太多的 page reference (一个 4 层的 B-tree，每个 page 存储 4KB，branching factor 为 500，那么可以存储多达 256 TB 的数据)。\n使 B-tree 更为可靠 B-tree 写入 page 的这种操作不同于上述介绍到的方法，这是一种 overwritten，而非 append 的方式。\n为了使数据库崩溃之后不至于难以恢复，通常 B-tree 的实现会伴随着另外一个额外的位于磁盘上的数据结构: write-ahead log (WAL，也称之为 redo log)。这种一种 append-only 的文件，B-tree 的每一次修改操作更新 page 之前，必须将此种修改先写入到这个文件中。数据库的崩溃恢复，就全靠这个 log 来重新让数据恢复一致性了。\n更新 page 的时候也得需要考虑到并发控制的问题，这期间可能有其它几个线程正在执行查询，不保护数据的话，线程容易看到不一致的状态。这通常是使用latches (lighweight locks) 来做到的。Log-结构的索引不存在这个问题，因为在后台归并 segment 文件的时候，这些 segment 文件并不会处理查询请求。\n优化 B-tree B-tree 已经出现这么多年了，针对它也有许多优化技术:\n 某些数据库 (例如 LMDB) 使用 copy-on-write 机制来恢复数据，而非采用 overwriting page 和维护 WAL 来恢复。某个 page 被修改之后，会被写入到一个不同的位置，parent page 也会指向这个新的 page。 存储 key 的时候无须存储整个 key，可以存储 key 的缩写。尤其是在 tree 中间的那些层，它只需要提供能维系一段范围的信息就足够了。 通常的实现: 一段 key 是连续的，这些 key 指向的 page 在磁盘上不一定是连续的。这样当有一个大的范围查询的时候，逐个读取这一个又一个不连续的 page 是非常低效的。许多 B-tree 在实现上都在尝试维护 leaf page 在磁盘上的有序性，尽管在 tree 增长的时候，同时维护 leaf page 的有序性，的确是一件很困难的事情。 我们可以在 tree 上添加更多的指针。例如，每一个 leaf page 可以有指针指向左右兄弟 page，这样在线性扫描的时候，可以直接跳到上游或者下游，而无需从 parent page 上绕路。 B-tree 的变种例如 fractal tree，引入了一些 log-structured 的想法来减少磁盘 I/O。  对比 B-tree 和 LSM-Tree 根据经验，LSM-tree 通常写的速度非常快 (读通常比较慢，因为伴随着 comopaction，SSTable 可能分别位于不同的阶段，一次读可能就需要查询几种不同的数据结构)，而 B-tree 通常读的速度比较快。\n然而，一切还是得让 benchmark 的结果说话。\nLSM-tree 的优势 B-tree 索引至少需要两次才能真正的写一份数据: 一次是写入 write-ahead log，一次是写入 B-tree 树本身 (如果 page 有分裂，那么很有可能还需要再写一次)。如果只修改了几个字节，就需要写入整个 page ，这在时间上耗费也是比较长的。一些数据库甚至需要写入两次相同的 page，以防止在断电时，出现部分更新的 page 这种情况。\nLog-structured 索引由于需要不停的对 SSTable 进行 compaction 和 merge，因此也是需要写入多次的。这种一次写入，却在磁盘层面引发了多次写入的现象，称之为 write apmlification (写入放大)。固态硬盘更要引起注意，因为其覆盖写入的次数是有限的。\nLSM-tree 之所以有更高的写吞吐量，是因为它的 write amplification 稍弱一点，也是因为其总是顺序地写入到 SSTable 文件中，而不是覆盖写。要知道，在机械硬盘上随机写是远远慢于顺序写的。\nLSM-tree 可压缩性更好，其比 B-tree 更能产生较小的压缩文件。在 B-tree 里，当一个 page 分裂之后，或者某一行记录无法容纳进已有的 page 里，这个 page 的部分空间是没有被利用上的。鉴于 LSM-tree 不是以 page 为单位的，并且其也会周期性的重新 SSTable 来消除分片，因此在磁盘空间利用率上更高，尤其是采用 leveled compaction 算法的 LSM-tree。\n在许多 SSD 上，其会在内部使用 Log-structured 算法将底层存储芯片上的随机写入转换为顺序写入，因此存储引擎写入模式的影响不太明显。但是，较低的 write amplification 和较少的碎片在 SSD 上仍然是有利的：在可用的 I/O 带宽范围内，更紧凑地表示数据，意味着可以进行更多的读写请求。\nLSM-tree 的劣势 log-structured 的劣势在于 compaction 进程可能会影响到 read 和 write 的性能。磁盘资源总是有限的，所以可能会出现一个 read 请求需要等待磁盘完成昂贵的 compaction 操作之后才能执行。这种影响或许不是很大，但是却有着更高的 percentiles，所以有时候会发现 log-structured 存储引擎有时候响应时间特别高，相比之下，B-tree 就更为稳定一些。\n另外 compaction 也给磁盘带来了更高的吞吐量，有限的磁盘写带宽需要被 logging、刷新 memtable 到磁盘以及后台的 compaction 线程所共享。\n如果写入量比较大同时 compaction 没有进行较好配置的话，可能还会发生 compaction 速度跟不上源源不断的 write 请求的速度。这种情况下，需要待合并的 segment 文件将会越来越多直至超出磁盘可用空间，读请求因为需要检查更多的 segment 文件，所以响应时间也会变长。而一般的基于 SSTable 存储引擎的实现，在 compaction 出现问题的时候，也不会对到来的写请求做任何约束，因此你需要自己取监控这种行为。\nB-tree 的优势在于每一个 key 仅存在于一个索引中，log-structured 存储引擎可能有相同 key 的位于多个 segment 文件中的多份拷贝。这给予了 B-tree 作为数据库存储引擎的更为迷人的魅力: 实现事物隔离，仅需要将锁与这棵树关联起来即可。\n没有某种简单方便的规则来告诉您，哪种存储引擎更适合您的应用程序，因此还是很值得测试的。\n其它索引结构 B-tree 和 log-structured 索引都可以用作二级索引。\n值存储在索引中 索引中的 key 关联的值，key 存储实际的行，也可以是一个指向实际行地址的引用。其中，存储引用的这种方式是比较常见的，这样有多个索引存在的时候，只需要其它索引的引用也指向就可以了，而无需复制数据。\n在一些场景下，通过引用再定位到实际行的这种存储方式，影响了读的性能，所以直接将值存储在索引中的做法更好一些，这种索引称之为聚簇索引。例如，在 MySQL 中，主键永远是聚簇索引，二级索引指向主键索引；在 SQL Server 中，你可以手动为每个表指定一个聚簇索引。聚集索引（存储索引中的所有行数据）和非聚集索引（仅存储对索引中数据的引用）之间的折衷称为覆盖索引或包含列的索引，它将表的某些列存储在索引中。这使得一些查询可以通过单独使用索引来直接响应（在这种情况下，索引被称为覆盖查询）。\n与任何类型的重复数据一样，聚集索引和覆盖索引可以加快读取速度，但它们需要额外的存储空间，并且会增加写入开销。\n多列索引 到目前为止讨论的索引只将一个键映射到一个值。如果我们需要同时查询表的多个列（或文档中的多个字段），那么这还不够。\n最常见的多列索引类型称为串联索引，它通过将一列附加到另一列而将多个字段合并为一个键（索引定义指定字段串联的顺序）。这就像一本老式的纸质电话簿，它提供了从（姓，名）到电话号码的索引。由于排序顺序不同，索引可用于查找具有特定姓氏的所有人员，或具有特定姓氏组合的所有人员。但是，如果你想找到所有有特定名字的人，索引就没有用了。\n多维空间索引是一种比较重要的多维数据查询方法。例如，一个餐馆搜索网站可能有一个包含每个餐馆的经纬度的数据库。当用户在地图上查看餐厅时，网站需要搜索用户当前正在查看的矩形地图区域内的所有餐厅。这需要一个二维范围查询，如下所示：\nSELECT * FROM restaurants WHERE latitude \u0026gt; 51.4946 AND latitude \u0026lt; 51.5079 AND longitude \u0026gt; -0.1162 AND longitude \u0026lt; -0.1004; 标准的 B-tree 或 LSM-tree 索引无法有效地处理此类查询：它可以提供纬度范围内（但在任何经度）的所有餐厅，或在经度范围内的所有餐厅（但在北极和南极之间的任何地方），但不能同时提供这两个索引。\n一种方法是使用空间填充曲线将二维位置转换为单个数字，然后使用常规 B 树索引。更常见的是使用特殊的空间索引，如 R 树。例如，PostGIS 使用 PostgreSQL 的广义搜索树索引将地理空间索引实现为 R 树。\n一个有趣的想法是，多维索引不仅仅针对地理位置。例如，在一个电子商务网站上，你可以使用三维索引（红、绿、蓝）来搜索特定颜色范围的产品，或者在天气观测数据库中，你可以在（日期，温度），以便有效地搜索 2013 年的所有观测值，其中温度在25至30℃之间。如果使用一维索引，则必须扫描2013年的所有记录（不考虑温度），然后按温度过滤，反之亦然。二维索引可以同时受到时间戳和温度的限制。HyperDex 就是使用的此类技术。\n全文检索与模糊索引 到目前为止讨论的所有索引都假定您有精确的数据，并允许您查询键的精确值，或者按排序顺序查询键的值范围。他们不允许你搜索相似的键，比如拼写错误的单词。这种模糊查询需要不同的技术。\n例如，全文搜索引擎通常允许对一个词的搜索进行扩展，以包括该词的同义词，忽略词的语法变化，并搜索同一文档中相邻词的出现，并支持依赖于文本语言分析的各种其他功能。为了处理文档或查询中的打字错误，Lucene 能够在一定编辑距离内搜索文本（编辑距离为 1 表示添加、删除或替换了一个字母）。\n在 Lucene 中，内存索引是键中字符的有限状态自动机，类似于 trie 树。这种自动机可以转换成 Levenshtein 自动机，它支持在给定编辑距离内有效地搜索单词。\n其他的模糊搜索技术则朝着文档分类和机器学习的方向发展。\nin-memory databases 一些内存中的键值存储（如 Memcached ）仅用于缓存，在这种情况下，如果计算机重新启动，数据丢失是可以接受的。但其他内存中数据库的目标是持久性，这可以通过特殊硬件（如电池供电的 RAM ）来实现，方法是将更改日志写入磁盘，定期将快照写入磁盘，或将内存中的状态复制到其他机器。\n当内存中的数据库重新启动时，它需要从磁盘或通过网络从副本重新加载其状态（除非使用了特殊的硬件）。尽管写入磁盘，但它仍然是内存中的数据库，因为磁盘只是作为一个仅附加的日志来使用，以保证持久性，而读取完全是从内存中提供的。写入磁盘也有操作上的优势：磁盘上的文件可以很容易地被外部实用程序备份、检查和分析。\nVoltDB、MemSQL 和 Oracle TimesTen 等产品都是具有关系模型的内存数据库，这些供应商声称，通过消除与管理磁盘上数据结构相关的所有开销，它们可以大大提高性能。RAMCloud 是一个具有持久性的开源内存键值存储（对内存中的数据和磁盘上的数据使用日志结构的方法）。Redis 和 Couchbase 通过异步写入磁盘来提供弱持久性。\n与直觉相反，内存中数据库的性能优势并不是因为它们不需要从磁盘读取。如果内存足够，即使是基于磁盘的存储引擎也可能永远不需要从磁盘读取数据，因为操作系统无论如何都会在内存中缓存最近使用的磁盘块。相反，它们之可以更快，是因为它们可以避免以可写入磁盘的形式对内存中的数据结构进行编码的开销。\n除了性能之外，内存数据库的另一个有趣的领域是提供难以用基于磁盘的索引实现的数据模型。例如，Redis 为各种数据结构（如优先级队列和集合）提供类似数据库的接口。因为它将所有数据保存在内存中，所以它的实现相对简单。\n最近的研究表明，内存数据库体系结构可以扩展到支持比可用内存更大的数据集，而不用担心以磁盘为中心的体系结构的开销。所谓的反缓存方法的工作原理是：当内存不足时，将最近最少使用的数据从内存逐出磁盘，并在将来再次访问时将其重新加载到内存中。这与操作系统对虚拟内存和交换文件的操作类似，但是数据库可以比操作系统更有效地管理内存，因为它可以在单个记录的粒度上工作，而不是整个内存页。不过，这种方法仍然需要索引完全能装入内存（就像本章开头的 Bitcask 示例）。\n如果非易失性存储器（NVM）技术得到更广泛的采用，则可能需要对存储引擎设计进行进一步的更改。目前，这是一个新的研究领域，但值得今后继续关注。\n交易处理还是分析？ 事务不必具有 ACID（原子性、一致性、隔离性和持久性）属性。事务处理只意味着允许客户端进行低延迟的读写操作，而不是只定期运行（例如，每天运行一次）的批处理作业。\nOLTP(online transaction processing) 和 OLAP(online analytic processing) 的不同:\n   属性 OLTP OLAP     主要读模式 每一次查询是一小部分数据集 聚合大规模的数据集   主要写模式 随机访问 Bulk import (ETL) 或事件流   主要用于 Web 应用的客户 分析做决策   数据来源 最近状态的数据 已发生事件的历史数据   数据量 GB 到 TB TB 到 PB    一开始，事务处理和分析查询都使用相同的数据库。SQL 在这方面非常灵活：它适用于 OLTP 类型的查询和 OLAP 类型的查询。然而，在20世纪80年代末和90年代初，有一种趋势是公司停止使用其 OLTP 系统进行分析，而是在单独的数据库上运行分析。这个单独的数据库称为数据仓库。\n数据仓库 数据仓库是一个独立的数据库，分析师可以查询他们心中的内容，而不会影响 OLTP 操作。数据仓库包含公司中所有不同 OLTP 系统中数据的只读副本。数据从 OLTP 数据库中提取（使用定期数据转储或连续更新流），转换为便于分析的模式，进行清理，然后加载到数据仓库中。将数据放入仓库的过程称为提取-转换-加载（ETL），如图所示。\n数据仓库的数据模型通常是关系型的，因为 SQL 通常很适合分析查询。有许多图形化的数据分析工具可以生成SQL查询，可视化结果，并允许分析人员探索数据（通过诸如向下钻取、切片和分片等操作）。\n从表面上看，数据仓库和关系OLTP数据库看起来很相似，因为它们都有一个SQL查询接口。但是，系统的内部结构看起来可能非常不同，因为它们针对非常不同的查询进行了优化模式。很多数据库供应商现在专注于支持事务处理或分析工作负载，但不是两者都支持。\n分析的模式: 星型和雪花型 星型示例:\n“星型模式”这个名字来自这样一个事实：当表关系被可视化时，事实表位于中间，被它的维度表包围；到这些表的连接就像星星一样。\n此模板的变体称为雪花模式，其中维度进一步细分为子维度。雪花模式比星型模式更规范化，但星型模式通常更受欢迎，因为它们更便于分析人员使用。\n面向列的存储 如果 fact 数据表中有数以万亿计的行和数 PB 的数据，那么高效地存储和查询这些数据将成为一个具有挑战性的问题。维度表通常要小得多（数百万行），因此我们将主要关注 fact 的存储。\n尽管 fact 表的宽度通常超过100列，但一个典型的数据仓库查询一次只能访问其中的 4 或 5 列（“SELECT”查询很少用于分析）。以如下查询为例：它访问了大量的行（在2013年的日历年中，每次有人购买水果或糖果），但它只需要访问fact_sales表的三列：date_key、product_sk和quantity。查询忽略所有其他列。\nSELECT dim_date.weekday, dim_product.category, SUM(fact_sales.quantity) AS quantity_sold FROM fact_sales JOIN dim_date ON fact_sales.date_key = dim_date.date_key JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk WHERE dim_date.year = 2013 AND dim_product.category IN (\u0026#39;Fresh fruit\u0026#39;, \u0026#39;Candy\u0026#39;) GROUP BY dim_date.weekday, dim_product.category; 如何有效地执行此查询？\n在大多数OLTP数据库中，存储是以面向行的方式进行的：表的一行中的所有值都是相邻存储的。它的主要劣势在于，面向行的存储引擎需要将所有这些行（每个行由 100 多个属性组成）从磁盘加载到内存中，解析它们，并过滤掉那些不满足所需条件的行。这可能需要很长时间。\n面向列存储的思想很简单：不要将一行中的所有值存储在一起，而是将每个列中的所有值存储在一起。如果每个列都存储在单独的文件中，则查询只需要读取和解析查询中使用的列，这样可以节省大量工作。这一原理如图所示。\n面向列的存储布局依赖于按相同顺序包含行的每个列文件。\n列压缩 除了只从磁盘加载查询所需的列之外，我们还可以通过压缩数据进一步降低对磁盘吞吐量的要求。幸运的是，面向列的存储通常非常适合于压缩。看一看上图每一列的值序列：它们通常看起来相当重复，这是压缩的一个好标志。根据列中的数据，可以使用不同的压缩技术。一种在数据仓库中特别有效的技术是位图编码，如下图所示。\n通常，与行数相比，列中不同值的数量很少。我们现在可以获取一个包含 n 个不同值的列，并将其转换为 n 个单独的位图：每个不同值对应一个位图，每一行对应一个位。如果行具有该值，则位为 1；如果没有，则为 0。如果 n 很小，那么这些位图可以以每行一位的形式存储。但是如果 n 更大，大多数位图中都会有很多零（我们说它们是稀疏的）。在这种情况下，位图还可以进行 run-length encoded，如图所示。这可以使列的编码非常紧凑。\n像这样的位图索引非常适合于数据仓库中常见的查询类型。例如:\n WHERE product_sk IN (30, 68, 69): 加载 product_sk = 30、product_sk = 68、product_sk = 69 的位图，然后这三个位图进行高效地 OR 操作即可。 WHERE product_sk = 31 AND store_sk = 3: 加载 product_sk = 31、store_sk = 3 的位图，然后对这两个位图执行 AND 操作，因为列包含的都是相同顺序的行，所以\t一个列的位图的第 k 个比特位和相同行的另外一个列的位图的第 k 个比特位是相关联的。   Bigtable 模型主要是面向行的。因为在每一个列族里面，所有列和行键是存储在一起的，并且也不使用列压缩技术。\n 列存储的顺序 在列存储中，行的存储顺序并不一定重要。最简单的方法是按插入的顺序存储它们，因为插入新行只意味着将追加到每个列文件中。但是，我们可以选择像之前对 SSTable 所做的那样，强制一个顺序，并将其用作索引机制。\n数据排序的时候，每次调整的都是整个一行的数据。比如说对 date 这一列进行排序，排序之后的这个 date 列中数据存储的可能是:\n2020-01-02,2020-01-05,2020-01-07,2020-02-03,... 那么，其他列中的数据的顺序也会随之进行调整。通过排序，可以只扫描几行即可，避免扫描所有的行来获得特定的日期范围内的记录。\n不同的查询受益于不同的排序，那为何不干脆直接将多种不同顺序的数据存储下来呢？反正在分布式系统中，数据终归还是要存储多份的。实际上，C-Store、Vertica 就是这么干的。\n列存储的写入 类似于 LSM 树，所有的写操作首先进入内存存储器，在那里它们被添加到一个已排序的结构中，并为写入磁盘做好准备。内存中的存储是面向行还是面向列并不重要。当累积了足够多的写操作后，它们将与磁盘上的列文件合并并批量写入新文件。这基本上就是 Vertica 所做的。\n查询需要同时检查磁盘上的列数据和内存中最近的写操作，并将两者结合起来。但是，查询优化器对用户隐藏了这种区别。从分析员的角度来看，通过插入、更新或删除修改的数据会立即反映在后续查询中。\n聚合: 数据立方体和物化视图 并非每个数据仓库都必须是列存储：传统的面向行的数据库和其他一些体系结构也被使用。然而，对于 ad hoc 分析查询，列式存储的速度要快得多，因此它正在迅速普及。\n数据仓库的另一个值得一提的方面是物化聚合。如前所述，数据仓库查询通常涉及聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。如果许多不同的查询使用相同的聚合，那么每次都要对原始数据进行处理是浪费资源的。为什么不缓存一些查询最常使用的计数或总和呢？\n创建这种缓存的一种方法是物化视图。在关系数据模型中，它通常被定义为标准（虚拟）视图：一个类似表的对象，其内容是某个查询的结果。区别在于物化视图是查询结果的实际副本，写入磁盘，而虚拟视图只是写入查询的快捷方式。当您从虚拟视图中读取时，SQL 引擎会动态地将其展开为视图的底层查询，然后处理展开的查询。\n当底层数据发生变化时，需要更新物化视图，因为它是数据的非规范化副本。数据库可以自动执行此操作，但是这样的更新会使写入操作变得更昂贵，这就是为什么在 OLTP 数据库中不经常使用物化视图。在大量读取的数据仓库中，它们更有意义（它们是否真的提高了读取性能取决于具体情况）。\n物化视图的一个常见的特殊情况称为数据立方体或 OLAP 多维数据集。它是按不同维度分组的聚合网格。下图显示了一个示例。\n物化数据立方体的优点是某些查询变得非常快，因为它们已经被有效地预计算过了。缺点是数据立方体没有查询原始数据的灵活性。因此，大多数数据仓库都尽量保留原始数据，并且只使用数据立方体之类的聚合来提高某些查询的性能。\n"});index.add({'id':207,'href':'/docs/programmer-interview/front-end/box-sizing/','title':"CSS 盒模型",'content':"CSS 盒模型 作用 决定一个元素占据多大的矩形面积。\nW3C 盒模型 （标准模型） .myClass { box-sizing: content-box; }  box-sizing 属性的默认值是：content-box\n IE 盒模型 .myClass { box-sizing: border-box; } W3C VS IE 对比 "});index.add({'id':208,'href':'/docs/programmer-interview/java/threadpool/','title':"并发 - 线程池",'content':"线程池 好处 说明：线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。\n设计哲学 将任务的提交与执行解耦开，从而无须太大的困难就能为某种类型的任务指定和修改执行策略。\n用法 任务无须返回值，调用这个方法：\npublic void execute(Runnable command) {} 需要返回值的任务，调用 submit：\nFuture\u0026lt;Object\u0026gt; future = executor.submit(hasReturnValuetask); try { Object s = future.get(); } catch (InterruptedException e) { // ... } catch (ExecutionException e) { // ... } 构造器参数 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { // ... } execute() 方法运行原理 （1） 如果当前运行的线程少于 corePoolSize，则创建新线程来执行任务。部分代码片段如下：\nif (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } addWorker 获取锁，创建线程，并运行任务，伪代码如下：\nw = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { workers.add(w); } finally { mainLock.unlock(); } t.start(); } 其中 new Worker(firstTask) 的内部，创建了新的线程：\nWorker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker  this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } （2）如果运行的线程等于或多于 corePoolSize，则将任务加入 BlockingQueue。\nworkQueue.offer(command) （3）如果无法将任务加入 BlockingQueue(队列已满)，则再一次执行 addWorker 尝试创建新的线程：\nif (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { // 没有命中这个 if 条件 } else if (!addWorker(command, false)) { // ... } 注意，这一次传递给 addWorker 的第二个参数是 core = false，即要求创建的是非核心线程，而 addWorker 内部也会根据 core 的值来循环检查是否大于 corePoolSize 还是 maximumPoolSize：\nint wc = workerCountOf(c); if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; （4）如果创建新线程将使当前运行的线程超出 maximumPoolSize，任务将被拒绝，并调用 RejectedExecutionHandler.rejectedExecution() 方法。\nfinal void reject(Runnable command) { handler.rejectedExecution(command, this); } 线程池大小 计算密集型任务 在拥有 N 个 CPU 的处理器上，线程池大小设置为 N + 1，通常实现最优利用率。\n 多出来的线程，确保线程偶尔暂停的时候，也不会浪费 CPU 时钟周期。\n I/O密集型任务 这种任务不会一直运行，因此线程池规模应该更大。需要估算任务的等待时间与计算时间的比值，或许通过一些分析/监控工具获得。\n线程池最优大小：\n线程池数量 = CPU 数量 * CPU 利用率 * (1 + 等待时间/计算时间) 拒绝策略  AbortPolicy：拒绝执行任务，并抛出 RejectedExecutionException 异常 DiscardPolicy：丢弃任务 DiscardOldestPolicy：移除掉排在队列中时间最久未处理的任务，然后尝试重新执行新任务 CallerRunsPolicy：直接由调用代码处所在的线程运行任务，而非在线程池中运行，即直接运行 r.run()  线程池关闭 shutdown() 不再接受新的任务，但是已经添加进队列的任务可以继续运行，也就是说它会等待正在执行的任务和等待队列中没有执行的任务全部执行完毕。当然这个过程是异步的，你调用这个方法，不会阻塞在这个方法上。\npublic void shutdown() { interruptIdleWorkers(); } shutdownNow() 不再接受新的任务，同时对所有的 Worker 都会尝试 interrupt，同时已经添加进队列的任务也不再等待执行，同时返回未执行的任务到一个新的队列中。\npublic List\u0026lt;Runnable\u0026gt; shutdownNow() { List\u0026lt;Runnable\u0026gt; tasks; interruptWorkers(); tasks = drainQueue(); return tasks; } 优雅关闭 生产环境中，多数都会调用 shutdown() 方法，即关闭之后，让已经 submit 的任务有机会继续执行一段时间。同时为了避免因任务太多长时间运行不完的情况，又会通过 awaitTermination 附加一个超时时间，这个时间过后，线程池强制关闭。\n示例代码：\nthis.scheduledExecutorService.shutdown(); try { this.scheduledExecutorService.awaitTermination(5000, TimeUnit.MILLISECONDS); } catch (InterruptedException e) { } Executors 创建线程弊端 线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明：Executors 返回的线程池对象的弊端如下：\n1） FixedThreadPool 和 SingleThreadPool：\n允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。\n2） CachedThreadPool：\n允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。\n"});index.add({'id':209,'href':'/docs/books/ddia/ddia-chapter4/','title':"设计数据密集型应用程序 - 编码与演化",'content':"设计数据密集型应用程序 - 编码与演化  笔记来自于 《Designing Data-Intensive Applications》 的第四章\n JSON 的二进制编码 { \u0026#34;userName\u0026#34;: \u0026#34;Martin\u0026#34;, \u0026#34;favoriteNumber\u0026#34;: 1337, \u0026#34;interests\u0026#34;: [\u0026#34;daydreaming\u0026#34;, \u0026#34;hacking\u0026#34;] } MessagePack, a binary encoding for JSON.\n第一个字节 0x83 表示接下来将会是一个对象，第二个字节 0xa8，表示接下来是一个字符串。\nThrift 和 Protocol Buffers Protocol Buffers 是由 Google 开发的，Thrift 是有 Facebook 开发的，二者均需要使用一个 schema 来帮助编码。在 Thrift 世界中，对上述 JSON 的编码，需要首先使用 Thrift IDL 来描述 schema:\nstruct Person { 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list\u0026lt;string\u0026gt; interests } Protocol Buffers 中定义的 schema 如下所示:\nmessage Person {\trequired string user_name = 1;\toptional int64 favorite_number = 2;\trepeated string interests = 3; }Thrift 有两种不同的二进制编码格式: BinaryProtocol 和 CompactProtocol，我们首先来看一下 BinaryProtocol:\n"});index.add({'id':210,'href':'/docs/programmer-interview/front-end/bfc_ifc/','title':"BFC 和 IFC",'content':"BFC 和 IFC BFC - Block formatting context 简介 BFC（Block Formatting Context）直译为“块级格式化范围”。是 W3C CSS 2.1 规范中的一个概念，它决定了元素如何对其内容进行定位，以及与其他元素的关系和相互作用。当涉及到可视化布局的时候，Block Formatting Context 提供了一个环境，HTML 元素在这个环境中按照一定规则进行布局。一个环境中的元素不会影响到其它环境中的布局。\n具有 BFC 特性的元素可以看作是隔离了的独立容器，容器里面的元素不会在布局上影响到外面的元素，并且 BFC 具有普通容器所没有的一些特性。\n何时触发 BFC  \u0026lt;html\u0026gt; float 属性不为 none position 属性是 absolute 或 fixed overflow 属性不为 visible display 属性为 inline-block display 属性为 table-cell、table-caption、table、table-row 等表格元素 display 属性为 flow-root display 属性为 flex、inline-flex display 属性为 grid、inline-grid column-count 或 column-width 属性不为 auto 的 column-span 属性为 all  BFC 作用 消除外边距重叠 发生外边距重叠 \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;margin-bottom: 100px\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;margin-top: 100px\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt;   分别放在不同的 BFC 容器中 \u0026lt;div style=\u0026#34;overflow: hidden;\u0026#34;\u0026gt; \u0026lt;p style=\u0026#34;margin-bottom: 100px;\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;overflow: hidden;\u0026#34;\u0026gt; \u0026lt;p style=\u0026#34;margin-top: 100px;\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt;    包含浮动的元素 包含 float 孩子，容器高度变窄 \u0026lt;div style=\u0026#34;border: 1px solid #000;\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;width: 100px; height: 100px; float: left;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   触发容器的 BFC，正常计算高度 \u0026lt;div style=\u0026#34;border: 1px solid #000; overflow: hidden\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;width: 100px; height: 100px; float: left;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    阻止元素被浮动元素覆盖 div B 被 float div A 覆盖 \u0026lt;div class=\u0026#34;A\u0026#34; style=\u0026#34;float: left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;B\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;   触发容器的 BFC \u0026lt;div class=\u0026#34;A\u0026#34; style=\u0026#34;float: left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;B\u0026#34; style=\u0026#34;overflow: hidden\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;    IFC - Inline formatting context 什么时候触发 IFC 当一个 block 容器只包含 inline 元素的时候就会触发 IFC\nIFC 布局规则 只计算横向样式，忽略纵向 如下示例，margin-top 和 margin-bottom 未生效：\n.warp { border: 1px solid red; display: inline-block; } .text { margin: 20px; background: green; } \u0026lt;div class=\u0026#34;warp\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;text\u0026#34;\u0026gt;文本一\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;text\u0026#34;\u0026gt;文本二\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; 水平排列依赖容器的 text-align 容器的 text-align 属性是 center，那么容器内部的多个元素就会居中的方式进行排列。\n.warp { border: 1px solid red; width: 200px; text-align: center; } .text { background: green; } \u0026lt;div class=\u0026#34;warp\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;text\u0026#34;\u0026gt;文本一\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;text\u0026#34;\u0026gt;文本二\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; "});index.add({'id':211,'href':'/docs/programmer-interview/java/volatile/','title':"并发 - volatile",'content':"volatile 作用  在多处理器中，保证共享变量的 “可见性”（一个线程修改后，另外一个线程能立即读取到这个最新修改的值） 禁止对指令进行重排序  三大特性  原子性 有序性 可见性  对比 synchronized  volatile 无法保证原子性 volatile 不会使线程陷入阻塞，不会引起线程上下文的切换和调度  典型用法 数绵羊程序：\nvolatile boolean sleep; while (!sleep) { countSleep(); } 底层原理 有 volatile 变量修饰的共享变量进行写操作的时候会多出一行以 lock;  指令开头的汇编代码。而 lock;  指令相当于一个内存屏障，其作用如下所示：\n 将当前处理器缓存行的数据写回到系统内存。 这个写回内存的操作会使在其他 CPU 里缓存了该内存地址的数据无效。  "});index.add({'id':212,'href':'/docs/books/ddia/ddia-chapter5/','title':"设计数据密集型应用程序 - Replication",'content':"设计数据密集型应用程序 - Replication Replication 就是将相同数据的拷贝防止在多个通过网络连接在一起的机器上。\n为什么需要 Replication  让数据在地理位置上更靠近用户 部分数据坏掉的时候，系统依然能持续工作 可伸缩，增加机器即可增加吞吐量  如果你需要 replication 的数据不发生变化，那么 replication 的过程是及其简单的，你只需拷贝到其它各个机器上，然后你的任务就完成了。然而 replication 最难的地方也就在这个地方，如何处理变化的数据？接下来就介绍三种常见的处理 replication 中数据变化的算法: single-leader、multi-leader、leaderless。\nLeaders 和 Followers 每一个存储一份数据库拷贝的节点称之为: replica。每一个 replica 都需要处理写数据的操作，久而久之，每一个节点之间存储的数据也就不再一致了。解决这种问题最常见的办法就是: leader-based replication (active/passive 或 master-slave replication)，它的工作原理如下:\n 其中某个 replica 被指定为 leader (master 或 primary)，客户端想要写数据，那么必须将它们的写数据的请求发送给 leader，然后 leader 随后写入到自己的本地磁盘中。 其余的 replica 称之为 follower (read replicas, slaves, secondaries, hot standbys)，当 leader 写入数据到本地磁盘的时候，同时将数据改变的部分作为 replication log 或者 change stream 发送给它的 followers。每一个 follower 根据收到的 log 按照和 leader 处理不同写操作之间的相同的顺序，来更新它自己本地的数据。 当一个客户端想要读取数据的时候，它可以发送读请求给 leader 或者任意一个 follower。但是写请求的话只能发送给 leader。  这种模式的 replication 内置在许多数据库中，例如: PostgreSQL、MySQL、Oracle Data Guard、SQL Server 的 AlwaysOn Availability Groups，甚至在许多非关系型数据库中也有它的身影，例如: MongoDB、RethinkDB、Espresso，这种模式也局限于数据库，像消息中间件 Kafka 和 RabbitMQ 高可用的队列都依赖它，一些网络文件系统和 replicated block devices 例如 DRBD 也是同样的道理。\n同步和异步 Replication replication 数据的过程可以分为同步或者异步。如下图所示，follower 1 配置为同步的，leader 等待 follower 1 确认它收到了 log 之后，才向 client 反馈更新成功；而被配置为异步的 follower 2 则无须这一步，leader 可以直接反馈更新成功。\n显然在 leader 响应 OK 与 follower 2 接受并处理完毕 write 请求之间是存在延迟的，尽管多数数据库系统的 replication 的过程是非常快的，然而这个时间的上限谁也并没有保证。follower 可能刚刚重启、follower 系统满负载了、leader 到 follower 之间的网络出现故障了等等，都有可能导致 follower 的数据落后 leader 1分钟或者多分钟。\n同步 replication 的优势在于 follower 总能保持和 leader 实时的一致性的数据，如果 leader 突然不可用了，我们可以确保数据在 follower 上仍然是可用的。缺点在于如果 follower 出现了问题，比如 crash 了，网络故障了，写这个操作就没办法进行下去了，leader 此刻必须阻塞等待 replica 变为可用状态才能继续处理其他 write 操作。\n所以，维持整个系统的所有 replica 一致性状态是不切实际的。实际上，如果你在数据库中启用了同步 replication，它通常是指指定一个 follower 变为同步的，其它的 follower 依然是异步的，如果同步的 follower 变得不可用或者速度变慢，那么某一个异步的 follower 将会晋升为同步的 follower，这保证了总是有两个节点 leader 和一个 follower 始终维持最新的数据。这种配置有时候也称之为 semi-synchronous。\n通常，leader-based replication 被配置为全部是异步的，如果 leader 一旦失败不可恢复，那么所有的还未同步给 follower 节点的写数据将会丢失，这也就意味着写不保证一定写成功。它的优势在于即使所有的 follower 都不可用，leader 依然可以处理写请求。\n 对于异步 replication 来说，leader 失败后的数据丢失是很严重的，所以当前也有针对其不丢失数据同时提供良好性能和可用性的方法。例如链 replication ，其是同步复制的一个变种，它已经成功应用在 Microsoft Azure Storage 系统中。\n 配备新的 follower  定时对 leader 的数据库构建快照。 将快照拷贝到 follower 机器上。 follower 连接上 leader，请求所有自快照到最新版本之间变更的数据，这就需要快照本身关联到 leader 的 replication log 的一个精确的位置上。PostgreSQL 称之为 log sequence number，MySQL 称之为 binlog coordinates。 当 follower 处理完自 snapshot 创建之后堆积的数据变化后，我们称它追赶上了 leader。接下来，它继续处理来自 leader 发送过来的 log 即可。  处理节点中断 我们的目标是在单个节点发生故障的情况下，保持系统作为一个整体运行，并使节点中断的影响尽可能小。\nfollower 失败: 赶快恢复 follower 每次收到 log 之后，都会先记录在本地磁盘中，所以当它恢复之后也能很快知道它发生故障时正在处理的最后一个事务，它可以继续连接上 leader，然后请求在这之间发生的数据变更的 log ，当它应用这些 log 后，也就追上了 leader，意味着可以继续像之前一样继续接受数据流了。\nleader 失败: 故障转移 leader 失败后: 某一个 follower 被提拔为 leader，客户端需要将 write 请求发送至新的 leader，其它 follower 需要从新的 leader 消费数据，这个过程称之为 failover (故障转移)。\nfailover 既可以自动执行，也可以手动执行，一个自动执行的 failover 包含如下步骤:\n 检测 leader 是否失败了。多数都是使用超时时间来检测节点是否存活。 选择新的 leader。这可以通过一个选举过程来完成（在这种过程中，leader 是由剩余副本的大多数选择的），也可以由先前选择的控制器节点指定新的 leader。leader 的最佳人选通常是具有最新数据更改的副本（以尽量减少数据丢失）。让所有节点都同意一个新的 leader 是一个共识问题。 重新配置系统以使用新的领导者。client 现在需要将他们的写请求发送给新的 leader 。如果原来的 leader 恢复了，它可能仍然相信自己是 leader，却没有意识到其他 replicas 已经迫使它下台了。系统需要确保旧的 leader 成为 follower，并识别新的 leader。  故障切换充满了风险：\n 如果采用的是异步 replication，那么新推选出来的 leader 可能还没有接收完所有的 old leader 在失败前发送的写请求。如果前一个 leader 又回来了，那么应该如何协调？新的 leader 同样可能收到写请求。最常见的做法是，直接丢弃 old leader 未被 replicated 的写请求，但这可能和客户端的高可用性原则相违背。 如果数据库之外的其他存储系统需要和数据库内容进行协调，丢弃写请求则是非常危险的行为。 某些情况下，可能出现两个节点都认为自己是 leader 的情况，这称之为 split brain。这是及其危险的，如果二者均接受到了写的请求，而又没有其他进程去协调，那么数据有可能出现混乱或者丢失。作为一种安全保护，部分系统在检测到出现了两个 leader 的时候，会强制性关闭某个 leader，但是如果设计的不好的话，也可能会出现两个 leader 都被关闭的尴尬状态。 宣告 leader 死亡的合适的超时时间是多久？时间久一点的意味着久一点才能恢复，时间短一点的，会造成不必要的故障切换，占用不必要的网络带宽和系统资源。  这些问题并没有简单的方法来去解决。鉴于此，部分系统默认需要手动进行故障切换，即使它本身能够支持自动鼓掌切换。\n实现 replication log 面向语句的 replication 最简单的是，将每次需要执行的语句 (statement) 的 log 发送给 follower。在关系型数据库中，每一次的 INSERT、UPDATE、DELETE 等语句需要发送给 follower，follower 需要解析并处理这些语句。这种方式听起来还可以，然而还是有一些情况会出现问题:\n 某些语句调用了 NOW() 或者 RAND() 这样的函数，每次得到的都是不同的数值，应该如何同步过去？ 如果语句使用了自增列，或者他们依赖数据中已有的数据进行判断 (UPDATE \u0026hellip; WHERE 某些条件)，他们就必须在每一个 replica 上严格按照顺序执行，否则就可能产生不同的结果，这显然不利于并发事务的执行。 有副作用的语句 (触发器、存储过程、用户自定义函数等) 可能在不同节点上会造成不同的副作用，除非这种副作用是明确的，能提前预测到的。  解决这些问题是可能的，例如，当记录语句时，leader 可以用一个固定的返回值替换任何不确定的函数调用，以便 follower 都获得相同的值。但是，由于存在太多的边缘情况，现在通常首选其他复制方法。\n基于语句的复制在 5.1 版之前的MySQL中使用。由于它非常紧凑，所以现在仍然使用它，但是默认情况下，如果语句中存在任何不确定性，MySQL 现在会切换到基于行的复制。\n预写日志 (WAL)\nWAL 中的内容: 哪一块磁盘的哪几个字节的数据被修改了，这使得 WAL 日志和存储引擎关联到一起了。如果数据库将存储格式换为另外一个版本，那么不太可能运行不同的数据库软件。\n如果复制协议允许 follower 使用更新的软件版本与 leader 相比，您可以执行数据库软件的零停机升级，方法是先升级 follower，然后执行故障转移，使升级的节点之一成为新的leader。如果复制协议不允许这种版本不匹配，就像 WAL-shipping 的情况一样，这种升级需要停机。\n逻辑(基于行) 的 log replication 另一种方法是对复制和存储使用不同的日志格式引擎，它允许复制日志与存储引擎内部分离。这种复制日志称为逻辑日志，以区别于存储引擎的（物理）数据表示。\n关系数据库的逻辑日志通常是一系列记录，以行的粒度描述对数据库表的写入：\n 对于 INSERT 的行，日志包含所有列的新值。 对于 DELETE 的行，日志包含的信息足以唯一标识已删除的行。通常这是主键，但如果表上没有主键，则需要记录所有列的旧值。 对于 UPDATE 的行，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少是更改的所有列的新值）。  一个修改多行的事务会生成多个这样的日志记录，后面跟着一个指示事务已提交的记录。MySQL的 binlog（当配置为使用基于行的复制时）使用的就是这种方法。\n由于逻辑日志与存储引擎内部分离，因此可以更容易地保持向后兼容，从而允许 leader 和 follower 运行不同版本的数据库软件，甚至不同的存储引擎。\n逻辑日志格式也更易于外部应用程序解析。如果要将数据库的内容发送到外部系统（如用于脱机分析的数据仓库或用于构建自定义索引和缓存）的话，此特性非常有用。这种技术称为变更数据捕获。\n基于触发的 replication 到目前为止描述的复制方法是由数据库系统实现的，不涉及任何应用程序代码。在许多情况下，这正是您想要的，但有些情况下需要更大的灵活性。例如，如果您只想复制数据的一个子集，或者想从一种数据库复制到另一种数据库，或者需要冲突解决逻辑，则可能需要将复制向上移动到应用程序层。\n一些工具，如 Oracle GoldenGate，可以通过读取数据库日志使数据更改对应用程序可用。另一种方法是使用许多关系数据库中可用的特性：触发器和存储过程。\n触发器允许您注册在数据库系统中发生数据更改（写入事务）时自动执行的自定义应用程序代码。可以把它从一个外部进程读成一个独立的进程，这个进程可以从这个进程中读出来。然后，该外部进程可以应用任何必要的应用程序逻辑，并将数据更改复制到另一个系统。例如，Oracle的 Databus 和 Postgres 的 Bucardo 都是这样工作的。\n基于触发器的复制通常比其他复制方法的开销更大，而且比数据库的内置复制更容易出现错误和限制。然而，由于其灵活性，它仍然是有用的。\n复制延迟问题 基于 Leader 的复制要求所有写操作都通过单个节点，但只读查询可以转到任何副本。对于主要由读操作和少量写入操作组成的工作负载（web上的一种常见模式），有一个很有吸引力的选项：创建许多 follower，并在这些 follower 之间分发读取请求。这将从 leader 中移除负载，并允许附近的副本为读请求提供服务。\n在这种读扩展架构中，只需添加更多 follower，就可以增加为只读请求提供服务的容量。但是，这种方法只适用于异步复制，如果您尝试同步复制到所有 follower，单个节点故障或网络中断将使整个系统无法写入。你拥有的节点越多，就越有可能出现故障，因此完全同步的配置就非常不可靠。\n不幸的是，如果应用程序从异步 follower 读取数据，那么如果 follower 落后，它可能会看到过时的信息。这会导致数据库中明显的不一致性：如果同时对 leader 和 follower 运行相同的查询，可能会得到不同的结果，因为并非所有的写入都能即时地反映在 follower 中。这种不一致只是一种暂时的状态，如果你停止对数据库的写入并等待一段时间，follower 最终会赶上并与 leader 保持一致。因此，这种效果被称为最终一致性。\n“最终”一词故意含糊其辞：一般来说，replica 节点的落后程度没有限制。在正常操作中，在主服务器上执行写入操作与在从服务器上反映复制延迟之间的延迟可能只有几秒钟，在实践中并不明显。但是，如果系统在接近负载容量的情况下运行，或者网络出现问题，延迟很容易增加到几秒钟甚至几分钟。\n当滞后如此之大时，它所带来的不一致性不仅仅是一个理论问题，而是一个实际的应用问题。在本节中，我们将重点介绍三个复制延迟时可能出现的问题，并概述一些解决这些问题的方法。\n读取你自己写入的数据 许多应用程序允许用户提交一些数据，然后查看他们提交的内容。这可能是客户数据库中的一条记录，或者是论坛上的评论，或者其他类似的东西。当新数据被提交时，它必须被发送给 leader，但是当用户查看数据时，它可以从 follower 那里读取。如果经常查看数据但只是偶尔写入数据，这一点尤其合适。\n在这种情况下，我们需要 read-after-write 一致性，也称为 read-your-writes 一致性。这是一个保证，如果用户重新加载页面，他们将始终看到自己提交的任何更新。它对其他用户不做任何承诺：其他用户的更新可能要等到以后才能看到。但是，它保证用户自己的输入已经正确保存。\n如何在 leader-based 系统中实现 read-after-write 一致性:\n 读取用户可能已修改的内容时，请从 leader 那读取；否则，从 follower 那里读。这需要您有某种方法来知道某些内容是否已被修改，而无需实际查询它。例如，社交网络上的用户配置文件信息通常只能由个人资料的所有者编辑，而不能由其他任何人编辑。因此，一个简单的规则是：始终从 leader 处读取用户自己的配置文件，从 follower 读取任何其他用户的配置文件。 如果应用程序中的大多数内容都有可能被用户编辑，那么这种方法就不会有效，因为大多数内容都必须从 leader 那里读取（否定了读伸缩的好处）。在这种情况下，可以使用其他标准来决定是否向 leader 读取。例如，您可以跟踪最后一次更新的时间，并在最后一次更新后的一分钟内，从 leader 读取所有数据。您还可以监视 followers 上的复制延迟，并防止对任何落后于 leader 一分钟以上的 follower 进行查询。 客户端可以记住其最近写入的时间戳，然后系统可以确保服务于该用户的任何读取的副本，至少要能够反映出那个时间点之前的更新都是最新的。如果副本不够及时，则可以由另一个副本处理读取，或者查询可以等到复制副本追上那个时间点为止。时间戳可以是逻辑时间戳（指示写入顺序的东西，例如日志序列号）或实际系统时钟（在这种情况下时钟同步变得至关重要）。 如果您的副本分布在多个数据中心（地理位置上更接近用户或更可用），还有额外的复杂性。任何需要由 leader 提供服务的请求都必须路由到包含该 leader 的数据中心。  当同一个用户从多个设备（例如桌面 web 浏览器和移动应用程序）访问您的服务时，另一个复杂的问题就会出现。在这种情况下，您可能需要提供跨设备的 read-after-write 一致性：如果用户在一个设备上输入一些信息，然后在另一个设备上查看它，那么他们应该看到他们刚刚输入的信息。\n这种情况下，还需要考虑其它问题:\n 需要记住用户上次更新的时间戳的方法变得更加困难，因为在一个设备上运行的代码不知道另一个设备上发生了什么更新。这个元数据需要集中化。 如果复制副本分布在不同的数据中心，则无法保证来自不同设备的连接将路由到同一个数据中心。（例如，如果用户的桌面计算机使用家庭宽带连接，而他们的移动设备使用蜂窝数据网络，则设备的网络路由可能完全不同。）如果您的方法需要从 leader 处读取，则您可能首先需要将请求从用户的所有设备路由到同一数据中心。  单调读 我们的第二个例子是，当从异步 follower 读取数据时，可能会发生异常，即用户可能会看到时间线上向后移动的东西。\n如果用户从不同的副本进行多次读取，就会发生这种情况。例如，图 5-4 显示用户 2345 对同一个查询进行了两次，首先是对延迟较小的 follower，然后是延迟较大的 follower。（如果用户刷新一个网页，并且每个请求都被路由到一个随机服务器，这种情况很可能发生。）第一个查询返回用户 1234 最近添加的评论，但是第二个查询没有返回任何内容，因为滞后的 follower 还没有接收到该写入。实际上，第二个查询是在比第一个查询更早的时间点来观察系统。如果第一个查询没有返回任何内容，这不会太糟糕，因为用户 2345 可能不知道用户 1234 最近添加了一个评论。然而，如果用户 2345 首先看到用户 1234 的评论出现，然后又看到它消失，这会让他们非常困惑。\n单调读保证了这种异常不会发生。这是比强一致性弱的保证，但比最终一致性更有力的保证。当您读取数据时，您可能会看到一个旧值；单调读取仅意味着如果一个用户按顺序进行多次读取，他们将看不到时间倒流，也就是说，在先前读取了较新的数据之后，他们不会再读取较旧的数据。\n实现单调读取的一种方法是确保每个用户总是从同一个副本进行读取（不同的用户可以从不同的副本读取）。例如，可以根据用户 ID 的哈希值选择副本，而不是随机选择。但是，如果该复制副本失败，则需要将用户的查询重新路由到另一个副本。\n一致的前缀读取 复制滞后异常的第三个例子涉及违反因果关系。你有可能先看到答案而非问题：\n防止这种异常需要另一种类型的保证：一致的前缀读取。这个保证说，如果一个写入序列按一定的顺序发生，那么任何阅读这些写入的人都会看到它们以相同的顺序出现。\n这是分区（分片）数据库中的一个特殊问题，我们将在第 6 章中讨论。如果数据库总是以相同的顺序应用写操作，那么读取总是会看到一致的前缀，因此不会发生这种异常情况。但是，在许多分布式数据库中，不同的分区是独立运行的，因此没有全局写入顺序：当用户从数据库中读取时，他们可能会看到数据库的某些部分处于旧状态，而另一些部分处于较新状态。\n一种解决方案是确保任何与彼此有因果关系的写操作都写入同一个分区，但在某些应用程序中无法有效地执行。还有一些算法可以明确地跟踪因果依赖关系，我们后续继续讨论。\n复制滞后的解决方案 在使用最终一致的系统时，如果复制延迟增加到几分钟甚至几个小时，那么应该考虑应用程序的行为。如果答案是“没问题”，那就太好了。但是，如果结果对用户来说是一种不好的体验，那么重要的是要设计出一个更强大的保证，比如先读后写。假装复制是同步的，而实际上它是异步的，这是导致问题的根本原因。\n如前所述，应用程序可以提供比底层数据库更强的保证，例如，通过对 leader 只执行某些类型的读取。然而，在应用程序代码中处理这些问题很复杂，而且很容易出错。\n如果应用程序开发人员不必担心细微的复制问题，而只需信任他们的数据库“做正确的事情”，那就更好了。这就是为什么存在事务：事务是数据库提供更强大保证的一种方式，这样应用程序可以更简单。\n单节点事务已经存在很久了。然而，在向分布式（复制和分区）数据库转移的过程中，许多系统放弃了它们，声称事务在性能和可用性方面过于昂贵，并断言在可伸缩系统中，最终的一致性是不可避免的。这句话有些道理，但过于简单化了，我们将在本书的其余部分形成一个更细致的观点。\n多 Leader 复制 Leader-based 的复制有一个主要的缺点：只有一个 leader，所有写入都必须经过它。如果你因为任何原因无法连接到 leader，例如由于你和 leader 之间的网络中断，你就不能写入数据库。\n基于 leader 的复制模型的一个自然扩展是允许多个节点接受写操作。复制仍然以同样的方式进行：每个处理写操作的节点都必须将数据更改转发给所有其他节点。我们称之为多 leader 配置（也称为主-主或主动/主动复制）。在此设置中，每个 leader 同时充当其他 leader 的 follower。\n多 Leader 复制的案例 （1）多数据中心\n假设您有一个数据库，在多个不同的数据中心中都有副本（可能是为了让您能够容忍整个数据中心的故障，或者是为了更接近您的用户）。对于普通的基于 leader 的复制设置，leader 必须位于其中一个数据中心中，并且所有的写操作都必须经过该数据中心。\n在多 leader 配置中，每个数据中心都可以有一个 leader。下图显示了这个架构可能是什么样子。在每个数据中心内，使用常规的主从复制；在数据中心之间，每个数据中心的领导者将其更改复制到其他数据中心的领导者。\n让我们比较一下 single-leader 和多 leader 配置在多数据中心部署中的表现：\n 性能  在单 leader 配置中，每次写入都必须通过网络发送到数据中心。这可能会给写入操作增加很大的延迟，并且可能与首先拥有多个数据中心的目的相抵触。在多 leader 配置中，每个写入都可以在本地数据中心进行处理，并异步复制到其他数据中心。因此，数据中心间的网络延迟对用户是隐藏的，这意味着感知性能可能更好。\n 数据中心中断容忍度  在单 leader 配置中，如果具有该 leader 的数据中心发生故障，则进行故障转移，可以将另一个数据中心的 follower 提升为 leader。在多 leader 配置中，每个数据中心都可以独立于其他数据中心继续运行，当出现故障的数据中心恢复联机时，复制将迎头赶上。\n 网络问题容忍度  数据中心之间的通信通常通过公共互联网，这可能不如数据中心内的本地网络可靠。单 leader 配置对这个数据中心间链路中的问题非常敏感，因为写操作是在这个链路上同步进行的。具有异步复制的多 leader 配置通常可以更好地容忍网络问题：临时网络中断不会阻止正在处理的写操作。\n一些数据库默认支持多 leader 配置，但也经常使用外部工具来实现，例如用于 MySQL 的 Tungsten Replicator、用于 PostgreSQL 的 BDR 和用于 Oracle 的 GoldenGate。\n虽然多 leader 复制有其优点，但也有一个很大的缺点：相同的数据可能在两个不同的数据中心中被并发修改，并且这些写入冲突必须得到解决。\n（2）脱机操作的客户端\n另一种适合多 leader 复制的情况是，如果您的应用程序需要在与网络断开连接的情况下继续工作。\n例如，考虑一下手机、笔记本电脑和其他设备上的日历应用程序。您需要能够随时查看您的会议（发出读取请求）和输入新会议（发出写入请求），而不管您的设备当前是否具有网络连接。如果在脱机时进行了任何更改，则需要在设备下次联机时与服务器和其他设备同步。\n在这种情况下，每个设备都有一个本地数据库充当 leader 服务器（它接受写入请求），并且在所有设备上的日历副本之间有一个异步多 leader 复制过程（sync）。复制延迟可能是数小时甚至几天，这取决于您何时可以访问网络。\n从体系结构的角度来看，这种设置本质上与数据中心之间的多 leader 复制相同，达到了极致：每个设备都是一个“数据中心”，它们之间的网络连接极其不可靠。正如有很多历史记录需要同步，复制是一个棘手的问题。\n有一些工具旨在使这种多 leader 配置更容易。例如，CouchDB 就是为这种操作模式而设计的。\n（3）协同编辑\n实时协作编辑应用程序允许多人同时编辑文档。例如，Etherpad 和 Google Docs 允许多人同时编辑文本文档或电子表格。\n我们通常不认为协作编辑是数据库复制问题，但它与前面提到的脱机编辑用例有很多共同之处。当一个用户编辑文档时，更改会立即应用到其本地副本（文档在其web浏览器或客户端应用程序中的状态），并异步复制到服务器和正在编辑同一文档的任何其他用户。\n如果要保证不会发生编辑冲突，则应用程序必须先获得文档的锁定，然后用户才能编辑它。如果另一个用户想要编辑同一个文档，他们首先必须等到第一个用户提交了他们的更改并释放了锁。此协作模型相当于在主节点上使用事务进行单 leader 复制。\n但是，为了更快的协作，您可能希望将更改单元设置得非常小（例如，一次击键）并避免锁定。这种方法允许多个用户同时编辑，但也带来了多 leader 复制的所有挑战，包括需要解决冲突。\n处理写入冲突 多 leader 复制的最大问题是可能会发生写入冲突，这意味着需要解决冲突。\n例如，考虑一个由两个用户同时编辑的 wiki 页面，如图所示。用户1将页面标题从A更改为B，用户2同时将标题从A更改为C。每个用户的更改都成功地应用于其本地 leader。但是，当异步复制更改时，会检测到冲突。此问题不会出现在单个 leader 数据库中。\n（1）同步冲突检测与异步冲突检测\n在单 leader 数据库中，第二个写入程序将阻塞并等待第一次写入完成，或者中止第二次写入事务，迫使用户重试写入。另一方面，在多 leader 设置中，两次写入都是成功的，并且冲突只在稍后的某个时间点异步检测到。此时，要求用户解决冲突可能为时已晚。\n原则上，您可以使冲突检测同步，即等待写入复制到所有副本，然后再告诉用户写入成功。但是，这样做，您将失去多 leader 复制的主要优势：允许每个复制副本独立地接受写操作。如果您希望使用单次复制，那么也可以使用单次复制。\n（2）避免冲突\n处理冲突的最简单策略是避免冲突：如果应用程序可以确保对特定记录的所有写入都经过同一个 leader，那么就不会发生冲突。由于许多多 leader 复制的实现处理冲突的能力相当差，因此避免冲突是一种经常被推荐的方法。\n例如，在一个用户可以编辑自己的数据的应用程序中，可以确保来自特定用户的请求始终路由到同一个数据中心，并使用该数据中心中的 leader 进行读写。不同的用户可能有不同的“家庭”数据中心（可能是根据地理位置与用户的接近程度来选择的），但是从任何一个用户的角度来看，配置基本上是单一 leader 的。\n但是，有时您可能需要更改记录的指定 leader，可能是因为一个数据中心发生故障，您需要将流量重新路由到另一个数据中心，或者可能是因为某个用户已移动到另一个位置，现在离另一个数据中心更近。在这种情况下，避免冲突就会失效，您必须处理在不同领导人身上并发写的可能性。\n（3）向一致状态汇聚\n单个 leader 的数据库按顺序执行写入：如果对同一字段进行多次更新，则最后一次的写入将确定字段的最终值。\n在多 leader 配置中，没有定义写入顺序，因此不清楚最终值应该是什么。在上图中，在 leader 1，标题首先更新为B，然后更新为C；在leader 2，标题首先更新为C，然后更新到B。两个顺序都不比另一个“更正确”。\n如果每个副本只是按照它看到写操作的顺序来执行写操作，那么数据库将以不一致的状态结束：在leader 1处，最终值为C，在leader 2处为B。这是不可接受的，每个复制方案都必须确保所有副本中的数据最终都是相同的。因此，数据库必须以一种收敛的方式解决冲突，这意味着当所有更新都被复制到子节点的时候，所有副本必须达到相同的最终值。\n解决冲突的方法多种多样：\n 给每个写操作一个唯一的ID（例如，一个时间戳、一个长随机数、一个UUID或一个键和值的散列），选择ID最高的写入作为赢家，并丢弃其他写操作。如果使用时间戳，这种技术称为最后写入胜利（LWW: last write wins）。尽管这种方法很流行，但它很容易导致数据丢失。 为每个副本指定一个唯一的ID，并让源于编号较高的副本的写入始终优先于源于编号较低的副本的写入。这种方法还意味着数据丢失。 以某种方式将这些值合并在一起—例如，按字母顺序排序，然后将它们连接起来（在上图中，合并的标题可能类似于“B/C”）。 在保留所有信息的显式数据结构中记录冲突，并编写稍后解决冲突的应用程序代码（可能通过提示用户）。  （4）自定义冲突解决逻辑\n由于解决冲突的最合适方法可能取决于应用程序，大多数多 leader 复制工具都允许您使用应用程序代码编写冲突解决逻辑。该代码可以在写入或读取时执行：\n当写入的时候: 一旦数据库系统在复制的更改日志中检测到冲突，它就会调用冲突处理程序。例如，Bucardo 允许您为此编写一个 Perl 代码片段。此处理程序通常无法提示在后台进程中运行的用户，必须快速执行。\n当读的时候: 当检测到冲突时，将存储所有冲突的写入。下次读取数据时，这些数据的多个版本将返回到应用程序。应用程序可以提示用户或自动解决冲突，并将结果写回数据库。例如，CouchDB 就是这样工作的。\n请注意，冲突解决通常适用于单个行或文档级别，而不是整个事务。因此，如果您有一个事务以原子方式进行多个不同的写入，出于解决冲突的目的，每个写入操作仍然是单独考虑的。\n（5）冲突是什么\n有些冲突是显而易见的。在上图的例子中，两个写操作同时修改了同一记录中的同一个字段，并将其设置为两个不同的值。毫无疑问，这是一场冲突。\n其他类型的冲突可能更容易察觉。例如，考虑一个会议室预订系统：它跟踪哪个房间是由哪些人在某个时间预订的。此应用程序需要确保每个房间在同一时间仅由一组人预订（即，同一房间不得有任何重叠预订）。在这种情况下，如果同时为同一房间创建了两个不同的预订，则可能会发生冲突。即使应用程序在允许用户进行预订之前检查可用性，但如果两个预订是在两个不同的 leader 上进行的，则可能会发生冲突。\n没有一个现成的快速答案，但是在下面的章节中，我们将追溯到一个好的理解这个问题的路径。\n多 leader 复制拓扑 复制拓扑描述了写入从一个节点传播到另一个节点的通信路径。如果您有两个 leader，如上图所示，那么只有一个合理的拓扑：leader 1必须将它的所有写操作发送给leader 2，反之亦然。对于两个以上的 leader，可以使用各种不同的拓扑结构。一些例子如下图所示:\n最普遍的拓扑结构是 all-to-all（图[c]），在这种拓扑中，每一个领导者都会将其写的内容发送给其他每一个领导者。然而，更严格的拓扑结构也有人使用：例如，MySQL默认只支持一个循环拓扑，其中每个节点接收来自一个节点的写操作，并将这些写入操作（加上自身的任何写入操作）转发到另一个节点。另一种流行的拓扑结构是星形的：一个指定的根节点将写操作转发给其他所有节点。星型拓扑可以推广到树。\n在圆形和星形拓扑中，一个写操作可能需要经过几个节点才能到达所有副本。因此，节点需要转发从其他节点接收到的数据更改。为了防止无限的复制循环，每个节点都有一个唯一的标识符，并且在复制日志中，每个写操作都用它经过的所有节点的标识符进行标记。当一个节点接收到用自己的标识符标记的数据更改时，该数据更改将被忽略，因为该节点知道它已被处理。\n循环拓扑和星型拓扑的一个问题是，如果只有一个节点发生故障，它可能会中断其他节点之间的复制消息流，导致它们在节点修复之前无法通信。可以对拓扑进行重新配置，以解决出现故障的节点，但在大多数部署中，这种重新配置必须手动完成。更密集连接的拓扑（如all-to-all）的容错性更好，因为它允许消息沿着不同的路径传输，避免了单点故障。\n另一方面，all-to-all 拓扑也可能有问题。特别是，某些网络链路可能比其他链路快（例如，由于网络拥塞），结果是一些复制消息可能“超过”其他的，如图所示。\n在上图中，client A 在 leader 1 的表中插入一行，client B 更新 leader 3 上的该行。但是，leader 2 可能以不同的顺序接收写入：它可能首先接收更新（从它的角度来看，这是对数据库中不存在的行的更新），然后才接收相应的 insert（应该在更新之前）。\n这是一个因果关系的问题，类似于我们在“一致前缀读取”中看到的问题：更新依赖于先前的插入，所以我们需要确保所有节点首先处理插入，然后处理更新。仅仅在每次写操作上附加一个时间戳是不够的，因为不能相信时钟足够同步，以便在 leader 2 中正确地对这些事件排序。\n为了对这些事件进行正确的排序，可以使用一种称为版本向量的技术，我们将在本章后面讨论。然而，在许多多 leader 复制系统中，冲突检测技术的实现并不理想。例如，在撰写本文时，PostgreSQL BDR 不提供写入的因果顺序，Tungsten Replicator For MySQL 甚至不尝试检测冲突。\n如果您使用的是具有多 leader 复制的系统，那么有必要了解这些问题，仔细阅读文档，并彻底测试数据库，以确保它确实提供了您认为它具有的保证。\n无 leader 复制 到目前为止，我们在本章中讨论的复制方法是基于这样一种思想：客户端向一个节点（主站）发送写请求，而数据库系统负责将写操作复制到其他副本。leader 决定了处理写操作的顺序，follower 以相同的顺序执行 leader 的写操作。\n有些数据存储系统采用了不同的方法，放弃了 leader 的概念，允许任何复制副本直接接受来自客户端的写操作。一些最早的复制数据系统是无领导的，但在关系数据库占主导地位的时代，这一想法基本上被遗忘了。在亚马逊将其用于内部发电机系统之后，它再次成为一种流行的数据库架构。Riak、Cassandra 和 Voldemort 都是源代码开放的数据存储库，采用的是受 Dynamo 启发的无领导复制模型，因此这种数据库也被称为 Dynamo 样式。\n在一些无领导的实现中，客户端直接将其写操作发送到多个副本，而在其他一些实现中，协调器节点代表客户端执行此操作。但是，与 leader 数据库不同，该协调器不强制执行特定的写入顺序。我们将看到，这种设计上的差异对数据库的使用方式有着深远的影响。\n节点 down 时写入数据库 假设您有一个包含三个副本的数据库，其中一个副本当前不可用—可能它正在重新启动以安装系统更新。在基于 leader 的配置中，如果要继续处理写操作，则可能需要执行故障转移。\n另一方面，在无 leader 配置中，不存在故障转移。下图显示了发生的情况：客户端（用户1234）并行地向所有三个副本发送写操作，两个可用副本接受写入操作，但未写入不可用的副本。假设三个副本中有两个确认写入就足够了：在用户1234收到两个ok响应之后，我们认为写入是成功的。客户端只是忽略了其中一个副本没有写入的事实。\n现在假设不可用的节点重新联机，客户端开始从中读取数据。节点关闭时发生的任何写入操作都将从该节点中丢失。因此，如果从该节点读取，可能会得到过时（过时）的值作为响应。\n为了解决这个问题，当客户端从数据库中读取数据时，它不仅仅将其请求发送到一个副本：读取请求还被并行地发送到多个节点。客户端可能从不同的节点得到不同的响应；即，一个节点的最新值和另一个节点的过时值。版本号用于确定哪个值更新。\n（1）读修复与反熵\n复制方案应确保最终将所有数据复制到每个副本。当一个不可用的节点恢复联机后，它如何弥补它丢失的写操作？\nDynamo 式的数据存储中通常使用两种机制：\n读修复: 当客户端从多个节点并行读取时，它可以检测到任何过时的响应。例如，在上图中，用户2345从副本3获取版本6值，从副本1和副本2获取版本7值。客户端发现副本3有一个过时的值，并将较新的值写回该副本。这种方法适用于经常读取的值。\n反熵过程: 此外，一些数据存储有一个后台进程，该进程不断查找副本之间的数据差异，并将丢失的数据从一个副本复制到另一个副本。与基于 leader 的复制中的复制日志不同，此反熵过程不会以任何特定的顺序复制写操作，而且在复制数据之前可能会有一个明显的延迟。\n并不是所有的系统都实现了这两个功能；例如，Voldemort 目前还没有反熵过程。请注意，如果没有反熵过程，则很少读取的值可能会从某些副本中丢失，从而降低了持久性，因为只有在应用程序读取值时才会执行读取修复。\n（2）仲裁读写\n在上图的例子中，我们认为写入是成功的，即使它只在三个副本中的两个上处理。如果三个副本中只有一个接受了写操作呢？我们能强行得出这个结论吗？\n如果我们知道每一次成功的写入都保证在三个副本中至少有两个副本上，这意味着最多只有一个副本过时。因此，如果我们从至少两个副本中读取，我们就可以确定其中至少有一个是最新的。如果第三个副本停止运行或响应速度慢，则读取仍可以继续返回最新值。\n更一般地说，如果有 n 个副本，则每次写入都必须由 w 个节点确认为成功，并且每次读取都必须至少查询 r 个节点。（在我们的例子中，n = 3，w = 2，r = 2。）只要 w + r \u0026gt; n，我们就期望在读取时得到一个最新的值，因为至少有一个我们正在读取的 r 节点必须是最新的。遵循这些 r 和 w 值的读和写被称为仲裁读写。你可以把 r 和 w 看作读写有效所需的最低票数。\n在 Dynamo 风格的数据库中，参数 n、w 和 r 通常是可配置的。一个常见的选择是将 n 设为奇数（通常为 3 或 5），并设置 w = r =（n + 1）/2（四舍五入）。但是，您可以根据需要改变数字。例如，一个写得少而读得多的工作负载可以从设置 w = n 和 r = 1 中获益。这使得读取速度更快，但缺点是只有一个失败的节点会导致所有数据库写入失败。\n仲裁条件 w + r \u0026gt; n 允许系统容忍不可用节点，如下所示：\n 如果 w \u0026lt; n，如果某个节点不可用的时候，系统可以继续处理写请求 如果 r \u0026lt; n，如果某个节点不可用的时候，系统可以继续处理读请求 n = 3，w = 2，r = 2 我们可以容忍一个节点不可用 n = 5，w = 3，r = 3 我们可以容忍两个节点不可用 (见下图)  通常，读写总是并行地发送到所有 n 个副本。参数 w 和 r 决定了我们等待多少个节点，也就是说，在我们认为读或写成功之前，n 个节点中有多少个需要报告成功。\n如果可用的 w 或 r 节点少于所需数量，则写入或读取将返回错误。节点不可用的原因有很多：节点关闭（崩溃、断电）、执行操作时出错（磁盘已满而无法写入）、客户端和节点之间的网络中断，或其他任何原因。我们只关心节点是否返回成功的响应，而不需要区分不同类型的错误。\n仲裁一致性的限制 如果有 n 个副本，并且选择 w 和 r 使 w + r \u0026gt; n，则通常可以期望每次读取都返回为 key 写入的最新值。这种情况是因为您写入的节点集和从中读取的节点集必须重叠。也就是说，在您读取的节点中，必须至少有一个具有最新值的节点。\n通常，r 和 w 被选为节点的大多数（超过 n/2），因为这样可以确保 w + r \u0026gt; n，同时仍然可以容忍多达 n/2 个节点故障。但是 quorums 不一定是多数，它只关心读写操作使用的节点集至少在一个节点上重叠。其他的仲裁分配也是可能的，这使得分布式算法的设计具有一定的灵活性。\n您还可以将 w 和 r 设置为较小的数字，使 w + r ≤ n（即不满足法定人数条件）。在这种情况下，读和写操作仍将发送到 n 个节点，但操作成功所需的成功响应数较少。\n对于较小的 w 和 r，您更有可能读取过时的值，因为您的读取更有可能没有包含具有最新值的节点。有利的一面是，这种配置允许更低的延迟和更高的可用性：如果出现网络中断，并且许多副本变得无法访问，则您可以继续处理读写操作的可能性更大。只有当可访问副本的数量降到 w 或 r 以下时，数据库才会分别变得不可写入或读取。\n然而，即使 w + r \u0026gt; n ，也可能存在返回过时值的边缘情况。这些取决于实施情况，但可能的情况包括：\n 如果使用草率仲裁，w 写入可能会在不同的节点上结束，而 r 节点和 w 节点之间不再有保证的重叠。 如果两个写操作同时发生，则不清楚哪一个先发生。在这种情况下，唯一安全的解决方案是合并并发写入。如果根据时间戳（最后一次写入获胜）选择赢家，则可能会由于时钟偏差而丢失写入。 如果写操作与读操作同时进行，则写操作可能只反映在部分副本上。在本例中，无法确定读取是返回旧值还是新值。 如果在某些副本上写入成功，但在其他副本上失败（例如，由于某些节点上的磁盘已满），并且在少于 w 个副本上总体成功，则不会在成功的副本上回滚。这意味着，如果写入被报告为失败，随后的读取可能会返回该写入的值，也可能不会返回。 如果承载新值的节点出现故障，并且从承载旧值的副本恢复其数据，则存储新值的副本数可能会低于 w，从而破坏仲裁条件。 即使一切工作正常，也有一些边缘情况，你可能会在时间上不走运，正如我们将在后续章节“线性化和量化”中看到的。  因此，尽管 quorums 看起来保证读返回最新的写入值，但实际上并不简单。Dynamo 风格的数据库通常针对能够容忍最终一致性的用例进行优化。参数 w 和 r 允许您调整读取过时值的概率，但最好不要将它们作为绝对保证。\n特别是，您通常得不到“复制延迟的问题”中讨论的保证（读写、单调读或一致前缀读），因此前面提到的异常可能会出现在应用程序中。更强有力的担保通常需要事务或协商一致。\n检测落后\n从操作的角度来看，监视数据库是否返回最新结果非常重要。即使您的应用程序可以容忍过时的读取，您也需要了解复制的运行状况。如果它严重落后，它应该警告您，以便您可以调查原因（例如，网络中的问题或节点过载）。\n通常情况下，您可以将基于复制的度量标准暴露到基于复制的系统中。这是可能的，因为写入操作以相同的顺序应用于主节点和从节点，并且每个节点在复制日志中都有一个位置（它在本地应用的写入数）。通过从 Leader 的当前位置减去 Follower 的当前位置，可以测量复制延迟的量。\n但是，在采用无 Leader 复制的系统中，写入的应用顺序没有固定的顺序，这使得监视更加困难。此外，如果数据库只使用读取修复（不使用反熵），则某个值过期了多久是没有上限的。如果仅偶尔读取值，则过时副本返回的值可能为特别老的。\n对于如何测量无 Leader 复制的数据库的陈旧性，以及根据参数 n、w 和 r 预测预期的过时读取百分比，这里有一些论文研究。不幸的是，这还不是常见的实践，但最好在数据库的标准度量集合中包含过时度量。最终的一致性是故意含糊的保证，但对于可操作性来说，能够量化“最终”是很重要的。\nsloppy quorums and hinted handoff 数据库，配置一个较为合适的 quorum 值，可以在无需快速失败的情况下，容忍单个节点的故障。单个节点处理速度变慢也是能容忍的，因为无需等待 n 个节点都给予响应，只需 w 或 r 个节点响应了就可以了。这些特征使得无 Leader 复制的数据库，特别适合需要更高的可靠性和更低的延迟，以及能容忍偶然的过时读取的场景。\n然而，quorum 也不是那么容易就容忍故障的。网络故障可以轻而易举地切断一个客户端与后台大量的数据库节点之间的连接。尽管数据库节点依然存活，其它客户端也能连接上，但是对于这个节点而言，这大量的数据库节点就宛若 down 掉了似的。在这种情况下，可能剩余的数据库节点少于 w 或 r 个，所以此客户端永远无法达成一个 quorum 连接的数量。\n在一个大型集群中（节点数明显多于 n 个），客户机很可能在网络中断期间连接到某些数据库节点，而不是连接到需要为特定值聚集仲裁的节点。在这种情况下，数据库设计者面临着一种权衡：\n 当冲裁节点数量少于 w 或 r 个的时候，我们要对于所有的请求都返回错误吗？ 又或者我们应该对于写请求全盘接受，并将它们写入某些节点，这些节点是可访问的，但不在通常存在该值的 n 个节点中？  后者称之为 sloppy quorum (草率的大多数): 读和写仍然需要 r 和 w 个成功的响应，但是，这些节点可以包括不在指定的 n 个 “home” 节点中的节点。以此类推，如果你把自己锁在门外，你可能会敲邻居的门，问你是否可以暂时呆在他们的沙发上。\n一旦网络中断被修复，任何一个节点临时代表另一个节点接受的写操作都会被发送到相应的 “home” 节点。这被称之为 hinted handoff (暗示的交接) (一旦你再次找到房子的钥匙，你的邻居会礼貌地让你从他们的沙发上下来回家)。\nSloppy quorums 对于增加写可靠性是非常有用的: 只要有任何 w 节点可用，数据库就可以接受写操作。但是，这意味着即使 w + r \u0026gt; n，也不能确定要读取键的最新值，因为最新的值可能已临时写入 n 个节点以外的某些节点。\n因此，sloppy quorum 实际上根本不是传统意义上的法定人数。这只是持久性的保证，即数据存储在 w 节点的某个地方。在提示的切换完成之前，不能保证对 r 节点的读取会看到它。\nSloppy quorum 在 Dynamo 实现中一般是可选的。在 Riak 中默认被启用，在 Cassandra 和 Voldemort 中它们被默认禁止。\n多个数据中心的操作 我们以前讨论过将跨数据中心复制作为多 Leader 复制的一个用例。无 Leader 复制也适用于多数据中心操作，因为它被设计为能够容忍冲突的并发写入、网络中断和延迟峰值。\nCassandra 和 Voldemort 在正常的无领导模型中实现了他们的多数据中心支持：复制副本的数量 n 包括所有数据中心中的节点，并且在配置中，您可以指定每个数据中心中需要多少个 n 个副本。客户机的每次写入都被发送到所有副本，不管数据中心如何，客户机通常只等待来自其本地数据中心内的节点仲裁的确认，这样它就不会受到跨数据中心链路上的延迟和中断的影响。对其他数据中心的高延迟写入通常配置为异步进行，尽管配置中有一些灵活性。\nRiak 将客户机和数据库节点之间的所有通信保持在一个数据中心的本地，因此 n 描述了一个数据中心内的副本数量。数据库集群之间的跨数据中心复制在后台异步进行，其方式类似于多 Leader 复制。\n检测并发写 Dynamo 风格的数据库允许多个客户端同时写入同一个 Key，这意味着即使使用严格的 quorum，也会发生冲突。\n事件会以不同的顺序到达不同的节点，是因为网络延迟和部分失败的原因。\n如果每个节点在接收到来自 Client 的写请求时简单地重写了一个键的值，那么这些节点就会变得永久不一致，如图中的最终 get 请求所示：节点 2 认为 X 的最终值是 B，而其他节点认为该值是 A 。\n为了最终保持一致，副本应该向相同的值收敛。他们怎么做到的？人们可能希望复制的数据库能够自动处理这一问题，但不幸的是，大多数实现都很差：如果您希望避免丢失数据，那么应用程序开发人员需要了解数据库冲突处理的内部结构。\n最后一次写入赢 (丢弃并发写) 实现最终收敛的一种方法是声明每个副本只需存储最新的值，并允许覆盖和丢弃“旧”值。然后，只要我们有某种方法可以明确地确定哪个写操作更“最近”，并且每个写入操作最终都会复制到每个副本，那么这些副本最终将收敛到相同的值。\n正如 “最近” 这个词汇所指出的，这个想法实际上是相当误导人的。在上图中，两个 Client 在向数据库节点发送写请求时都不知道另一个 Client，所以不清楚哪个 Client 先发生了写操作。事实上，如果说这两种情况都是“先发生”的，这是没有意义的：我们说写入是并发的，所以它们的顺序是未定义的。\n即使写入没有自然顺序，我们也可以对它们强制执行任意顺序。例如，我们可以为每次写入附加一个时间戳，选择最大的时间戳作为最“最近”的时间戳，并丢弃任何具有较早时间戳的写入。这种冲突解决算法称为 last write wins（LWW），是 Cassandra 中唯一支持的冲突解决方法，也是 Riak 中的一个可选特性。\nLWW 实现了最终聚合的目标，但以持久性为代价：如果对同一个 Key 有多个并发写入，即使它们都被报告为成功（因为它们被写入 w 个副本中），那么只有一个写入将继续存活，而其他写入操作将被静默丢弃。此外，LWW 甚至可以删除非并发的写入。\n在某些情况下，例如缓存，丢失的写入可能是可以接受的。如果丢失数据是不可接受的，那么 LWW 是解决冲突的糟糕选择。\n在 LWW 中使用数据库的唯一安全方法是确保只写入一次 Key，然后将其视为不可变的，从而避免对同一个 Key 进行任何并发更新。例如，推荐使用 Cassandra 的方法是使用 UUID 作为键，从而给每个写操作一个唯一的键。\nhappens-before 关系和并发 如何确定两种操作是并发还是非并发？我们来看一些例子：\n 上图中，这两种操作不是并发的。B 的递增必须先发生于在 A 的插入操作之后，B 的操作建立在 A 操作之上。 而在上上图中展示的，两种写是并发的，两个客户端开始操作的时候，不知道对方正在对同一个 key 执行操作，所以这两种操作并无依赖关系。  如果 B 知道 A，或者依赖 A，或者以某种方式建立在 A 之上，则操作 A 发生在另一个操作 B 之前。一个操作是否先于另一个操作发生是定义并发性的关键。事实上，我们可以简单地说两个操作是并发的。，如果两个操作都不发生在另一个操作之前（也就是说，两个操作都不知道另一个操作）。\n因此，无论何时有两个操作 A 和 B，都有三种可能：A 发生在 B 之前，或者 B 发生在 A 之前，或者 A 和 B 是并发的。我们需要的是一个算法来判断两个操作是否并发。如果一个操作发生在另一个操作之前，后面的操作应该覆盖前面的操作，但是如果这些操作是并发的，那么就有一个冲突需要解决。\n同时性、时间和相对性\n如果两个操作“同时发生”，看起来应该称它们为并发的，但实际上，它们是否在时间上重叠并不重要。由于分布式系统中时钟的问题，实际上很难判断两件事是否同时发生——我们将在第8章更详细地讨论这个问题。\n对于定义并发性，确切的时间并不重要：如果两个操作都不知道彼此，那么我们就简单地将它们称为并发操作，而不管它们发生的物理时间。人们有时会将这一原理与物理学中的狭义相对论联系起来，狭义相对论引入了信息传播速度不能超过光速的概念。因此，相隔一定距离发生的两个事件不可能相互影响，如果两个事件之间的时间短于光在它们之间传播距离所需的时间。\n在计算机系统中，尽管光速原则上允许一个操作影响另一个操作，但两个操作可能是并发的。例如，如果当时网络速度慢或中断，两个操作可能会间隔一段时间，但仍然是并发的，因为网络问题使一个操作无法了解另一个操作。\n捕获 happens-before 关系 让我们看一个算法，它决定两个操作是并发的，还是一个操作在另一个操作之前发生。为了简单起见，让我们从只有一个副本的数据库开始。一旦我们找到了如何在单个副本上执行此操作，我们就可以将该方法推广到具有多个副本的无领导数据库。\n下图显示两个客户端同时向同一个购物车添加商品（如果您觉得该示例太空洞，请设想两个空中交通管制员同时向他们跟踪的扇区添加飞机。）最初，购物车是空的。在它们之间，客户端对数据库进行五次写入：\n Client 1 添加了 [milk]，这是第一次写入，所以服务器为其赋了一个 version 1。 Client 2 添加了 eggs，服务器赋了 version 2，然后把 [eggs]、[milk] 同时返回给 Client 2。 Client 1 不晓得 Client 2 的写入，它想要添加 flour，服务器赋了 version 3，将 version-3:[milk, floor] 和 version-2:[eggs] 同时返回给 Client 1。 Client 2 合并 [eggs]、[milk] 并添加 ham，为 [eggs, milk, ham]，并伴随着 version 2 发送给 server。server 检测到 version 2 覆盖了 [egg]，但是与 [milk, flour] 冲突，因此保留下来的值是: version-3: [milk, floor]、version-4: [eggs, milk, ham] 最后，Client 1 想要添加 bacon，以 version-3:[milk,flour,eggs,bacon] 发送给服务器，服务器最终保留 [eggs, milk, ham] 和 [milk,flour,eggs,bacon]。  上图操作之间的数据流如下图所示。箭头指示哪个操作发生在另一个操作之前，从某种意义上说，后面的操作知道或依赖于前一个操作。在这个例子中，客户机永远不会完全更新服务器上的数据，因为总是有另一个操作同时进行。但是旧版本的值最终会被覆盖，并且不会丢失任何写操作。\n注意，服务器可以通过查看版本号来确定两个操作是否并发，而不需要解释值本身（因此该值可以是任何数据结构）。算法的工作原理如下：\n 服务器为每个 Key 维护一个版本号，在每次写入 Key 时增加版本号，并存储新的版本号和写入的值。 当客户端读取 Key 时，服务器返回所有未重写的值以及最新版本号。客户端必须在写入前读取 Key。 当客户机写入密钥时，它必须包括先前读取的版本号，并且必须将先前读取的所有值合并在一起。（写请求的响应可以类似于 read，返回所有当前值，这允许我们像购物车示例中那样链接多个写操作。） 当服务器接收到具有特定版本号的写入时，它可以用该版本号或更低版本号覆盖所有值（因为它知道它们已合并到新值中），但它必须保留具有更高版本号的所有值（因为这些值与传入的写操作并发）。  当写操作包含先前读取的版本号时，它会告诉我们写操作基于的是以前的状态。如果写入时不包含版本号，则它与所有其他写入操作同时进行，因此不会覆盖任何内容，它只是在后续读取时作为值之一返回。\n合并并发写入的值 这个算法确保没有数据被静默地丢弃，但不幸的是，它需要客户端做一些额外的工作：如果多个操作同时发生，客户机必须在随后通过合并并发写入的值进行清理。Riak 称这些并发值为兄弟。\n合并同级值本质上与多 Leader 复制中的冲突解决问题是相同的，我们前面已经讨论过了（参见“处理写入冲突”）。一种简单的方法是根据版本号或时间戳（最后写入获胜）选择其中一个值，但这意味着数据丢失。因此，您可能需要在应用程序代码中做一些更智能的事情。\n以购物车为例，合并兄弟姐妹的一个合理方法是只接受联合。在上图中，最后两个兄弟是[milk, flour, eggs, bacon]和[egg, milk, ham]；注意 milk 和 eggs 出现在这两个词中，尽管它们各自只写了一次。合并后的值可能是[milk, flour, eggs, bacon, ham]，没有重复。\n但是，如果你想让人们也从他们的购物车中移除物品，而不仅仅是添加物品，那么使用兄弟的联合可能不会产生正确的结果：如果你合并了两个兄弟购物车，而其中一个物品只在其中一个中被移除，那么移除的物品将重新出现在兄弟的联合中。为了防止出现此问题，在删除项时，不能简单地从数据库中删除它；相反，系统必须留下一个带有适当版本号的标记，以指示在合并同级项时该项已被删除。这种删除标记被称为墓碑。（我们之前在“Hash index”中看到了日志压缩上下文中的逻辑删除。）\n由于在应用程序代码中合并同级项是复杂且容易出错的，因此有一些工作需要设计能够自动执行这种合并的数据结构，如“自动冲突解决”所述。例如，Riak 的数据类型支持使用一系列名为 CRDTs 的数据结构，这些结构可以以合理的方式自动合并同级，包括保留删除。\n版本向量 图中的例子只使用了一个副本。当有多个副本，但没有 Leader 时，算法如何改变？\n图中使用一个版本号来捕获操作之间的依赖关系，但是当有多个副本同时接受写操作时，这是不够的。相反，我们需要为每个副本和每个 Key 使用一个版本号。在处理写操作时，每个复制副本都会增加其自己的版本号，并且还会跟踪它从其他每个副本看到的版本号。此信息指示要覆盖的值以及作为同级保留的值。\n所有副本的版本号集合称为版本向量。这种想法的一些变体正在使用，但最有趣的可能是点式版本向量，它在 RIAK2.0 中使用。我们将不深入讨论细节，但它的工作方式与我们在cart示例中看到的非常相似。\n与图中的版本号一样，在读取值时，版本向量从数据库副本发送到客户端，在随后写入值时，需要将版本向量发送回数据库。（Riak 将版本向量编码为一个字符串，称之为因果上下文。）版本向量允许数据库区分重写和并发写入。\n此外，与单副本示例一样，应用程序可能需要合并兄弟版本向量结构确保从一个副本中读取并随后写回另一个副本是安全的。这样做可能会导致创建同级，但只要同级正确合并，就不会丢失任何数据。\n 版本向量和向量时钟\n 版本向量有时也被称为向量时钟，尽管它们并不完全相同。差别很小，详情请参阅参考文献[57，60，61]。简言之，在比较副本的状态时，版本向量是正确的数据结构。\n总结 几种不同的复制方法：\n 单 Leader 复制：客户机将所有写操作发送到单个节点（leader），该节点向其他副本（followers）发送数据更改事件流。可以对任何复制副本执行读取，但从 follower 中读取的可能已经过时。 多 Leader 复制：客户机将每个写操作发送到多个leader节点中的一个节点，其中任何一个节点都可以接受写操作。领导者向彼此和任何 follower 节点发送数据更改事件流。 无 Leader 复制：客户机将每次写入发送到多个节点，并从多个节点并行读取数据，以便检测和更正具有过时数据的节点。  每种方法都有优点和缺点。单 Leader 复制之所以流行，是因为它非常容易理解，而且不需要担心解决冲突。在存在故障节点、网络中断和延迟峰值的情况下，多 Leader 和无 Leader 的复制可以更加健壮，但代价是更难以解释，而且只能提供非常弱的一致性保证。\n复制可以是同步的，也可以是异步的，这对出现故障时的系统行为有着深远的影响。虽然异步复制在系统平稳运行时可能很快，但重要的是要弄清楚当复制延迟增加和服务器发生故障时会发生什么。如果一个leader失败，并且您将异步更新的 follower 提升为新的 leader，那么最近提交的数据可能会丢失。\n"});index.add({'id':213,'href':'/docs/programmer-interview/java/concurrenthashmap-18/','title':"ConcurrentHashMap 1.8",'content':"ConcurrentHashMap 1.8 计算 size() final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i \u0026lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } public int size() { long n = sumCount(); return ((n \u0026lt; 0L) ? 0 : (n \u0026gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } "});index.add({'id':214,'href':'/docs/programmer-interview/java/cyclicbarrier/','title':"CyclicBarrier",'content':"CyclicBarrier 作用 让一组线程等待某个事件(barrier)的发生。\n实现原理 在 dowait 方法中有如下计数器：\nint index = --count; if (index == 0) { // tripped  boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } 其中 runGeneration 或 breakBarrier 中有如下代码片段：\ntrip.signalAll(); trip 是信号量：\n/** Condition to wait on until tripped */ private final Condition trip = lock.newCondition(); "});index.add({'id':215,'href':'/docs/programmer-interview/front-end/layout-left-fix-right-responsive/','title':"左固定右自适应",'content':"两列布局：左固定，右自适应 \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt; \u0026lt;p\u0026gt;这是左边的盒子\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt; \u0026lt;p\u0026gt;这是右边的盒子\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 双 float .container::after { content: \u0026#34;\u0026#34;; display: block; clear: both; } .left, .right { box-sizing: border-box; float: left; } .right { width: calc(100% - 120px); } float + margin-left .container::after { content: \u0026#34;\u0026#34;; display: block; clear: both; } .left { box-sizing: border-box; float: left; } .right { margin-left: 120px; } absolute + margin-left .left { box-sizing: border-box; position: absolute; } .right { margin-left: 120px; } float + BFC .container::after { content: \u0026#34;\u0026#34;; display: block; clear: both; } .left { float: left; } .right { margin-left: 0; overflow: auto; }  BFC 可以阻止元素被浮动元素覆盖\n flex .container { display: flex; } .right { flex: 1 } "});index.add({'id':216,'href':'/docs/books/ddia/ddia-chapter6/','title':"设计数据密集型应用程序 - Partitioning",'content':"设计数据密集型应用程序 - Partitioning  在上一章中，我们讨论了复制，即在不同的节点上拥有相同数据的多个副本。对于非常大的数据集或非常高的查询吞吐量，这是不够的：我们需要将数据分成多个分区，也称为分片。\n 通常，分区的定义方式是，每一条数据（每个记录、行或文档）只属于一个分区。实现这一点有多种方法，我们将在本章中深入讨论。实际上，每个分区都是自己的小数据库，尽管数据库可能支持同时接触多个分区的操作。\n想要分区数据的主要原因是可伸缩性。不同的分区可以放在一个无共享集群中的不同节点。因此，大型数据集可以分布在多个磁盘上，查询负载可以分布在多个处理器上。\n对于在单个分区上操作的查询，每个节点都可以独立地为自己的分区执行查询，因此可以通过添加更多的节点来扩展查询吞吐量。大型、复杂的查询可以跨多个节点并行化，尽管这会变得非常困难。\n分区数据库在20世纪80年代由 Teradata 和 Tandem NonStop SQL[1] 等产品开创，最近又被 NoSQL 数据库和基于 Hadoop 的数据仓库重新发现。有些系统是为事务性工作负载而设计的，而其他系统则是为分析而设计的：这种差异会影响系统的优化方式，但是分区的基本原理适用于这两种工作负载。\n在这一章中，我们将首先了解划分大型数据集的不同方法，并观察数据索引与分区之间的交互作用。然后我们将讨论负载均衡，如果您想在集群中添加或删除节点，这是必需的。最后，我们将概述数据库如何将请求路由到正确的分区并执行查询。\n分区和复制 每个分区通常与复制节点的多个副本合并存储。尽管每个记录的容错性可能仍属于一个不同的节点，但这意味着每个记录的容错性仍然不同。\n一个节点可以存储多个分区。如果使用主从复制模型，分区和复制的组合可以如下图所示。每个分区的 Leader 被分配给一个节点，它的跟随者被分配给其他节点。每个节点可能是某些分区的 Leader 节点和其他分区的跟随节点。\n我们在第5章中讨论的关于数据库复制的所有内容都同样适用于分区的复制。分区方案的选择主要与复制方案的选择无关，因此在本章中我们将保持简单，而忽略复制。\n键值对数据的分区 假设你有大量的数据，你想对它进行分区。如何决定在哪些节点上存储哪些记录？\n我们分区的目标是将数据和查询负载均匀地分布在节点上。如果每个节点都得到公平的共享，那么理论上10个节点应该能够处理10倍于单个节点的数据量和10倍的读写吞吐量（暂时忽略复制）。\n如果分区不公平，使得一些分区比其他分区拥有更多的数据或查询，我们称之为倾斜分区。倾斜的存在使得分区变得更少有效。在一个极端的情况是，所有的负载都可能在一个分区上结束，因此10个节点中有9个是空闲的，而您的瓶颈是单个繁忙的节点。具有不成比例的高负载的分区称为热点。\n避免热点的最简单方法是将记录随机分配给节点。这样可以将数据均匀地分布在各个节点上，但它有一个很大的缺点：当您试图读取一个特定的项时，您无法知道它在哪个节点上，所以必须并行地查询所有节点。\n我们可以做得更好。现在我们假设您有一个简单的键值数据模型，在这个模型中，您总是通过主键访问记录。例如，在一本老式的纸质百科全书中，您按标题查找条目；由于所有条目都是按标题字母顺序排序的，因此您可以很快找到要查找的条目。\n根据键的范围分区 分区的一种方法是给每个分区分配一个连续的键范围（从最小值到最大值），就像纸质百科全书一样。如果知道范围之间的边界，就可以很容易地确定哪个分区包含给定的键。如果您还知道哪个分区分配给哪个节点，那么您可以直接向适当的节点提出请求（或者，对于百科全书，从书架上挑选正确的书）。\n键的范围不一定均匀分布，因为数据可能不均匀分布。例如，在图6-2中，卷1包含以A和B开头的单词，而第12卷包含以T、U、V、X、Y和Z开头的单词。如果字母表中每两个字母有一个卷，则某些卷会比其他的大得多。为了使数据均匀分布，分区边界需要与数据相适应。\n分区边界可以由管理员手动选择，也可以由数据库自动选择（我们将在第209页的“重新平衡分区”中更详细地讨论分区边界的选择）。Bigtable、它的开源等价HBase[2，3]、reinstdb和2.4[4]之前的MongoDB都使用这种分区策略。\n在每个分区中，我们可以按排序顺序保存键（参见第76页的“SSTables和LSMTrees”）。这样做的优点是范围扫描很容易，您可以将键视为一个连接索引，以便在一个查询中获取多个相关记录（请参阅第87页的“多列索引”）。例如，考虑一个存储来自传感器网络的数据的应用程序，其中的键是测量的时间戳（年-月-日-时-分-秒）。在这种情况下，范围扫描非常有用，因为它们可以让你很容易地获取某个月的所有读数。\n但是，键范围划分的缺点是某些访问模式可能导致热点。如果键是时间戳，则分区对应于时间范围，例如，每天一个分区。不幸的是，由于我们在测量时将数据从传感器写入数据库，所以所有的写入操作最终都会转到同一个分区（今天的分区），这样分区就可以在其他分区空闲的情况下进行写操作[5]。\n为了避免传感器数据库中的这个问题，您需要使用时间戳以外的东西作为 Key 的第一个元素。例如，您可以在每个时间戳前面加上传感器名称，以便首先按传感器名称，然后按时间进行分区。假设有多个传感器同时处于活动状态，那么写入负载最终将更加均匀地分布在各个分区上。现在，当您想要在一个时间范围内获取多个传感器的值时，需要对每个传感器名称执行单独的范围查询。\n根据键的 Hash 进行分区 由于存在倾斜和热点的风险，许多分布式数据存储使用哈希函数来确定给定 Key 的分区。\n一个好的散列函数接受倾斜的数据并使其均匀分布。假设您有一个32位哈希函数，它接受一个字符串。无论何时给它一个新字符串，它都会返回一个介于0和232-1之间的随机数。即使输入字符串非常相似，它们的哈希值也均匀地分布在这个数字范围内。\n出于分区的目的，散列函数不需要加密性强：例如，Cassandra 和 MongoDB 使用 MD5，而 Voldemort 使用 Fowler–Noll–Vo 函数。许多编程语言都内置了简单的哈希函数（因为它们用于哈希表），但是它们可能不适合分区：例如，在 Java 中 Object.hashCode（）和 Ruby 的 Object# 哈希，同一个键在不同的进程中可能有不同的哈希值[6]。\n一旦你有了一个合适的 Key 哈希函数，你就可以为每个分区分配一个哈希范围（而不是一个Key范围），哈希在分区范围内的每个 Key 都将存储在该分区中。如图6-3所示。\n这种技术很擅长在分区之间公平地分配 Key。分区边界可以均匀分布，也可以伪随机选择（在这种情况下，这种技术有时被称为一致哈希）。\n一致性哈希，由Karger等人定义。[7] ，是一种在internet范围的缓存系统（如内容交付网络（CDN））上均匀分配负载的方法。它使用随机选择的分区边界来避免中央控制或分布式共识的需要。注意，这里的一致性与副本一致性（见第5章）或ACID一致性（见第7章）无关，而是描述了一种重新平衡的特殊方法。\n正如我们将在209页的“重新平衡分区”中看到的，这种特殊的方法实际上对数据库[8]不是很有效，因此在实践中很少使用（一些数据库的文档仍然引用一致的哈希，但它通常是不准确的）。因为这太令人费解了，所以最好避免使用术语一致性散列，而将其称为散列分区。\n然而不幸的是，通过使用键的散列进行分区，我们失去了键范围分区的一个很好的特性：进行有效范围查询的能力。曾经相邻的键现在分散在所有分区中，因此它们的排序顺序将丢失。在 MongoDB 中，如果启用了基于散列的分片模式，那么任何范围查询都必须发送到所有分区[4]。Riak[9]、Couchbase[10]或Voldemort不支持对主键的范围查询。\nCassandra在两种分区策略之间达成了妥协[11,12,13]。Cassandra中的表可以用由多个列组成的复合主键声明。只有该键的第一部分被散列以确定分区，但其他列用作连接索引，用于对Cassandra的SSTables中的数据进行排序。因此，查询无法在复合键的第一列中搜索值的范围，但如果它为第一列指定了固定值，则可以对键的其他列执行有效的范围扫描。\n串联索引方法支持一对多关系的优雅数据模型。例如，在一个社交媒体网站上，一个用户可以发布许多更新。如果更新的主键选择为（user_id，update_timestamp），则可以有效地检索特定用户在某个时间间隔内所做的所有更新，并按时间戳排序。不同的用户可以存储在不同的分区上，但是在每个用户中，更新是按时间戳顺序存储在单个分区上的。\n工作负荷倾斜，缓解热点 如前所述，散列键以确定其分区可以帮助减少热斑点 (hot spots)。但是，它不能完全避免它们：在极端情况下，所有的读和写都是针对同一个键的，但最终还是会将所有请求路由到同一个分区。\n这种工作量可能不寻常，但并非闻所未闻：例如，在一个社交媒体网站上，一个拥有数百万粉丝的名人用户在做某件事时可能会引发一场活动风暴[14]。此事件可能导致对同一个键的大量写入（其中该键可能是名人的用户ID，或者是人们评论的动作的ID）。散列键没有帮助，因为两个相同的 id 的散列仍然是相同的。\n今天，大多数数据系统都无法自动补偿这种高度倾斜的工作负载，因此应用程序有责任减少这种偏差。例如，如果知道一个键非常热，一个简单的技术就是在键的开头或结尾添加一个随机数。只要一个两位数的十进制随机数，就可以在100个不同的键上平均分配对该键的写入，从而允许这些键分布到不同的分区。\n但是，在将写操作拆分到不同的键之后，任何读取操作都必须执行额外的工作，因为它们必须从所有100个键中读取数据并将其合并。这种技术还需要额外的簿记：只有为少数热键追加随机数才有意义；对于大多数写吞吐量较低的键，这将是不必要的开销。因此，您还需要某种方法来跟踪哪些键被拆分。\n也许在将来，数据系统将能够自动检测并补偿不均衡的工作负载；但是现在，您需要考虑一下您自己的应用程序的权衡。\n分区和二级索引 到目前为止，我们讨论的分区方案依赖于键值数据模型。如果只通过主键访问记录，我们可以从该键确定分区，并使用它将读写请求路由到负责该键的分区。\n如果涉及二级索引，情况会变得更加复杂（另请参阅第85页的“其他索引结构”）。二级索引通常不会唯一地标识记录，而是一种搜索特定值出现的方法：查找用户123的所有操作、查找包含hogwash一词的所有文章、查找所有颜色为红色的汽车等。\n二级索引是关系数据库的主要组成部分，在文档数据库中也很常见。许多键值存储（如HBase和Voldemort）都避免使用二级索引，因为它们增加了实现的复杂性，但是有些（如Riak）已经开始添加它们，因为它们对于数据建模非常有用。最后，二级索引是Solr和Elasticsearch等搜索服务器存在的理由。\n二级索引的问题是它们不能整齐地映射到分区。使用二级索引对数据库进行分区有两种主要方法：基于文档的分区和基于术语的分区。\n基于文档的二级索引分区 例如，假设您正在运营一个销售二手车的网站（如图6-4所示）。每个列表都有一个惟一的ID，称之为documentid，您可以根据文档ID对数据库进行分区（例如，分区0中的id0到499，分区1中的ids500到999，等等）。\n您希望让用户搜索汽车，允许他们按颜色和make进行筛选，所以您需要一个关于color和make的二级索引（在文档数据库中，这些是字段；在关系数据库中，它们是列）。如果已声明索引，则数据库可以执行索引自动。例如，每当一辆红色汽车添加到数据库中时，数据库分区会自动将其添加到索引项的文档ID列表中 color:red。\n在这种索引方法中，每个分区都是完全独立的：每个分区维护自己的辅助索引，只覆盖该分区中的文档。它不关心存储在其他分区中的数据。每当您需要写入数据库以添加、删除或更新文档时，您只需要处理包含您正在编写的文档ID的分区。因此，文档分区索引也称为本地索引（与下一节中描述的全局索引相反）。\n但是，从文档分区索引中读取需要小心：除非您对文档id做了一些特殊处理，否则没有理由将具有特定颜色或特定品牌的所有汽车放在同一个分区中。在图6-4中，0区和1区都出现了红色的汽车。因此，如果您想搜索redcars，您需要将查询发送到所有分区，并合并返回的所有结果。\n这种查询分区数据库的方法有时被称为分散/聚集（scatter/gather），它会使对辅助索引的读取查询变得非常昂贵。即使您并行地查询分区，分散/聚集也容易导致尾部延迟放大（参见第16页的“实践中的百分位”）。然而，它被广泛使用：MongoDB、Riak[15]、Cassandra[16]、Elasticsearch[17]、SolrCloud[18]和VoltDB[19]都使用文档分区的二级索引。大多数数据库供应商建议您构建分区方案，以便从单个分区为二级索引查询提供服务，但这并不总是可行的，尤其是当您在一个查询中使用多个二级索引时（例如同时按颜色和make筛选汽车）。\n基于术语的二级索引分区 我们可以构造一个覆盖所有分区中数据的全局索引，而不是每个分区都有自己的辅助索引（本地索引）。但是，我们不能只在一个节点上存储该索引，因为它很可能会成为瓶颈，并破坏分区的目的。全局索引也必须进行分区，但可以与主键索引进行不同的分区。\n图6-5说明了这可能是什么样子：所有分区的红色汽车都显示在下面颜色：红色索引，但是索引是分区的，这样以字母a到r开头的颜色出现在分区0中，以s到z开头的颜色出现在分区1中。对汽车品牌的索引进行了类似的划分（分区边界在f和h之间）。\n我们称这种索引项是分区的，因为我们要查找的项决定了索引的分区。在这里，一个术语是 color:red 例如。名称 term来自全文索引（一种特殊的二级索引），其中的术语是文档中出现的所有单词。\n如前所述，我们可以通过术语本身或者使用术语的散列来划分索引。按术语本身进行分区对于范围扫描（例如，在数字属性上，如汽车的要价）是有用的，而对术语的散列进行分区可以提供更均匀的负载分布。\n与文档分区索引相比，全局（术语分区）索引的优点是它可以使读取更高效：客户机只需向包含所需术语的分区发出请求，而不必对所有分区执行分散/聚集操作。但是，全局索引的缺点是写入速度较慢且更复杂，因为对单个文档的写入现在可能会影响索引的多个分区（文档中的每个术语可能位于不同的分区、不同的节点上）。\n在理想情况下，索引总是最新的，写入数据库的每个文档都会立即反映在索引中。但是，在term分区索引中，这将需要在受写操作影响的所有分区上进行分布式事务处理，而这并不是所有数据库都支持的（参见第7章和第9章）。\n实际上，对全局二级索引的更新通常是异步的（也就是说，如果您在写入后不久就读取了索引，那么您刚才所做的更改可能还没有反映在索引中）。例如，Amazon DynamoDB声明，在正常情况下，它的全局二级索引会在几分之一秒内更新，但在基础设施出现故障的情况下，可能会经历更长的传播延迟[20]。\n全局术语分区索引的其他用途包括Riak的搜索功能[21]和Oracle数据仓库，它允许您在本地索引和全局索引之间进行选择[22]。我们将在第12章回到实现术语分区二级索引的主题。\n分区重平衡 随着时间的推移，数据库中的情况会发生变化：\n 查询吞吐量增加，因此需要添加更多CPU来处理负载。 数据集大小增加，因此您需要添加更多磁盘和RAM来存储它。 一台机器发生故障，其他机器需要接管故障机器的责任。  所有这些更改都需要将数据和请求从一个节点移动到另一个节点。将负载从群集中的一个节点移动到另一个节点的过程称为重新平衡。\n无论使用哪种分区方案，重新平衡通常需要满足一些最低要求：\n 重新平衡后，负载（数据存储、读写请求）应该在集群中的节点之间公平地共享。 在进行重新平衡时，数据库应继续接受读写操作。 不应在节点之间移动超过需要的数据，以便快速重新平衡，并将网络和磁盘I/O负载降至最低。  重平衡的策略 （1）如何不这样做：hash mod N\n当按一个键的散列进行分区时，我们前面说过（图6-3），最好将可能的散列划分成范围，并将每个范围分配给一个分区（例如，如果 0≤hash（key）\u0026lt;b0，则将 key 分配给分区0；如果 b0≤hash（key）\u0026lt;b1，则将key分配给分区1，等等）。\n也许你想知道为什么我们不只用mod（许多编程语言中的%运算符）。例如，hash（key）mod 10将返回一个介于0和9之间的数字（如果我们将哈希写为十进制数，hash mod 10将是最后一个数字）。如果我们有10个节点，编号为0到9，这似乎是一种将每个键分配给一个节点的简单方法。\nmod N 方法的问题是，如果节点 N 的数量发生变化，则大多数键将需要从一个节点移动到另一个节点。例如，假设hash（key）= 123456。如果最初有10个节点，则该键从节点6开始（因为123456 mod 10=6）。当您增长到11个节点时，Key需要移动到节点3（123456 mod 11=3），当您增长到12个节点时，它需要移动到节点0（123456 mod 12=0）。如此频繁的变动使得再平衡成本过高。\n我们需要一种方法，它不需要移动数据。\n（2）固定分区数\n幸运的是，有一个相当简单的解决方案：创建比节点多得多的分区，并为每个节点分配多个分区。例如，在一个由10个节点组成的集群上运行的数据库可以从一开始就分成1000个分区，这样每个节点就可以分配大约100个分区。\n现在，如果将一个节点添加到集群中，新节点可以从每个现有节点中窃取一些分区，直到分区再次公平分布为止。该过程如图6-6所示。如果从集群中删除了一个节点，则会发生相反的情况。\n只有整个分区在节点之间移动。分区的数目不会改变，对分区的键的分配也不会改变。唯一改变的是将分区分配给节点。这种分配的改变不是立即的\u0026ndash;它需要一些时间来通过网络传输大量的数据，因此旧的分区分配用于在传输过程中发生的任何读写操作。\n原则上，您甚至可以考虑集群中不匹配的硬件：通过为功能更强大的节点分配更多分区，可以强制这些节点承担更大的负载份额。\nRiak[15]、Elasticsearch[24]、Couchbase[10]和Voldemort [25]都使用了这种重新平衡的方法。\n在这种配置中，分区的数量通常在数据库第一次设置时是固定的，之后不会更改。虽然原则上可以分割和合并分区（见下一节），但是固定数量的分区在操作上更简单，因此许多固定分区数据库选择不实现分区分割。因此，在开始时配置的分区数量是您可以拥有的最大节点数，因此您需要选择足够高的分区以适应将来的增长。然而，每个分区也有管理开销，所以选择过高的数字会适得其反。\n如果数据集的总大小是高度可变的（例如，如果它开始时很小，但随着时间的推移可能会增长得更大），那么选择正确的分区数就很困难了。由于每个分区包含总数据的固定部分，因此每个分区的大小与集群中的数据总量成比例增长。如果分区非常大，重新平衡和从节点故障中恢复将变得非常昂贵。但是如果分区太小，则会产生太多的开销。当分区的大小“恰到好处”，既不太大也不太小时，就可以获得最佳性能；如果分区的数量固定，但数据集大小不同，则很难实现这一点。\n（3）动态分区\n对于使用键范围分区的数据库（请参阅第202页的“按键范围划分”），使用固定边界的固定数量的分区将非常不方便：如果边界设置错误，可能会导致一个分区中的所有数据都为空，而所有其他分区都为空。手动重新配置分区边界将非常繁琐。\n因此，键范围分区数据库（如 HBase 和 rejectdb ）会动态地创建分区。当一个分区增长到超过配置的大小（在 HBase 上，默认值为10gb），它将被拆分为两个分区，这样大约一半的数据最终会在分割的每一侧结束[26]。相反，如果删除了大量数据，并且一个分区收缩到某个阈值以下，则可以将其与相邻分区合并。这个过程类似于B树的顶层（见第79页的“BTrees”）。\n每个分区分配给一个节点，每个节点可以处理多个分区，就像在固定数量的分区情况下一样。在一个大的分区被分割后，它的两个部分中的一个可以被转移到另一个节点，以平衡负载。对于HBase，分区文件的传输是通过HDFS进行的，HDFS是底层的分布式文件系统[3]。\n动态分区的一个优点是分区的数量与总数据量相适应。如果只有少量的数据，那么少量的分区就足够了，因此开销也很小；如果数据量很大，则每个单独分区的大小限制为可配置的最大值[23]。\n但是，需要注意的是，空数据库从一个分区开始，因为没有关于在哪里绘制分区边界的先验信息。当数据集很小，直到它到达第一个分区被拆分的点时，所有的写操作都必须由一个节点处理，而其他节点则处于空闲状态。为了缓解这个问题，HBase 和 MongoDB 允许在空数据库上配置一组初始分区（这称为预拆分）。在Key范围划分的情况下，预分割要求您已经知道 Key 分布是什么样子的[4，26]。\n动态分区不仅适用于键范围分区的数据，而且同样可以用于散列分区数据。MongoDB从2.4版开始支持key-range和hash分区，在任何一种情况下，它都会动态地分割分区。\n（4）按比例划分节点\n对于动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在某个固定的最小值和最大值之间。另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，分区数与节点数无关。\nCassandra 和 Ketama 使用的第三个选项是使分区的数量与节点的数量成比例，换句话说，每个节点有固定数量的分区[23,27,28]。在这种情况下，每个分区的大小与数据集的大小成比例增长，而节点数保持不变，但是当增加节点数时，分区又会变小。由于更大的数据量通常需要更多的节点来存储，因此这种方法还可以使每个分区的大小保持相当稳定。\n当一个新节点加入集群时，它会随机选择一个固定数量的现有分区进行拆分，然后在保留每个分区的另一半的同时，获得每个分区的一半的所有权。随机化可以产生不公平的分割，但是当在较大数量的分区（在Cassandra中，默认情况下每个节点256个分区）上取平均值时，新节点最终会从现有节点获得公平的负载份额。Cassandra3.0引入了一种替代的再平衡算法，避免了不公平的分割[29]。\n随机选择分区边界需要使用基于哈希的分区（因此可以从哈希函数产生的数字范围中选择边界）。实际上，这种方法最接近于一致哈希的原始定义[7]（参见第204页的“一致哈希”）。较新的哈希函数可以用较低的元数据开销实现类似的效果[8]。\n操作：自动或手动重新平衡 关于再平衡，有一个重要的问题我们一直在回避：重新平衡是自动进行还是手动进行？\n在完全自动的重新平衡（系统自动决定何时将分区从一个节点移动到另一个节点，而不需要任何管理员交互）和完全手动（将分区分配给节点是由管理员显式配置的，只有在管理员显式地重新配置分区时才会更改）。例如，Couchbase、Riak和Voldemort会自动生成一个建议的分区分配，但是需要管理员在它生效之前提交它。\n全自动再平衡可以很方便，因为正常维护的操作工作较少。然而，它可能是不可预测的。重新平衡是一项代价高昂的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个节点。如果不小心进行，此过程可能会使网络或节点过载，并在重新平衡过程中损害其他请求的性能。\n这种自动化与自动故障检测结合起来可能是危险的。例如，假设一个节点过载，对请求的响应暂时较慢。其他节点得出结论，重载的节点是死的，并自动重新平衡集群以转移负载。这会给过载的节点、其他节点和网络带来额外的负载，使情况变得更糟，并可能导致级联故障。\n出于这个原因，有人参与再平衡可能是件好事。它比全自动过程慢，但它有助于防止操作上的意外。\n请求路由 我们现在已经在多台机器上运行的多个节点上对数据集进行了分区。但是仍然存在一个悬而未决的问题：当客户机想要发出请求时，它如何知道要连接到哪个节点？当分区重新平衡时，分配给节点的分区也会发生变化。有人需要掌握这些变化，才能回答这个问题：如果我想读或写键“foo”，我需要连接到哪个IP地址和端口号？\n这是一个称为服务发现（service discovery）的更普遍问题的一个实例，它不仅限于数据库。任何可以通过网络访问的软件都有这个问题，特别是如果它的目标是高可用性（在多台机器上以冗余配置运行）。许多公司都编写了自己的内部服务发现工具，其中许多已经作为开源发布[30]。\n在高层次上，有几种不同的方法来解决这个问题（如图6-7所示）：\n 允许客户端联系任何节点（例如，通过循环负载平衡器）。如果该节点碰巧拥有应用请求的分区，那么它可以直接处理请求；否则，它将请求转发到适当的节点，接收应答，并将应答传递给客户机。 首先将来自客户端的所有请求发送到路由层，路由层确定应该处理每个请求的节点并相应地转发它。此路由层本身不处理任何请求；它只充当分区感知负载平衡器。 要求客户机知道分区和将分区分配给节点。在这种情况下，客户机可以直接连接到适当的节点，而不需要任何中介。  在所有情况下，关键问题是：做出路由决策的组件（可能是节点之一、路由层或客户端）如何了解将分区分配给节点的更改？\n这是一个具有挑战性的问题，因为重要的是所有参与者都同意，否则请求将被发送到错误的节点，并且不能得到正确的处理。在分布式系统中有一些协议可以达成共识，但是很难正确实现（见第9章）。\n许多分布式数据系统依赖于一个单独的协调服务（如ZooKeeper）来跟踪集群元数据，如图6-8所示。每个节点在ZooKeeper中注册自己，ZooKeeper维护分区到节点的权威映射。其他参与者，如路由层或支持分区的客户端，可以在ZooKeeper中订阅此信息。每当分区更改所有权，或添加或删除节点时，ZooKeeper会通知路由层，以便它可以保持其路由信息的最新状态。\n例如，LinkedIn的Espresso使用Helix[31]进行集群管理（反过来依赖ZooKeeper），实现了如图6-8所示的路由层。HBase、SolrCloud和Kafka还使用ZooKeeper跟踪分区分配。MongoDB有一个类似的架构，但它依赖于自己的配置服务器实现和mongos守护进程作为路由层。\nCassandra和Riak采取了不同的方法：他们在节点之间使用 gossip 协议来传播集群状态的任何变化。请求可以发送到任何节点，该节点将请求转发给请求分区的相应节点（图6-7中的方法1）。这种模型增加了数据库节点的复杂性，但避免了对外部协调服务（如ZooKeeper）的依赖。\nCouchbase不会自动重新平衡，这简化了设计。通常它配置有一个名为moxi的路由层，它从集群节点了解路由变化[32]。\n当使用路由层或向随机节点发送请求时，客户端仍然需要找到要连接的IP地址。它们的变化不如将分区分配给节点那样快，因此通常使用DNS就足够了。\n并行查询执行 到目前为止，我们关注的是读写单个键的非常简单的查询（在文档分区的二级索引的情况下，加上分散/聚集查询）。这与大多数NoSQL分布式数据存储支持的访问级别有关。\n然而，通常用于分析的大规模并行处理（MPP）关系数据库产品在它们支持的查询类型方面要复杂得多。典型的数据仓库查询包含几个连接、筛选、分组和聚合操作。MPP查询优化器将这个复杂的查询划分为许多执行阶段和分区，其中许多可以在数据库集群的不同节点上并行执行。涉及对数据集的大部分进行扫描的查询特别受益于这种并行执行。\n数据仓库查询的快速并行执行是一个专门的课题，鉴于分析的业务重要性，它受到了很多商业兴趣。我们将在第10章讨论一些并行查询执行的技术。有关并行数据库中使用的技术的更详细的概述，请参阅参考文献[1,33]。\n总结 分区的目标是将数据和查询负载均匀地分布在多台机器上，避免热点（负载过高的节点）。这需要选择适合您的数据的分区方案，并在向集群添加或从集群中删除节点时重新平衡分区。\n我们讨论了两种主要的分区方法：\n Key 范围分区，其中Key被排序，分区拥有从最小值到最大值的所有Key。排序的优点是可以进行有效的范围查询，但是如果应用程序经常访问排序顺序相近的键，则存在热点的风险。在这种方法中，当一个分区太大时，分区通常通过将范围分成两个子范围来动态地重新平衡。 哈希分区，其中每个键都应用一个哈希函数，一个分区拥有一系列哈希值。此方法破坏了键的顺序，使范围查询效率低下，但可以更均匀地分配负载。当使用哈希进行分区时，通常会预先创建固定数量的分区，为每个节点分配多个分区，并在添加或删除节点时将整个分区从一个节点移动到另一个节点。也可以使用动态分区。  混合方法也是可能的，例如使用复合键：使用键的一部分来标识分区，另一部分用于排序顺序。\n我们还讨论了分区和二级索引之间的交互作用。二级索引也需要分区，有两种方法：\n 文档分区索引（本地索引），其中辅助索引与主键和值存储在同一个分区中。这意味着只有一个分区需要在写入时更新，但是读取辅助索引需要分散/聚集所有分区。 术语分区索引（全局索引），其中使用索引值分别对二级索引进行分区。辅助索引中的一个条目可以包括来自主键所有分区的记录。在编写文档时，需要更新辅助索引的几个分区；但是，可以从单个分区执行读取。  "});index.add({'id':217,'href':'/docs/programmer-interview/java/arraylist/','title':"ArrayList",'content':"ArrayList  基于 JDK 1.8 实现分析的\n 初始容量 /** * Default initial capacity. */ private static final int DEFAULT_CAPACITY = 10; /** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * Constructs an empty list with an initial capacity of ten. */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } new ArrayList() 时拥有的是一个空数组，在第一次 add 的时候，才会创建一个长度为 10 的数组。\n如何扩容 private void grow(int minCapacity) { // overflow-conscious code  int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); }  扩容后新的数组长度为 oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1)，也就是旧容量的 1.5 倍 然后使用 Arrays.copyOf 静态方法将旧数组里面的内容全部拷贝到新的数组里面  放满了会发生什么 "});index.add({'id':218,'href':'/docs/programmer-interview/front-end/flex/','title':"flex 布局",'content':"flex 布局 flex 容器 开启 flex 容器 .myClass { /** inline-flex 也可以开启 */ display: flex; } flex-direction flex-direction 定义了 main axis，其决定容器内的元素是横向布局还是纵向布局，其可选值如下：\n row (默认) row-reverse column column-reverse  flex-wrap flex-wrap 指定容器内的元素如何换行，默认情况下是挤在一行。其可选值如下：\n nowrap (默认) wrap：from top to bottom wrap-reverse：from bottom to top  flex-flow flex-flow 是 flex-direction 和 flex-wrap 这两个属性的缩写，能够同时定义这两个属性。\n.myClass { flex-flow: flex-direction flex-wrap; }  flex-flow 的默认值：row nowrap\n justify-content 在 main axis 轴上如何利用剩余空间：\nalign-items 在 cross axis 轴上如何利用空间：\nalign-content 在 main axis 轴如果有多行，应该如何布局：\nflex 元素(项) order 控制元素出现的顺序：\nflex-grow 定义元素自己自我 grow(伸张扩展) 的能力：\nflex-shrink 定义元素自我伸缩的能力：\nflex-basis 定义元素默认的初始的 size，接受如下几个值：\n 长度：20% 或 5rem 这种 auto: 取决于元素本身的 width 和 height content：取决于元素的内容  flex flex 是 flex-grow、flex-shrink、flex-basis 这三个属性的简略写法。flex 的默认值：0 1 auto。\n(1) flex 1\nflex: 1 与 flex: 1 1 0% 含义相同。它可以均分元素。\nalign-self 可以覆盖 flex 容器的 align-items 的默认布局，调整单个元素的对齐方式：\n"});index.add({'id':219,'href':'/docs/books/ddia/ddia-chapter7/','title':"设计数据密集型应用程序 - 事务",'content':"设计数据密集型应用程序 - 事务  一些作者声称，一般的两阶段提交由于其带来的性能或可用性问题，支持起来过于昂贵。我们认为，最好让应用程序程序员在瓶颈出现时处理由于过度使用事务而导致的性能问题，而不是总是围绕缺少事务进行编码。 \u0026mdash; James Corbett et al., Spanner: Google’s Globally-Distributed Database (2012)\n 在数据系统的严酷现实中，很多事情都会出错：\n 数据库软件或硬件可能随时发生故障（包括在写入操作的中间）。 应用程序可能随时崩溃（包括一系列操作的中途）。 网络中断可能会意外地切断应用程序与数据库的连接，或断开一个数据库节点与另一个数据库节点的连接。 多个客户端可能同时写入数据库，覆盖彼此的更改。 客户端可能会读取没有意义的数据，因为它只更新了一部分。 客户端之间的竞争条件可能会导致令人惊讶的错误。  为了可靠，系统必须处理这些故障，并确保它们不会导致整个系统的灾难性故障。然而，实现容错机制需要大量的工作。它需要仔细考虑所有可能出错的地方，并进行大量测试以确保解决方案能够实际工作。\n几十年来，事务一直是简化这些问题的首选机制。事务是应用程序将多个读写操作组合到一个逻辑单元中的一种方法。从概念上讲，事务中的所有读写操作都作为一个操作执行：要么整个事务成功（commit），要么失败（abort，rollback）。如果失败，应用程序可以安全地重试。有了事务，应用程序的错误处理就变得简单多了，因为它不需要担心部分失败，也就是说，有些操作成功，有些操作失败（无论什么原因）。\n如果你花了数年时间处理事务，它们可能看起来很明显，但我们不应该认为它们是理所当然的。事务不是自然规律；创建事务的目的是为了简化访问数据库的应用程序的编程模型。通过使用事务，应用程序可以自由地忽略某些潜在的错误场景和并发问题，因为数据库会处理它们（我们称之为安全保证）。\n并不是每个应用程序都需要事务，有时削弱事务性保证或完全放弃事务性保证有好处（例如，为了获得更高的性能或更高的可用性）。一些安全属性可以在没有事务的情况下实现。\n你如何判断你是否需要事务？为了回答这个问题，我们首先需要确切地了解事务可以提供什么样的安全保障，以及与之相关的成本。虽然乍一看事务似乎很简单，但实际上有许多微妙但重要的细节在起作用。\n在本章中，我们将研究许多可能出错的例子，并探索数据库用来防范这些问题的算法。我们将特别深入到并发控制领域，讨论可能发生的各种竞争条件，以及数据库如何实现隔离级别，如读提交、快照隔离和串行化。\n本章适用于单节点和分布式数据库；在第8章中，我们将重点讨论仅在分布式系统中出现的特定挑战。\n事务的模糊概念 现在几乎所有的关系型数据库和一些非关系型数据库都支持事务。其中大多数都遵循ibmsystemr在1975年引入的样式，第一个SQL数据库[1，2，3]。虽然一些实现细节有所改变，但40年来，总体思路基本不变：MySQL、PostgreSQL、Oracle、sqlserver等的事务支持与systemr惊人地相似。\n在21世纪末，非关系（NoSQL）数据库开始流行起来。他们希望通过提供新的数据模型（见第2章）和默认的复制（第5章）和分区（第6章）来改善关系现状。事务是这场运动的主要牺牲品：许多新一代数据库完全放弃了事务，或者重新定义了这个词来描述一组比以前理解的要弱得多的保证[4]。\n随着这种新的分布式数据库的大肆宣传，人们普遍认为事务是可伸缩性的对立面，任何大型系统都必须放弃事务，以保持良好的性能和高可用性[5,6]。另一方面，事务性保证有时由数据库供应商提出，作为“严肃的应用程序”和“有价值的数据”的基本要求。\n事实并非如此简单：与其他技术设计选择一样，事务有其优势和局限性。为了理解这些权衡，让我们详细介绍事务在正常操作和各种极端（但现实）情况下可以提供的保证。\nACID 的意义 事务提供的安全保证通常用众所周知的缩写ACID来描述，它代表原子性、一致性、隔离性和持久性。它是1983年由Theo Härder和Andreas Reuter[7]创造的，目的是为了在数据库。\n但是在实践中，一个数据库的ACID实现并不等同于另一个数据库的实现。例如，正如我们将要看到的，孤立的含义有很多模棱两可的地方。高层次的想法是正确的，但魔鬼在于细节。今天，当一个系统声称“符合ACID”时，你还不清楚到底能得到什么样的保证。不幸的是，ACID已经成为一个市场术语。\n（不符合ACID标准的系统有时称为BASE，它代表基本可用、软状态和最终一致性[9]。这甚至比ACID的定义更模糊。似乎对base唯一合理的定义是“not ACID”；也就是说，它几乎可以表示任何你想要的东西。）\n让我们深入研究原子性、一致性、隔离性和持久性的定义，因为这将使我们完善我们对事务的概念。\n（1）原子性\n一般来说，原子是指不能分解成更小的东西零件。零件在计算机的不同分支中，单词的意思是相似但又微妙不同的东西。例如，在多线程编程中，如果一个线程执行原子操作，这意味着另一个线程不可能看到该操作的一半finishedresult。系统只能处于操作前或操作后的状态，而不是介于两者之间的状态。\n相比之下，在ACID环境中，原子性与并发性无关。它没有描述如果多个进程试图同时访问同一个数据会发生什么，因为这是在字母I下的隔离（参见第225页的“隔离”）。\n相反，ACID原子性描述的是，如果客户端想要进行多个写操作，但在处理了一些写入操作之后发生了错误，例如，进程崩溃、网络连接中断、磁盘已满或违反了某些完整性约束。如果写入操作被组合到一个atomic transaction中，并且由于错误而无法完成（提交）事务，则事务将中止，数据库必须放弃或撤消迄今为止在该事务中所做的任何写入操作。\n如果没有原子性，如果在进行多次更改的过程中发生了错误，则很难知道哪些更改已生效，哪些更改没有生效。应用程序可以重试，但这有可能使同一更改发生两次，从而导致重复或不正确的数据。原子性简化了这个问题：如果一个事务被中止，应用程序可以确保它没有改变任何东西，所以它可以安全地被中止重试过了。\nACID原子性的定义特性是能够在出错时中止事务并放弃该事务中的所有写操作。也许可终止性比原子性更好，但我们还是坚持原子性，因为这是一个常用的词。\n（2）一致性\n“一致性”这个词严重超载：\n 在第5章中，我们讨论了副本一致性以及异步复制系统中出现的最终一致性问题（请参阅第161页的“复制延迟问题”）。 一致哈希是一些系统用于重新平衡的分区方法（请参阅第204页的“一致哈希”）。 在CAP定理（见第9章）中，一致性一词用于表示线性化（见324页的“线性化能力”）。 在ACID的上下文中，一致性是指数据库处于“良好状态”的特定于应用程序的概念  不幸的是，同一个词至少有四种不同的意思。\nACID一致性的思想是关于数据的某些陈述（不变量）必须始终为真-例如，在一个会计系统中，所有账户的贷方和借方必须总是平衡的。如果事务以根据这些不变量有效的adatabase开始，并且事务期间的任何写入都保持有效性，那么您可以确保这些不变量始终是满意。\n不过，这种一致性的思想依赖于应用程序的不变量概念，并且由应用程序负责定义它的事务处理是正确的，以便保留一致性。这不是数据库可以保证的：如果你写的坏数据违反了你的不变量，数据库不能阻止你。（数据库可以检查某些特定类型的不变量，例如使用foreignkey约束或唯一性约束。但是，一般情况下，应用程序会定义哪些数据有效或无效，而数据库只存储这些数据。）\n原子性、隔离性和持久性是数据库的属性，而一致性（在 ACID 意义上）是应用程序的属性。为了实现一致性，应用程序可能会重新连接数据库的原子性和隔离属性，但这并不仅仅取决于数据库。因此，字母C并不真的属于ACID。\n（3）隔离\n大多数数据库同时由多个客户端访问。如果他们在读写数据库的不同部分，这是没有问题的，但是如果他们访问相同的数据库记录，你可能会遇到并发问题（竞争条件）。\n假设您有两个客户端同时递增存储在数据库中的计数器。每个客户端需要读取当前值，添加1，然后将新值写回（假设数据库中没有内置任何增量操作）。在图7-1中，计数器应该从42增加到44，因为发生了两个增量，但实际上由于竞争条件，它只增加到43。\nACID意义上的隔离意味着并发执行的事务是相互隔离的：它们不能相互干涉。经典的数据库教科书将隔离形式化为可序列化，这意味着每个事务都可以假装它是整个数据库上运行的唯一事务。数据库确保当事务提交后，结果与串行运行（一个接一个）相同，即使实际上它们可能同时运行[10]。\n然而，在实践中，可序列化隔离很少使用，因为它会带来性能损失。一些流行的数据库，比如oracle11g，甚至没有实现它。在Oracle有一个名为“serializable”的隔离级别，但实际上它实现了一种称为snapshot isolation的方法，这比serializability的保证要弱[8,11]。我们将在233页的“弱隔离级别”中探讨快照隔离和其他形式的隔离。\n（4）耐久性\n数据库系统的目的是提供一个安全的地方来存储数据，而不必担心丢失。持久性是指一旦事务成功提交，它所写的任何数据都不会被遗忘，即使存在硬件故障或数据库崩溃。\n在单节点数据库中，持久性通常意味着数据已被写入非易失性存储器，如硬盘驱动器或SSD。它通常还需要写一个headlog或类似的（参见第82页的“使B树可靠”），这样可以在磁盘上的数据结构损坏时进行恢复。在复制的数据库中，持久性可能意味着数据已成功复制到一定数量的节点。为了提供持久性保证，数据库必须等到这些写入或复制完成后才能成功地报告事务承诺。\n作为在第6页的“可靠性”中讨论过，完美的耐久性是不存在的：如果你的所有硬盘和备份同时被销毁，那么显然什么都没有你的数据库可以帮你保存。\n复制和耐用性\n历史上，耐用性意味着写入存档磁带。然后它被理解为写入磁盘或SSD。最近，它被改编成复制。哪个执行更好？\n事实是，没有什么是完美的：\n 如果您写入磁盘后机器死机，即使您的数据没有丢失，在您修复机器或将磁盘传输到另一台热机器之前，它仍然是不可访问的。复制的系统可以保持可用。 相关故障—停电或使特定输入上的每个节点崩溃的bug可以一次关闭所有副本（请参阅第6页的“可靠性”），从而丢失仅存在于内存中的任何数据。因此，写入磁盘仍然与内存数据库相关。•在异步复制系统中，当读卡器不可用时，最近的写入可能会丢失（请参阅第156页的“处理节点中断”）。 当电源突然中断时，特别是固态硬盘有时会违反它们应该提供的保证：即使是 fsync 也不能保证正常工作[12]。磁盘固件可能有错误，就像任何其他类型的软件一样[13，14]。 存储引擎和文件系统实现之间的细微交互可能导致难以跟踪的错误，并可能导致磁盘上的文件在崩溃后损坏[15,16]。 磁盘上的数据可能会逐渐损坏而不会被检测到[17]。如果数据已损坏一段时间，副本和最近的备份也可能会被破坏。在这种情况下，您需要尝试从历史备份中恢复数据。 对固态硬盘的一项研究发现，30%到80%的硬盘在运行的头四年内至少会出现一个坏块[18]。与固态硬盘相比，磁性硬盘的坏扇区率更低，但完全故障率更高。 如果SSD断开电源，它可能在几周内开始丢失数据，具体取决于温度[19]。  实际上，没有一种技术可以提供绝对的保证。只有各种降低风险的技术，包括写入磁盘、复制到远程计算机和备份，它们可以而且应该一起使用。同样，明智的做法是在理论上保证健康。\n单对象和多对象操作 概括地说，在ACID中，原子性和隔离性描述了如果客户端在同一事务中进行多次写入，数据库应该做什么：\n 原子性：如果在写入序列的中途发生错误，则应中止事务，并应放弃在此之前进行的写入。换句话说，数据库通过提供全有或全无的保证，使您不必担心部分故障。 隔离性：事务之间不应该相互干扰。例如，如果一个事务进行了多次写入，那么另一个事务应该看到这些写入的全部或全部，而不是某个子集。  这些定义假定您希望一次修改多个对象（行、文档、记录）。如果多个数据需要保持同步，则通常需要这样的多对象事务。图7-2显示了一个电子邮件示例应用程序。到显示用户的未读邮件数，您可以查询如下内容：\nSELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true 但是，如果有许多电子邮件，您可能会发现此查询太慢，并决定将未读邮件的数量存储在单独的字段中（一种非规范化）。现在，每当有新消息传入时，您也必须增加未读计数器，并且每当消息被标记为已读时，您还必须删除未读计数器。\n在图7-2中，用户2经历了一个异常：邮箱列表显示一个未读消息，但计数器显示零个未读消息，因为计数器尚未递增发生了。隔离可以通过确保用户2同时看到插入的电子邮件和更新的计数器，或者两者都看不到来防止这个问题，但没有不一致的中间点。\n图7-3说明了原子性的必要性：如果在事务处理过程中某个地方发生错误，邮箱和未读计数器的内容可能会不同步。在原子事务中，如果对计数器的更新失败，事务将中止，插入的电子邮件将回滚。\n多对象事务需要某种方法来确定哪些读操作和写操作属于同一事务。在关系数据库中，这通常是基于客户端到数据库服务器的TCP连接的一种方式：在任何特定的连接中，BEGIN事务和COMMIT语句之间的所有内容都被视为同一事务的一部分。\n另一方面，许多非关系数据库没有这种将操作分组在一起的方法。即使有一个多对象API（例如，一个key-valuestore可能有一个在一个操作中更新多个键的multi-put操作），也不一定意味着它具有事务语义：对于某些键，命令可能成功，而对于其他键，命令可能会失败，从而使数据库处于部分更新的状态州。\n（1）单对象写入\n当单个对象被更改时，原子性和隔离性也适用。例如，假设您正在向数据库写入一个20 KB的JSON文档：\n 如果在发送前10 KB后网络连接中断，数据库是否存储了无法解析的10 KB JSON片段？ 如果在数据库覆盖磁盘上以前的值的过程中电源出现故障，您是否最终将新旧值拼接在一起？ 如果另一个客户端在写入过程中读取该文档，它是否会看到部分更新的值？  这些问题会令人难以置信地令人困惑，因此存储引擎几乎普遍都会在一个节点上的单个对象（如keyvalue对）级别上提供原子性和隔离性。原子性可以使用崩溃恢复日志来实现（参见第82页的“使B树可靠”），并且可以通过对每个对象的锁来实现隔离（允许一个线程在任何时候访问一个对象）。\n有些数据库还提供了更复杂的原子操作，如增量操作，这就不需要像图7-1所示那样进行 read-modify-write 循环。同样流行的是 compare-and-set 操作，它只允许在值没有被其他人同时更改的情况下进行写入（参见第245页的“比较和设置”）。\n这些单对象操作很有用，因为当多个客户端试图同时写入同一个对象时，它们可以防止丢失更新（请参阅第242页的“防止丢失”）。然而，它们并不是通常意义上的事务。出于营销目的，Compare and set和其他单对象操作被称为“轻量级事务”甚至“ACID”[20,21,22]，但这个术语是误导性的。事务通常被理解为将多个对象上的多个操作组合成一个执行单元的机制。\n（2）多对象事务的需要\n许多分布式数据存储已经放弃了多对象事务，因为它们很难跨分区实现，而且在某些需要非常高可用性或高性能的场景中，它们可能会成为障碍。然而，在分布式数据库中并没有什么能从根本上阻止事务，我们将在第9章讨论分布式事务的实现。\n但我们需要多对象事务吗？是否有可能只使用键值数据模型和单对象操作来实现任何应用程序？在一些用例中，单对象插入、更新和删除就足够了。但是，在许多其他情况下，对多个不同对象的写入需要进行协调：\n 在关系数据模型中，一个表中的一行通常具有另一个表中某行的外键引用。（类似地，在类似于图的数据模型中，一个顶点与其他顶点之间存在边。）多对象事务允许您确保这些引用保持有效：当插入多个相互引用的记录时，foreign键必须是正确的和最新的，否则数据将变得毫无意义。 在文档数据模型中，需要一起更新的字段通常位于同一个文档中，该文档被视为单个对象，更新单个文档时不需要多对象事务处理。然而，缺少join功能的document databases也鼓励非规范化（参见第38页的“关系数据库与文档数据库的对比”）。非标准化时 信息需要更新，如图7-2所示，您需要一次性更新多个文档。在这种情况下，事务对于防止非规范化数据不同步非常有用。 在具有二级索引的数据库中（除了纯键值存储区以外的几乎所有内容），每次更改值时都需要更新索引。从事务的角度来看，这些索引是不同的数据库对象：例如，如果没有事务隔离，一个记录可能出现在一个索引中而不是另一个索引中，因为第二个索引的更新还没有发生。  这样应用程序仍然可以在没有事务的情况下实现。然而，如果没有原子性，错误处理会变得更加复杂，并且缺乏隔离可能会导致并发问题。我们将在233页讨论“弱隔离级别”中的问题，并在第12章中探讨替代方法。\n（3）处理错误和中止\n事务的一个关键特性是，如果发生错误，可以中止并安全地重试。ACID数据库基于这样的理念：如果数据库有可能违反其对原子性、隔离性或持久性的保证，那么它宁愿完全放弃事务，也不愿让事务保持半成品状态。\n不过，并非所有的系统都遵循这一理念。特别是，使用无 Leader 复制的数据存储（请参阅177页的“无领导复制”）在“尽力而为”的基础上工作得更多，这可以概括为“数据库将尽其所能，如果遇到错误，它不会撤消它已经做过的事情”，所以应用程序有责任从错误中恢复。\n错误不可避免地会发生，但是许多软件开发人员宁愿只考虑快乐的道路，而不是复杂的错误处理。例如，流行的对象关系映射（ORM）框架（如Rails的ActiveRecord和django）不会重试中止的事务，错误通常会导致异常冒出堆栈，因此任何用户输入都会被丢弃，用户会收到一条错误消息。这是一种耻辱，因为中止的全部目的是为了确保安全重试。不过重试中止的事务是一种简单有效的错误处理机制，但并不完美：\n 如果事务实际成功，但网络在服务器尝试确认成功提交给客户端时失败（因此客户端认为它失败），则重试事务会导致事务执行两次，除非您有附加的应用程序级重复数据消除机制。 如果错误是由于过载导致的，则重试事务会使问题变得更糟，而不是更好。为了避免这种反馈周期，可以限制重试次数，使用指数回退，并处理与重载相关的错误，这些错误与其他错误不同（如果可能）。 只有在短暂错误（例如由于死锁、隔离冲突、临时网络中断和故障转移）之后才值得重试；在永久性错误（例如，违反约束）之后，重试将毫无意义。 如果事务在数据库之外也有副作用，即使事务被中止，这些副作用也可能发生。例如，如果您要发送电子邮件，您不希望每次重试事务时都再次发送电子邮件。如果您想确保几个不同的系统要么提交要么一起提交，那么两阶段提交会有所帮助（我们将在第354页的“原子提交和两阶段提交（2PC）”中讨论这个问题。 如果客户端进程在重试时失败，它试图写入数据库的任何数据都将丢失。  弱隔离等级 如果两个事务不接触相同的数据，它们可以安全地并行运行，因为这两个事务都不依赖于另一个事务。只有当一个事务读取被另一个事务同时修改的数据，或者两个事务试图同时修改samedata时，并发问题（竞争条件）才会起作用。\n通过测试很难发现并发性错误，因为只有在时间不走运的情况下才会触发此类bug。这样的时间问题可能很少发生，而且通常很难重现。并发性也是很难推理的，尤其是在大型应用程序中，您不一定知道还有哪些代码正在访问数据库。如果一次只有一个用户，那么应用程序开发就很困难；拥有许多并发用户会使开发变得更加困难，因为任何数据块都可能在任何时候意外地发生更改时间。\n所以，数据库长期以来一直试图通过提供事务隔离来向应用程序开发人员隐藏并发问题。从理论上讲，隔离应该让你的生活更轻松，让你假装没有并发发生：serializable isolation意味着数据库保证事务具有与串行运行相同的效果（即，一次一个事务，没有任何并发）。\n在实践中，不幸的是，隔离并不是那么简单。可串行化隔离有一个性能代价，许多数据库不想为此付出代价[8]。因此，建议系统使用较弱级别的隔离，这样可以防止一些并发问题，但不是全部。这些级别的隔离更难理解，它们可能会导致细微的错误，但它们仍然在实践中使用[23]。\n弱事务隔离导致的并发错误不仅仅是一个理论问题。它们造成了巨大的资金损失[24,25]，导致了财务审计师的调查[26]，并导致客户数据被破坏[27]。对于这些问题的揭露，一个普遍的评论是“如果你在处理财务数据，就使用ACID数据库！“-但这没有抓住重点。甚至许多流行的关系数据库系统（通常被认为是“ACID”）都使用弱隔离，因此它们不一定能够阻止这些错误的发生。\n我们不必盲目依赖工具，而是需要对存在的各种并发问题以及如何预防它们有一个很好的理解。然后我们就可以使用我们可以使用的工具来构建可靠和正确的应用程序。我们对隔离级别的讨论将是非正式的，使用示例。如果你想对它们的性质进行有趣的定义和分析，你可以在学术文献[28,29,30]中找到它们。\n读已提交 最基本的事务隔离级别是read committed。它有两个保证：\n 从数据库读取时，您将只看到已提交的数据（没有脏读）。 写入数据库时，只覆盖已提交的数据（无脏写）。  让我们更详细地讨论这两个保证。\n（1）没有脏读\n假设一个事务已将一些数据写入数据库，但该事务尚未提交或中止。另一个事务可以看到未提交的数据吗？如果是的，那就叫做脏读[2]。\n在读提交隔离级别运行的事务必须防止脏读。这个意味着只有当事务提交时，事务的任何写入才会对其他事务可见（然后它的所有写操作都会立即可见）。如图7-4所示，其中用户1设置了 x=3，但用户2的 get x 仍然返回oldvalue 2，而用户1尚未提交。\n防止脏读的原因如下：\n 如果一个事务需要更新多个对象，脏读意味着另一个事务可能会看到某些更新，而其他事务则看不到其他更新。例如，在 图7-2，用户看到新的未读邮件，但没有更新的计数器。这是对这封邮件的一个肮脏的解读。看到数据库处于部分更新状态会让用户感到困惑，并可能导致其他事务做出错误的决定。 如果事务中止，则需要回滚它所做的任何写入操作（如图7-3所示）。如果数据库允许脏读，这意味着事务可能会生成稍后回滚的数据，也就是说，它从未真正提交到数据库。对后果的推理很快变得令人费解。  （2）没有脏写\n如果两个事务同时尝试更新adatabase中的同一对象，会发生什么情况？我们不知道写操作的顺序，但我们通常假设后面的写会覆盖前面的写。\n但是，如果前面的写入操作是未提交yetcommitted的事务的一部分，那么后面的写入操作会覆盖未提交的值，会发生什么情况？这叫做dirty write[28]。通常情况下，在第二个已提交的写入事务中，必须通过延迟第二个已提交的写操作来阻止第二个事务的写入操作。\n通过防止脏写，此隔离级别避免了某些类型的并发问题：\n 如果事务更新多个对象，脏写可能导致坏结果。例如，考虑图7-5，它展示了一个二手车销售网站，上面有两个人，爱丽丝和鲍勃，同时都在试图买同样的汽车。买一辆车需要两个数据库写入：网站上的列表需要更新以反映买家，销售发票需要发送到买方。输入在图7-5中，销售被授予Bob（因为他对listings表执行了获胜更新），但是发票被发送给Alice（因为她对invoices表执行了winning更新）。读取已提交的可以防止这种不幸。 但是，read committed不会阻止图7-1中的计数器增量之间的竞争条件。在本例中，第二次写入发生在第一个事务已提交之后，因此它不是脏写。这仍然是不正确的，在242页的“防止丢失更新”中，我们将讨论如何使这种计数器增量安全。  （3）实现读提交\nRead committed是一个非常流行的隔离级别。它是Oracle 11g、PostgreSQL、SQL Server 2012、MemSQL和许多其他数据库的默认设置[8]。\n最常见的情况是，数据库通过使用行级锁来防止脏写：当事务要修改特定对象（行或文档）时，必须首先获取该对象的锁。然后它必须保持该锁，直到事务被提交或中止。只有一个事务可以持有任何给定对象的锁；如果另一个事务要写入同一对象，则必须等到第一个事务被提交或中止后才能获取锁并继续。这种锁定是由数据库以read committed模式（或更强的隔离级别）自动完成的。\n我们如何防止脏读？一种选择是使用同一个锁，并要求任何想要读取对象的事务短暂地获取锁，然后在读取后立即释放它。这将确保在对象具有脏的未提交值时不会发生读取（因为在此期间，进行写入的事务将持有锁）。\n然而，要求读锁的方法在实践中并不能很好地工作，因为一个长时间运行的写事务会迫使许多只读事务等待长时间运行的事务完成。这会损害只读事务的响应时间，并且不利于可操作性：由于等待锁，应用程序某个部分的速度减慢可能会对应用程序的另一个完全不同的部分产生连锁反应。\n因此，大多数数据库都使用图7-4所示的方法来防止脏读：对于写入的每个对象，数据库都会记住旧的提交值和当前持有写锁的事务设置的新值。当事务正在进行时，读取该对象的任何其他事务都将被简单地赋予旧值。只有在提交新值时，事务才会切换到读取新值。\n快照隔离和可重复读取 如果您对read-committed隔离进行了肤浅的研究，那么您认为它做了事务需要做的所有事情：它允许中止（原子性所必需的），它防止读取事务的不完整结果，并防止并发写操作混合在一起。实际上，这些都是有用的特性，而且比没有事务处理的系统能提供的更强大的保证。\n但是，在使用这个隔离级别时，仍然有很多方法可以导致并发错误。例如，图7-6说明了在提交读取时可能出现的问题。\n假设爱丽丝在一家银行有1000美元的存款，分成两个账户，每个账户500美元。现在一笔事务将100美元从她的一个账户转到另一个账户。如果她不走运，在事务处理的同一时刻查看账户余额列表，她可能会在收到付款之前一次看到一个账户余额（余额为500美元），另一个账户在转出转账后（新余额为400美元）。对爱丽丝来说，她现在似乎只有900美元在她的帐户上，似乎100美元已经消失在空气中。\n这种异常被称为不可重复读取或读取倾斜：如果Alice在事务结束时再次读取account 1的余额，她将看到与她在上一次查询中看到的不同的值（$600）。在Read-committed隔离下，Read-skew被认为是可以接受的：Alice看到的帐户余额确实在她读取时被提交。\n在爱丽丝的例子中，这并不是一个持久的问题，因为如果她在几秒钟后重新加载网上银行网站，她很可能会看到稳定的账户余额。但是，有些情况不能容忍这种暂时的不一致：\n 备份  备份需要制作整个数据库的副本，在大型数据库上可能需要几个小时。在备份进程运行期间，将继续对数据库进行写入操作。因此，备份的某些部分可能包含旧版本的数据，而其他部分则包含较新版本的数据。如果您需要从这样的备份中恢复，则矛盾（比如消失的钱）成为永久性的。\n 分析查询和完整性检查  有时，您可能希望运行一个对数据库的大部分进行扫描的查询。这类查询在分析中很常见（参见“事务处理或分析”？或是定期完整性检查的一部分，以确保一切正常（监控数据损坏）。如果这些查询在不同的时间点观察数据库的某些部分，则可能返回无意义的结果。 快照隔离[28]是解决这个问题的最常见方法。其思想是，每个事务都从数据库的一致快照中读取，也就是说，事务操作会看到在事务开始时提交到数据库中的所有数据。即使数据随后被另一个事务更改，每个事务只看到特定时间点的旧数据。\n快照隔离对于长时间运行的只读查询（如备份和分析）是一个好处。如果查询操作的数据在执行查询的同时发生变化，那么很难对查询的含义进行推理。当事务可以看到数据库的一致快照（在特定时间点冻结）时，更容易理解。\n快照隔离是一个流行的特性：PostgreSQL、带有InnoDB存储引擎的MySQL、Oracle、SQL Server等都支持快照隔离[23、31、32]。\n（1）实现快照隔离\n与读提交隔离一样，快照隔离的实现通常使用写锁来防止脏写（请参阅第236页的“实现已提交读取”），这意味着进行写入的事务可以阻止另一个写入同一对象的事务的进程。但是，读取不需要任何锁。从性能的角度来看，快照隔离的一个关键原则是读者从不阻塞写者，而写者从不阻塞读者。这使得数据库能够在正常处理写操作的同时处理一致性快照上的长时间运行的读查询，而不会在两者之间发生任何锁争用。\n为了实现快照隔离，数据库使用了我们在图7-4中看到的防止脏读的机制。数据库可能必须保留一个对象的几个不同的提交版本，因为各种正在进行的事务可能需要在不同的时间点查看数据库的状态。因为它并行维护一个对象的多个版本，所以这种技术被称为多版本并发控制（MVCC）。\n如果数据库只需要提供读提交隔离，而不需要提供快照隔离，则只需保留对象的两个版本：已提交版本和已覆盖但尚未提交的版本。但是，支持快照隔离的存储引擎通常也将MVCC用于其已提交读取的隔离级别。一种典型的方法是，read committed对每个查询使用单独的快照，而快照隔离对整个事务使用相同的快照。\n图7-7说明了在PostgreSQL[31]中如何实现基于MVCC的快照隔离（其他实现类似）。当一个事务启动时，它被赋予一个唯一的、总是递增的事务ID（txid）。每当事务向数据库写入任何内容时，它写入的数据都会被标记为写入程序的事务ID。\n表中的每一行都有一个 created_by 字段，其中包含将此行插入表中的事务的 ID。此外，每一行都有一个 deleted_by 字段，它最初是空的。如果一个事务删除了一行，该行实际上并没有从数据库中删除，而是通过将 deleted_by 字段设置为请求删除的事务的 ID 来标记该行。稍后，当确定没有事务可以再访问已删除的数据时，数据库中的垃圾回收进程将删除标记为删除的所有行并释放它们的空间。\n更新在内部转换为删除和创建。例如，在图7-7中，事务13从帐户2中扣除100美元，将余额从500美元更改为400美元。accounts表现在实际上包含account2的两行：一行余额为$500，被事务13标记为deleted；另一行余额为$400，由事务13创建。\n（2）观察一致快照的可见性规则\n当一个事务从数据库中读取时，事务id用于决定它可以看到哪些对象，哪些对象是不可见的。通过仔细定义可见性规则，数据库可以向应用程序提供数据库的一致快照。其工作原理如下：\n1.在每个事务开始时，数据库列出当时正在进行（尚未提交或中止）的所有其他事务的列表。这些事务所做的任何写入都将被忽略，即使这些事务随后提交。 2.中止的事务所做的任何写入都将被忽略。 3.无论事务是否已提交，都将忽略具有事务 ID 较大的事务（即，在当前事务启动之后启动的事务）所做的任何写入。 4.所有其他写入对应用程序的查询可见。\n这些规则适用于对象的创建和删除。在图7-7中，当事务12从账户2读取时，它看到的余额是500美元，因为500美元余额的删除是由事务13完成的（根据规则3，事务12看不到事务13所做的删除），并且400美元余额的创建还不可见（根据相同的规则）。\n换句话说，如果以下两个条件都为真，则对象是可见的：\n 在 reader 事务启动时，创建对象的事务已经提交。 对象未标记为删除，或已被标记为删除，同时请求删除的事务在 reader 事务启动时尚未提交。  一个长时间运行的事务可能会继续使用快照很长一段时间，继续读取（从其他事务的角度来看）已被重写或删除的值。通过从不就地更新值，而是在每次更改值时创建一个新版本，数据库可以提供一致的快照，同时只会产生少量开销。\n（3）索引和快照隔离\n索引在多版本数据库中如何工作？一种选择是让索引简单地指向对象的所有版本，并需要一个索引查询来过滤掉当前事务不可见的任何对象版本。当垃圾回收删除不再对任何事务可见的旧对象版本时，也可以删除相应的响应索引项。\n在实践中，许多实现细节决定了多版本并发控制的性能。例如，如果同一个对象的不同版本可以放在同一页上，PostgreSQL有一些优化，可以避免索引更新[31]。\nCouchDB、Datomic和LMDB中使用了另一种方法。尽管它们也使用B树（见第79页的“B树”），但它们使用了一个只追加/写入时复制的变体，在更新树时不会覆盖树的页面，而是为每个修改过的页面创建一个新副本。父页（直到树的根）被复制和更新，以指向其子页的新版本。没有受到写入影响的页，无需拷贝，仍然维持不可变状态 [33,34,35].\n使用仅追加B树，每个写入事务（或一批事务）都会创建一个新的B树根，而特定的根是数据库在创建时的一致快照。无法根据现有的根来修改后续的ID树，因为它们不能根据现有的根来创建新的对象。但是，这种方法还需要一个用于压缩和垃圾收集的后台过程。\n（4）可重复阅读和命名混乱\n快照隔离是一个有用的隔离级别，特别是对于只读事务。然而，许多实现它的数据库使用不同的名称来调用它。在Oracle中称为serializable，在PostgreSQL和MySQL中称为repeatable read[23]。\n造成这种命名混乱的原因是SQL标准没有快照隔离的概念，因为该标准是基于systemr 1975年对隔离级别的定义[2]，而快照隔离当时还没有发明。相反，它定义了可重复读取（repeatable read），这在表面上看起来类似于快照隔离。PostgreSQL和MySQL称其快照隔离级别为repeatable read，因为它满足标准的要求，因此它们可以声明符合标准。\n不幸的是，SQL标准对隔离级别的定义是有缺陷的——它模糊、不精确，而且不像标准那样独立于实现[28]。尽管有几个数据库实现了可重复读取，但它们实际提供的保证却有很大的不同，尽管表面上是标准化的[23]。在研究文献[29，30]中有一个可重复阅读的正式定义，但大多数实现并不满足这个正式定义。最重要的是，ibmdb2使用“repeatableread”来表示可序列化性[8]。\n因此，没有人真正了解可重复阅读的含义。\n防止丢失更新 到目前为止，我们讨论的读提交和快照隔离级别主要是为了保证在并发写入的情况下只读事务可以看到什么。我们基本上忽略了并发写两个事务的问题，我们只讨论了脏写（参见第235页的“无脏写”），一种可能发生的写-写冲突的特殊类型。\n在并发编写事务之间还可能发生其他几种有趣的冲突。其中最著名的是丢失更新问题，图7-1以两个并发计数器增量为例进行了说明。\n如果应用程序从数据库中读取某个值，对其进行修改，然后将修改后的值写回（read-modify-write cycle），则可能发生更新丢失问题。如果两个事务同时执行此操作，则其中一个修改可能会丢失，因为第二次写入不包括第一次修改。（我们有时会说，后一个写会比前一个写强。）这种模式出现在各种不同的场景中：\n 增加计数器或更新帐户余额（需要读取当前值、计算新值并回写更新后的值） 对复杂值进行局部更改，例如，在 JSON 文档中向列表中添加元素（需要解析文档、进行更改并写入返回修改后的文档） 两个用户同时编辑wiki页面，每个用户通过将整个页面内容发送到服务器来保存他们的更改，覆盖数据库中当前的内容  因为这是一个如此普遍的问题，各种各样的解决方案都被开发出来了。\n（1）原子写入操作\n许多数据库提供原子更新操作，这就不需要在应用程序代码中实现 read-modify-write cycle。如果你的代码可以用这些操作来表达，它们通常是最好的解决方案。例如，以下指令在大多数关系数据库中是并发安全的：\nUPDATE counters SET value = value + 1 WHERE key = \u0026#39;foo\u0026#39;; 类似地，MongoDB等文档数据库支持对JSON文档的一部分进行原子本地修改，Redis为修改数据结构（如优先级队列）提供了原子操作。并不是所有的写操作都可以很容易地用原子操作来表达例如，wiki页面的更新涉及到任意的文本编辑，但是在可以使用原子操作的情况下，它们通常是最好的选择。\n原子操作通常是通过在对象被读取时对其进行独占锁来实现的，这样在应用更新之前没有其他事务可以读取它。这种技术有时被称为光标稳定性[36，37]。另一个选择是简单地强制在单个线程上执行所有原子操作。\n不幸的是，对象关系映射框架很容易意外地编写执行不安全的 read-modify-write cycle 的代码，而不是使用数据库提供的原子操作[38]。如果你知道你在做什么，这不是问题，但它可能是一个微妙的错误来源，很难通过测试找到。\n（2）显式锁定\n如果数据库的内置原子操作没有提供必要的功能，另一个防止更新丢失的选项是应用程序显式锁定要更新的对象。然后，应用程序可以执行read-modify-write cycle，如果任何其他事务试图并发读取同一个对象，它将被迫等待，直到第一个 read-modify-write 循环完成。\n例如，考虑一个多人游戏，其中几个玩家可以同时移动同一个图形。在这种情况下，原子操作可能还不够，因为应用程序还需要确保玩家的移动遵守游戏规则，这涉及到一些逻辑，而这些逻辑是您无法合理实现的数据库查询。相反，你可以使用一个锁来防止两个玩家同时移动同一个棋子，如例7-1所示。\nBEGIN TRANSACTION; SELECT * FROM figures WHERE name = \u0026#39;robot\u0026#39; AND game_id = 222 FOR UPDATE; -- Check whether move is valid, then update the position -- of the piece that was returned by the previous SELECT. UPDATE figures SET position = \u0026#39;c4\u0026#39; WHERE id = 1234; COMMIT;  FOR UPDATE 告诉数据库锁定返回的所有行。\n 这是可行的，但是要想正确，您需要仔细考虑您的应用程序逻辑。很容易忘记在代码的某个地方添加必要的锁，从而引入竞争条件。\n（3）自动检测丢失的更新\n原子操作和锁是通过强制 read-modify-write 周期顺序发生来防止更新丢失的方法。另一种方法是允许它们并行执行，如果事务管理器检测到丢失的更新，则中止事务并强制它重试 read-modify-write 循环。\n这种方法的一个优点是，数据库可以与快照隔离一起有效地执行此检查。实际上，PostgreSQL的repeatable read、Oracle的serializable和sqlserver的snapshot隔离级别都会自动检测更新丢失的时间，并中止有问题的事务。但是，MySQL/InnoDB的repeatable read没有检测到丢失的更新[23]。一些作者[28，30]认为数据库必须防止丢失更新，才能符合提供快照隔离的条件，因此MySQL在这个定义下不提供快照隔离。\n丢失更新检测是一个很好的特性，因为它不需要应用程序代码来使用任何特殊的数据库功能—您可能会忘记使用锁或原子操作，从而导致错误，但是丢失更新检测会自动发生，因此不易出错。\n（4）compare-and-set\n在不提供事务的数据库中，有时会发现一个原子 compare-and-set 操作（前面在230页的“单对象写入”中提到过）。此操作的目的是为了避免更新丢失，方法是只允许在值自上次读取后没有更改的情况下进行更新。如果当前值与以前读取的值不匹配，则更新无效，必须重试读-改-写循环。\n例如，为了防止两个用户同时更新同一个wiki页面，您可以尝试类似的操作，仅当用户开始编辑页面后该页面的内容没有更改时才会进行更新：\n-- This may or may not be safe, depending on the database implementation UPDATE wiki_pages SET content = \u0026#39;new content\u0026#39; WHERE id = 1234 AND content = \u0026#39;old content\u0026#39;; 如果内容已更改且不再与“旧内容”匹配，则此更新将无效，因此您需要检查更新是否生效，必要时重试。但是，如果数据库允许 WHERE 子句从旧快照中读取，则此语句可能无法阻止丢失的更新，因为即使正在发生另一个并发写入，该条件也可能为真。在依赖某个数据库之前，请检查它的 compare-and-set 操作是否安全的。\n（5）冲突解决和复制\n在复制的数据库中（见第5章），防止丢失更新具有另一个维度：由于它们在多个节点上有数据的副本，并且数据可能在不同的节点上被并发修改，因此需要采取一些额外的步骤来防止丢失更新。\nLocks、compare和set操作假定只有一个数据的最新副本。但是，具有多 Leader 或无 Leader 复制的数据库通常允许多个写操作同时发生，并以异步方式复制它们，因此它们不能保证有一个数据的最新副本。因此，基于锁或 compare-and-set 的技术不适用于这种情况。（我们将在第324页的“线性化”中更详细地讨论这个问题。）\n相反，正如第184页“检测并发写入”中所讨论的，这种复制数据库中的一种常见方法是允许并发写入创建一个值的多个冲突版本（也称为同级），并在事后使用应用程序代码或特殊数据结构来解析和合并这些版本。\n原子操作在复制的上下文中可以很好地工作，特别是当它们是可变的（也就是说，您可以在不同的副本上以不同的顺序应用它们，但仍然得到相同的结果）。例如，递增计数器或向集合中添加元素都是交换操作。这就是RIAK2.0数据类型背后的思想，它可以防止跨副本丢失更新。当一个值由不同的客户端同时更新时，Riak会自动合并更新，这样就不会丢失更新[39]。\n另一方面，last write wins（LWW）冲突解决方法容易丢失更新，如第186页“last write wins（丢弃并发写入）”中所述。不幸的是，LWW 是许多复制数据库中的默认值。\n写歪斜和幻影 在前面的部分中，我们看到了脏写和丢失更新，当不同的事务同时试图写入同一个对象时，会出现两种竞争情况。为了避免数据损坏，需要防止这些竞争条件——要么由数据库自动阻止，要么通过使用锁或原子写入操作等手动保护措施来防止。\n但是，这并不是并发写入之间可能发生的潜在争用条件列表的末尾。在本节中，我们将看到一些更微妙的冲突例子。\n首先，想象一下这个例子：您正在编写一个应用程序，让医生管理医院的随叫随到轮班。医院通常会在同一时间安排几名医生随叫随到，但绝对必须至少有一名医生随叫随到。医生可以放弃轮班（例如，如果他们自己生病了），前提是至少有一名同事在轮班时随时待命[40，41]。\n现在想象一下爱丽丝和鲍勃是某个特定班次的两个随叫随到的医生。两人都觉得不舒服，所以决定请假。不幸的是，他们碰巧在差不多同一时间按一下按钮就关机了。接下来的发生的如图7-8所示。\n在每个事务处理中，应用程序首先检查两个或多个医生当前正在呼叫；如果是，则假定一个医生停止呼叫是安全的。由于数据库使用快照隔离，所以两个检查都返回2，因此这两个事务都将继续进行下一个阶段。爱丽丝更新了自己的记录以取消通话，鲍勃也更新了自己的记录。两个事务都提交了，现在没有医生随叫随到。这违反了你要求至少有一名医生随叫随到的要求。\n（1）描述写入倾斜\n这种异常称为写倾斜[28]。这既不是脏写也不是丢失更新，因为这两个事务正在更新两个不同的对象（分别是Alice和Bob的oncall记录）。在这里发生冲突并不那么明显，但这绝对是一个竞争条件：如果两个事务一个接一个地运行，那么第二个医生就不会停止通话。异常行为是可能的，因为事务同时运行。\n您可以将write skew看作是丢失更新问题的一个概括。如果两个事务读取相同的对象，然后更新其中的一些对象（不同的事务可能会更新不同的对象），则可能会发生写入倾斜。在不同事务更新同一对象的特殊情况下，会出现脏写或更新丢失异常（取决于时间）。\n我们看到有各种不同的方法来防止更新丢失。对于写倾斜，我们的选项更受限制：\n 原子单对象操作没有帮助，因为涉及多个对象。 在快照隔离的某些实现中，自动检测丢失的更新也没有帮助：在PostgreSQL的repeatable read、MySQL/InnoDB的repeatable read、Oracle的serializable或SQL Server的snapshot isolation level[23]中，不会自动检测到写倾斜。自动防止写入倾斜需要真正的可序列化隔离（请参阅第251页的“可序列化性”）。 某些数据库允许您配置约束，然后由数据库强制执行（例如，唯一性、外键约束或对特定值的限制）。但是，为了指定至少一个医生必须随叫随到，您需要一个涉及多个对象的约束。大多数数据库都没有对此类约束的内置支持，但是您可以使用触发器或物化视图来实现它们，具体取决于数据库[42]。 如果不能使用可序列化的隔离级别，在这种情况下，第二个最佳选择可能是显式锁定事务所依赖的行。在doctors示例中，您可以编写如下内容：  BEGIN TRANSACTION; SELECT * FROM doctors WHERE on_call = true AND shift_id = 1234 FOR UPDATE; UPDATE doctors SET on_call = false WHERE name = \u0026#39;Alice\u0026#39; AND shift_id = 1234; COMMIT;  FOR UPDATE 告诉数据库锁定返回的所有行。\n （２）写歪斜的更多例子\n写歪斜一开始看起来像是一个深奥的问题，但是一旦你意识到它，你可能会注意到它可能发生的更多情况。这里还有一些例子：\n－　会议室预订系统\n假设您要强制执行同一会议室不能同时有两个预订[43]。如果有人想预约，你先检查是否有任何冲突的预订（例如，同一房间的预订时间范围重叠），如果没有找到，则创建会议（请参见例7-2）：\nBEGIN TRANSACTION; -- Check for any existing bookings that overlap with the period of noon-1pm SELECT COUNT(*) FROM bookings WHERE room_id = 123 AND end_time \u0026gt; \u0026#39;2015-01-01 12:00\u0026#39; AND start_time \u0026lt; \u0026#39;2015-01-01 13:00\u0026#39;; -- If the previous query returned zero: INSERT INTO bookings (room_id, start_time, end_time, user_id) VALUES (123, \u0026#39;2015-01-01 12:00\u0026#39;, \u0026#39;2015-01-01 13:00\u0026#39;, 666); COMMIT; 不幸的是，快照隔离并不能阻止另一个用户同时插入冲突的会议。为了保证你不会再发生冲突，你就不需要再把它序列化了。\n 多人游戏  在例7-1中，我们使用了一个锁来防止更新丢失（也就是说，确保两个玩家不能同时移动同一个图形）。然而，让另一个棋盘上的两个不同的棋盘上的棋盘上的两个棋子的移动可能会违反规则。根据您正在实施的规则类型，您可能能够使用唯一约束，但否则您很容易出现写歪。\n 申请用户名  在一个每个用户都有一个唯一用户名的网站上，两个用户可以尝试同时使用相同的用户名创建帐户。您可以使用事务来检查是否使用了某个名称，如果没有，则使用该名称创建一个帐户。但是，与前面的示例一样，在快照隔离下这是不安全的。幸运的是，唯一约束在这里是一个简单的解决方案（尝试注册用户名的第二个事务将因违反约束而中止）。\n 防止双重支出  允许用户花钱或积分的服务需要检查用户的消费是否超过了他们的支出。您可以通过在用户帐户中插入一个10个暂定支出项目来实现这一点，列出帐户中的所有项目，并检查总和是否为正[44]。使用write skew时，可能会同时插入两个支出项，它们一起导致余额变为负数，但两个事务都不会注意到另一个。\n（3）导致写入倾斜的幻影\n所有这些例子都遵循类似的模式：\n1.SELECT 查询通过搜索符合某些搜索条件的行来检查是否满足了某些要求（至少有两个医生在打电话，当时该房间没有现有的预订，董事会上的职位上没有其他数字，用户名还没有被占用，还有钱在账户）。 2.根据第一个查询的结果，应用程序代码决定如何继续（可能继续执行操作，或者向用户报告错误并中止）。 3.如果应用程序决定继续，它将对数据库进行写入（插入、更新或删除）并提交事务。\n此写入的效果改变了第2步决策的前提条件。换言之，如果在提交写入操作后重复步骤1中的 SELECT 查询，则会得到不同的结果，因为写入操作更改了与搜索条件匹配的行集（现在，随叫随到的医生减少了一位，此时会议室已被预订，board 位置上的现在是被刚移动的棋子占据了，用户名现在被占用，帐户中的钱现在减少了）。\n这些步骤可能以不同的顺序出现。例如，您可以先执行 write，然后执行 SELECT 查询，最后根据查询的结果决定是中止还是提交。\n在医生随叫随到的例子中，步骤3中修改的行是步骤1中返回的行之一，因此我们可以通过在步骤1中锁定行（SELECT FOR UPDATE）来保证事务的安全性并避免写歪斜。但是，其他四个例子是不同的：它们检查是否缺少与某些搜索条件匹配的行，而write则添加一个与相同条件匹配的行。如果步骤1中的查询未返回任何行，则 SELECT FOR UPDATE 无法将锁附加到任何内容。\n当一个事务中的写入操作更改另一个事务中的搜索查询结果时，这种效果称为幻影[3]。快照隔离避免了只读查询中的幻影，但是在读写事务中，像我们讨论的例子一样，幻影可能导致特别棘手的写倾斜情况。\n（4）物化冲突\n如果幻影的问题是没有可以附加锁的对象，也许我们可以人为地将锁对象引入数据库？\n例如，在会议室预订案例中，您可以想象创建一个时间表和房间。此表中的每一行对应于特定时间段（例如，15分钟）的特定房间。您可以提前为房间和时间段的所有可能组合创建行，例如在接下来的六个月内。现在，想要创建预订的事务可以锁定（选择更新）表中与所需房间和时间段相对应的行。在获得锁之后，它可以检查重叠预订并像以前一样插入新预订。请注意，附加表并不是用来存储有关预订的信息，它只是一个锁的集合，用于防止在同一房间和时间范围内的预订同时被修改。\n这种方法被称为物化冲突，因为它采用了一个幻象，并将其转化为数据库中存在的一组具体行的锁冲突[11]。不幸的是，很难找出如何将冲突具体化，并且很容易出错，并且让并发控制机制泄漏到应用程序数据模型中是很难看的。出于这些原因，如果没有其他办法，应将具体化的冲突视为最后手段。在大多数情况下，可序列化的隔离级别更可取。\n序列化 在本章中，我们看到了几个容易出现竞争条件的事务的示例。某些争用条件可以通过 read committed 和 snapshot 隔离级别来阻止，但其他一些则不是。我们遇到了一些特别棘手的考试题，有书写歪斜和幻影。这是一个可悲的情况：\n 隔离级别很难理解，并且在不同的数据库中实现不一致（例如，“可重复读取”的含义差别很大）。 如果您查看应用程序代码，很难判断在特定隔离级别运行是否安全，尤其是在大型应用程序中，您可能没有意识到可能同时发生的所有事情。 没有好的工具来帮助我们检测竞态情况。原则上，静态分析可能会有所帮助[26]，但研究技术尚未找到实际应用的途径。测试并发性问题是很困难的，因为它们通常是不确定的问题，只有在时间安排不好的情况下才会发生。  自从20世纪70年代以来，隔离问题就不再是新的了。一直以来，研究人员的答案都是简单的：使用可序列化隔离！\n串行隔离通常被认为是最强的隔离级别。它保证即使事务可以并行执行，最终结果也与它们一次执行一个事务一样，串行执行，没有任何并发性。因此，数据库保证，如果事务在单独运行时行为正确，那么它们在并发运行时仍然是正确的，换句话说，数据库可以防止所有可能的竞争条件。\n但是，如果可序列化的隔离比糟糕的弱隔离级别要好得多，那么为什么每个人都不使用它呢？为了回答这个问题，我们需要研究实现序列化的选项，以及它们的执行方式。目前大多数提供可序列化性的数据库都使用以下三种技术之一，我们将在本章的其余部分中探讨这些技术：\n 按串行顺序逐字执行事务（参见第252页的“实际串行执行”） 二阶段锁定（见第257页的“二阶段锁定（2PL）”），几十年来，这是唯一可行的选择 乐观并发控制技术，如可序列化快照隔离（请参阅第261页的“可序列化快照隔离（SSI）”）  现在，我们将主要在单节点数据库的上下文中讨论这些技术；在第9章中，我们将研究如何将这些技术推广到分布式系统中涉及多个节点的事务。\n实际串行执行 避免并发问题的最简单方法是完全消除并发性：在一个线程上一次只执行一个事务，按串行顺序。通过这样做，我们可以完全防止事务之间的冲突。\n尽管这似乎是一个显而易见的想法，但数据库设计者直到最近才——大约在2007年左右——才认为执行事务的单线程循环是可行的[45]。如果在过去的30年中，多线程并发被认为是获得良好性能的关键，那么，为了使单线程执行成为可能，有什么变化呢？\n有两个发展引起了这种反思：\n RAM变得足够便宜，对于许多用例来说，将整个活动数据集保存在内存中是可行的（请参见第页的“将所有数据都保存在内存中”） 88页）。当一个事务需要访问的所有数据都在内存中时，事务的执行速度比必须等待从磁盘加载数据的速度快得多。 数据库设计者意识到，OLTP事务通常很短，只进行少量的读写操作（参见“事务处理或分析“第90页）。相比之下，长时间运行的分析查询通常是只读的，因此可以在串行执行循环之外的一致快照上运行（使用快照隔离）。  串行执行事务的方法在VoltDB/H-Store、Redis和Datomic中实现[46，47，48]。为单线程执行而设计的系统有时比支持并发的系统性能更好，因为它可以避免锁的协调开销。然而，它的吞吐量仅限于单个CPU核心的吞吐量。为了充分利用单线程，事务需要与传统形式不同的结构。\n（1）在存储过程中封装事务\n在数据库的早期，其目的是数据库事务可以包含整个用户活动流。例如，预订机票是一个多阶段的过程（搜索路线、票价和可用座位；决定行程；预订行程中每个航班的座位；输入乘客详细信息；付款）。数据库设计人员认为，如果整个过程是一个事务，这样就可以以原子方式提交。\n不幸的是，人类做出决定和做出反应的速度非常慢。如果数据库事务需要等待用户的输入，则数据库需要支持大量的并发事务，其中大多数事务是空闲的。大多数数据库无法有效地做到这一点，因此几乎所有的OLTP应用程序都通过避免在事务中交互地等待用户来缩短事务处理时间。在web上，这意味着在同一个HTTP请求中提交一个事务—一个事务不跨越多个请求。一个新的HTTP请求开启一个新的事务。\n即使人类已经脱离了关键路径，事务仍然以交互的客户端/服务器风格执行，一次只执行一个语句。应用程序进行查询，读取结果，或者进行另一个查询 取决于第一次查询的结果，依此类推。查询和结果在应用程序代码（在一台机器上运行）和数据库服务器（在另一台机器上）之间来回发送。\n在这种交互式事务风格中，应用程序和数据库之间的网络通信花费了大量时间。如果不允许数据库中的并发，并且一次只处理一个事务，那么吞吐量将非常糟糕，因为数据库将花费大部分时间等待应用程序为当前事务发出下一个查询。在这种数据库中，为了获得合理的性能，需要同时处理多个事务。\n因此，具有单线程串行事务处理的系统不允许交互式多语句事务。相反，应用程序必须提前将整个事务代码作为存储过程提交到数据库。这些方法之间的差异如图7-9所示。如果事务所需的所有数据都在内存中，则存储过程可以非常快速地执行，而无需等待任何网络或磁盘I/O。\n（2）存储过程的利弊\n存储过程在关系数据库中已经存在了一段时间，自1999年以来，它们已经成为SQL标准（SQL/PSM）的一部分。他们的名声有点不好，原因有很多：\n 每个数据库供应商都有自己的存储过程语言（Oracle有PL/SQL，SQL Server有T-SQL，PostgreSQL有PL/pgSQL等）。这些语言 没有跟上通用编程语言的发展，因此，从今天的角度来看，它们看起来非常丑陋和过时，而且它们缺乏大多数编程语言所能找到的库的生态系统。 数据库中运行的代码很难管理：与应用服务器相比，调试更困难，版本控制和部署更困难，测试更困难，并且难以与度量收集系统集成以进行监视。 数据库通常比应用程序服务器对性能敏感得多，因为单个数据库实例通常由多个应用程序服务器共享。数据库中写得不好的存储过程（例如，使用大量内存或CPU时间）比在应用服务器中编写得不好的代码引起的麻烦要多得多。  然而，这些问题是可以克服的。存储过程的现代实现已经放弃了PL/SQL，而是使用现有的通用编程语言：VoltDB使用Java或Groovy，Datomic使用Java或Clojure，Redis使用Lua。\n使用存储过程和内存中的数据，可以在单个线程上执行所有事务。由于它们不需要等待I/O，并且避免了其他并发控制机制的开销，所以它们在单线程上可以获得相当好的吞吐量。\nVoltDB还使用存储过程进行复制：它不是将事务的写操作从一个节点复制到另一个节点，而是在每个replica上执行相同的存储过程。因此，VoltDB要求存储过程是确定性的（当在不同的节点上运行时，它们必须产生相同的结果）。例如，如果一个事务需要使用当前的日期和时间，它必须通过特殊的确定性api来使用。\n（3）分区\n串行地执行所有事务使并发控制更加简单，但它使数据库的事务吞吐量达到单台计算机上单个CPU核心的速度。只读事务可以使用快照隔离在其他地方执行，但对于具有高写入吞吐量的应用程序，单线程事务处理器可能成为严重的瓶颈。\n为了扩展到多个CPU核和多个节点，您可以潜在地划分数据（见第6章），这是VoltDB支持的。如果您能找到一种方法来对数据集进行分区，使每个事务只需要在一个分区中读写数据，那么每个分区都可以有自己独立于其他分区运行的事务处理线程。在这种情况下，您可以为每个CPU核心分配自己的分区，这样您的事务吞吐量就可以随着CPU核心的数量线性扩展[47]。\n但是，对于任何需要访问多个分区的事务，数据库必须在它所涉及的所有分区之间协调事务。存储过程需要在所有分区中以锁步执行，以确保整个系统的可序列化性。\n由于跨分区事务有额外的协调开销，因此它们比单分区事务慢得多。VoltDB报告的吞吐量大约为每秒1000次跨分区写入，这比它的单分区吞吐量低一个数量级，并且不能通过添加更多的机器来提高[49]。\n事务是否可以是单个分区在很大程度上取决于应用程序使用的数据结构。简单的键值数据通常可以非常容易地进行分区，但是具有多个二级索引的数据可能需要大量的跨分区协调（请参见第206页的“分区和辅助索引”）。\n（4）串行执行摘要\n在某些限制条件下，事务的串行执行已成为实现可串行化隔离的可行方法：\n 每一笔事务都必须是小而快的，因为只需一次缓慢的事务就可以拖延所有的事务处理。 仅限于活动数据集可以放入内存的用例。很少访问的数据可能会被移动到磁盘上，但是如果需要在单线程事务中访问它，系统将变得非常慢 写入吞吐量必须足够低，以便在单个CPU核心上处理，否则需要在不需要跨分区协调的情况下对事务进行分区。 跨分区事务是可能的，但它们的使用范围有一个硬限制。  二阶段锁定 (2PL) 在大约30年的时间里，只有一种广泛使用的数据库可串行化算法：两阶段锁定（2PL）。\n我们之前已经看到锁经常被用来防止脏写（见第235页的“无脏写”）：如果两个事务同时试图写入同一个对象，锁确保第二个 writer 必须等到第一个事务完成（中止或提交）后才能继续。\n2PL 与此类似，但使锁的要求更高。只要没有人在写同一个对象，就允许多个事务同时读取该对象。但只要有人想写（修改或删除）对象，就需要独占访问：\n 如果事务A已读取对象，而事务B希望写入该对象，则B必须等到A提交或中止后才能继续。（这个确保B不会在A背后意外更改对象。） 如果事务A已写入对象，而事务B要读取该对象，则B必须等到A提交或中止后才能继续。（在2PL下，读取对象的旧版本，如图7-1所示，是不可接受的。）  在2PL中，写 writer 不仅仅阻止其他 writer；他们还阻止 reader，反之亦然。快照隔离有这样一个咒语： reader 从不阻止 writer，writer 从不阻止 reader（请参阅第239页的“实现快照隔离”），它抓住了快照隔离和两阶段锁定之间的关键区别。另一方面，由于2PL提供了可序列化性，因此它可以防止前面讨论的所有竞争条件，包括丢失更新和写入倾斜。\n（1）两阶段锁的实现\n2PL用于MySQL（InnoDB）和SQL Server中的可序列化隔离级别，以及DB2中的可重复读取隔离级别[23，36]。\n对 reader 和 writer 的阻塞是通过对数据库中的每个对象都有一个锁来实现的。锁可以是共享模式，也可以是独占模式。锁的用途如下：\n 如果事务要读取对象，必须首先在共享模式下获取锁。允许多个事务同时在共享模式下持有锁，但如果另一个事务已经对对象拥有独占锁，则这些事务必须等待。 如果事务要写入对象，则必须首先以独占模式获取锁。没有其他事务可以同时持有锁（在共享或独占模式下），因此如果对象上存在任何现有锁，则该事务必须等待。 如果事务先读后写对象，它可能会将其共享锁升级为独占锁。升级的工作原理与直接获得独占锁的方式相同。 事务获得锁后，它必须继续持有锁，直到事务结束（提交或中止）。这就是“two phase”这个名称的由来：第一阶段（在事务执行期间）是在获取锁的时候，第二阶段（在事务结束时）是在释放所有锁的时候。 由于使用了如此多的锁，很容易发生事务A被卡住等待事务B释放其锁，反之亦然。这种情况称为死锁。数据库自动检测事务之间的死锁，并中止其中一个事务，以便其他事务可以继续进行。应用程序需要重试中止的事务。  （2）两阶段锁性能\n两阶段锁的一大缺点，以及自20世纪70年代以来没有被所有人使用的原因是性能：在两阶段锁定下，事务吞吐量和查询响应时间比在弱隔离下要差得多。\n这部分是由于获取和释放所有这些锁的开销，但更重要的是由于减少了并发性。按照设计，如果两个并发事务试图做任何可能以任何方式导致争用条件的操作，则一个事务必须等待另一个事务完成。\n传统的关系数据库不限制事务的持续时间，因为它们是为等待人工输入的交互式应用程序设计的。\n因此，当一个事务必须等待另一个事务时，它可能需要等待的时间没有限制。即使您确保您的所有事务都很短，如果多个事务要访问同一个对象，则可能会形成一个队列，因此一个事务可能必须等待其他几个事务完成才能执行任何操作。由于这个原因，运行2PL的数据库可能有相当不稳定的延迟，如果工作负载中存在争用，那么它们在高百分位（请参阅第13页的“描述性能”）时会非常慢。它可能只需要一个缓慢的事务，或者一个访问大量数据并获取许多锁的事务，就会导致系统的其余部分陷入停顿。当需要稳健的操作时，这种不稳定性是有问题的。\n虽然死锁可以在基于锁的读提交隔离级别下发生，但在2PL可序列化隔离下（取决于事务的访问模式），死锁发生的频率要高得多。这可能是一个额外的性能问题：当事务因死锁而中止并重试时，它需要重新执行其工作。如果死锁频繁发生，这可能意味着大量的精力浪费。\n（3）谓词锁\n在前面对锁的描述中，我们忽略了一个微妙但重要的细节。在第250页的“导致写入倾斜的幻影”中，我们讨论了幻影的问题，也就是说，一个事务改变了另一个事务的搜索查询结果。具有可序列化隔离的数据库必须防止幻影。\n在会议室预订示例中，这意味着如果一个事务在特定时间窗口内搜索房间的现有预订（请参见例7-2），不允许另一个事务同时插入或更新同一房间和时间范围的另一个预订。（可以同时插入其他房间的预订，或在不影响拟定预订的不同时间插入同一房间的预订。）\n我们如何实现这一点？从概念上讲，我们需要一个谓词锁[3]。它类似于前面描述的共享/独占锁，但它不属于特定对象（例如，表中的一行），而是属于与某些搜索条件匹配的所有对象，例如：\nSELECT * FROM bookings WHERE room_id = 123 AND end_time \u0026gt; \u0026#39;2018-01-01 12:00\u0026#39; AND start_time \u0026lt; \u0026#39;2018-01-01 13:00\u0026#39;; 谓词锁的限制访问，如下所示：\n 如果事务 A 想要读取与某个条件匹配的对象，比如在那个 SELECT 查询中，它必须获得对查询条件的共享模式谓词锁。如果另一个事务B当前对任何符合这些条件的对象具有排他锁，则 A 必须等到 B 释放其锁后才允许进行查询。 如果事务A要插入、更新或删除任何对象，它必须首先检查旧值还是新值与任何现有谓词锁匹配。如果事务B持有匹配的谓词锁，那么A必须等到B提交或中止后才能继续。  这里的关键思想是谓词锁甚至适用于数据库中尚不存在但将来可能添加的对象（幻影）。如果两阶段锁包括谓词锁，则数据库将防止所有形式的写倾斜和其他竞争条件，因此它的隔离将变为可序列化的。\n（4）Index-range 锁\n不幸的是，谓词锁的性能不好：如果活动事务有很多锁，那么检查匹配的锁就会非常耗时。因此，大多数使用2PL的数据库实际上实现了 Index-range 锁（也称为 next-key 锁），这是谓词锁的简化近似值[41，50]。\n通过使谓词与更大的对象集相匹配来简化谓词是安全的。例如，如果您有一个谓词锁，用于在中午到下午1点之间预订123号房间，您可以随时锁定123号房间的预订，或者您可以通过在中午到下午1点之间锁定所有房间（不仅仅是123号房间）来近似模拟它。这是安全的，因为任何与原始谓词匹配的 write 肯定也会匹配近似值。\n在room bookings数据库中，您可能会在 room_id 列上有索引，和/或在 start_time和 end_time 上有索引（否则前面的查询在大型数据库上会非常慢）：\n 假设您的索引在 room_id 上，数据库使用该索引查找123房间的现有预订。现在，数据库可以简单地将一个共享锁附加到这个索引项上，这表明一个事务已经搜索了123房间的预订。 或者，如果数据库使用基于时间的索引来查找现有预订，则可以为该索引中的一系列值附加一个共享锁，表示事务已搜索到与2018年1月1日中午至下午1点重叠的预订。  不管怎样，搜索条件的近似值都会附加到其中一个索引上。现在，如果另一个事务要插入、更新或删除同一房间和/或重叠时间段的预订，则必须更新索引的相同部分。在这样做的过程中，它将遇到共享锁，并将被迫等待，直到锁被释放。\n这为防止幻影和写歪斜提供了有效的保护。Index-range 锁并不像谓词锁那样精确（它们可能会锁定更大范围的对象，而不是严格地维护可序列化性所必需的），但是由于索引范围锁的开销要低得多，所以它们是一个很好的折衷方案。\n如果没有合适的索引可以附加一个范围锁，那么数据库可以退回到整个表上的共享锁。这对性能没有好处，因为它将停止所有其他事务写入表，但这是一个安全的后备位置。\n可序列化快照隔离（SSI） 本章描绘了数据库中并发控制的惨淡图景。一方面，我们的可串行化实现不能很好地执行（两阶段锁定）或伸缩性不好（串行执行）。另一方面，我们有弱隔离级别，这些级别具有良好的性能，但是容易出现各种竞争情况（丢失更新、写入倾斜、幻影等）。串行隔离和良好的性能在根本上是相互矛盾的吗？\n也许不是这样：一种叫做串行化快照隔离（SSI）的算法非常有效。它提供了完全的可序列化性，但与快照隔离相比，性能损失很小。SSI是相当新的：它在2008年首次被描述[40]，是迈克尔·卡希尔博士论文[51]的主题。\n如今，SSI既用于单节点数据库（PostgreSQL自9.1[41]版本以来的可串行化隔离级别）也用于分布式数据库（FoundationDB使用类似的算法）。由于与其他并发控制机制相比，SSI还很年轻，它仍在实践中证明它的性能，但它有可能以足够快的速度成为未来新的默认机制。\n（1）悲观与乐观并发控制\n两阶段锁是一种所谓的悲观并发控制机制：它基于这样一个原则：如果任何事情可能出错（如另一个事务持有的锁所示），最好等到情况再次安全后再做任何事情。它就像互斥，在多线程编程中用于保护数据结构。\n从某种意义上讲，串行执行是悲观到极端的：它本质上相当于每个事务在事务持续期间对整个数据库（或数据库的一个分区）具有排他锁。我们通过使每个事务的执行速度非常快来补偿这种悲观情绪，因此它只需要在短时间内保持“锁”。\n相比之下，可串行化快照隔离是一种乐观的并发控制技术。在这种情况下，乐观意味着，如果发生了非常危险的事情，事务将继续进行，希望一切都会好起来。当一个事务需要提交时，数据库会检查是否发生了任何不好的事情（例如，是否违反了隔离）；如果发生了，事务操作将被中止并必须重试。只允许提交可序列化执行的事务。\n乐观并发控制是一个古老的概念[52]，它的优缺点已经争论了很长时间[53]。如果存在高冲突（许多事务试图访问相同的对象），那么它的性能会很差，因为这会导致大量事务需要中止。如果系统已经接近其最大吞吐量，则重试事务的额外事务负载可能会使性能更差。\n然而，如果有足够的空闲容量，并且事务之间的争用不是太高，乐观并发控制技术往往比悲观并发控制技术性能更好。通过交换原子操作可以减少争用：例如，如果多个事务并发地想要增加一个计数器，那么递增的应用顺序无关紧要（只要计数器不是在同一事务中读取的），因此并发的增量可以全部应用而不会发生冲突。\n顾名思义，SSI基于快照隔离，也就是说，事务中的所有读取都是从数据库的一致快照进行的（请参阅第237页的“快照隔离和可重复读取”）。这是与早期的乐观并发控制技术相比的主要区别。在快照隔离的基础上，SSI添加了一个算法，用于检测写入之间的序列化冲突并确定要中止哪些事务。\n（2）基于过时前提的决策\n当我们先前讨论快照隔离中的写歪斜（请参阅246页的“write skew and Phantoms”）时，我们观察到了一个反复出现的模式：事务从数据库中读取一些数据，检查查询的结果，并决定根据所看到的结果采取一些操作（写入数据库）。但是，在快照隔离下，原始查询的结果可能在事务提交时不再是最新的，因为数据可能同时被修改。\n换言之，事务是基于一个前提（在事务开始时这一事实是正确的，例如，“目前有两名医生随时待命”）。以后，当事务要提交时，原来的数据可能已经改变，前提可能不再是真的。\n当应用程序进行查询时（例如，“当前有多少医生在待命？），数据库不知道应用程序逻辑如何使用该查询的结果。为了安全起见，数据库需要假定查询结果（前提）中的任何更改都意味着在该事务中的写入可能无效。换句话说，在事务中的查询和写入之间可能存在因果依赖关系。为了提供可序列化的隔离，数据库必须检测事务可能在过时的前提下执行的情况，并在这种情况下中止事务。\n数据库如何知道查询结果是否已更改？有两种情况需要考虑：\n 检测对过时的 MVCC 对象版本的读取（在读取之前发生未提交的写入） 检测影响先前读取的写入（写入发生在读取之后）  （3）检测过时的MVCC读取\n回想一下快照隔离通常是通过多版本并发控制（MVCC）实现的，见图7-10。当一个事务从MVCC数据库中的一致快照中读取时，它将忽略在拍摄快照时尚未提交的任何其他事务所做的写入操作。在图7-10，事务43将Alice视为 on_call = true，因为事务42（修改了Alice的 on-call 状态）是未提交的。然而，当事务43想要提交时，事务42已经提交。这意味着从一致快照读取时被忽略的写操作现在已经生效，事务43的前提不再正确。\n为了防止这种异常，数据库需要跟踪一个事务何时由于 MVCC 可见性规则而忽略另一个事务的写操作。当事务需要提交时，数据库会检查是否有任何被忽略的写操作已经提交。如果是，则必须中止事务。\n为什么必须要在提交之前一直等待？为什么不在检测到过时读取时立即中止事务 43？好吧，如果事务43是一个只读事务，它就不需要被中止，因为没有写歪斜的风险。当事务43进行读取时，数据库还不知道该事务是否将在以后执行写入。此外，事务42在提交事务43时可能尚未中止或仍然是未提交的，因此读取可能最终没有过时。通过避免对快照的长时间隔离而中止快照的不必要的隔离。\n（4）检测影响先前读取的写入\n要考虑的第二种情况是当另一个事务在读取数据后修改它。这种情况如图7-11所示。\n在两阶段锁的上下文中，我们讨论了索引范围锁（请参阅第260页的“索引范围锁”），它允许数据库锁定对与某些搜索查询匹配的所有行的访问，例如 WHERE shift_id = 1234。我们可以在这里使用类似的技术，除了SSI锁不阻止其他事务。\n在图7-11中，事务42和事务43都搜索1234班次期间的随叫随到医生。如果 shift_id 上有索引，数据库可以使用索引条目1234记录事务42和43读取此数据的事实。（如果没有索引，则可以在表级别跟踪此信息。）此信息只需保留一段时间：在一个事务完成（提交或中止），并且所有并发事务完成后，数据库可以忘记它读取的数据。\n当事务写入数据库时，它必须在索引中查找最近读取受影响数据的任何其他事务。这个过程类似于在受影响的 Key 范围内获取写锁，但它并不是在 reader 提交之前阻塞，而是充当一个tripwire：它只是通知事务，它们读取的数据可能不再是最新的。\n在图7-11中，事务43通知事务42其先前的读取已过时，反之亦然。事务42是第一个提交的事务，它是成功的：虽然事务43的写入影响了42，但是43还没有提交，所以写操作还没有生效。但是，当事务43要提交时，来自42的冲突写入已经提交，因此43必须中止。\n（5）可串行化快照隔离的性能\n和往常一样，许多工程细节会影响算法在实践中的工作效果。例如，一个权衡是跟踪事务读写的粒度。如果数据库非常详细地跟踪每个事务的活动，就可以精确地知道哪些事务需要中止，但是簿记开销可能会变得很大。不太详细的跟踪会更快，但可能会导致比严格要求更多的事务被中止。\n在某些情况下，事务可以读取被另一个事务覆盖的信息：根据发生的其他情况，有时可以证明执行的结果仍然是可序列化的。PostgreSQL使用这个理论来减少不必要的中止[11,41]。\n与两阶段锁定相比，可序列化快照隔离的最大优点是一个事务不需要阻塞等待另一个事务持有的锁。与快照隔离下一样，写入程序不会阻止 reader，反之亦然。这种设计原则使查询延迟更具可预测性和更少的可变性。特别是，只读查询可以在一致的快照上运行，而不需要任何锁，这对于读重的工作负载非常有吸引力。\n与串行执行相比，可串行化快照隔离不限于单个CPU内核的吞吐量：FoundationDB将串行化冲突的检测分布在多台机器上，使其能够扩展到非常高的吞吐量。即使数据可以跨多台机器进行分区，事务也可以在多个分区中读写数据，同时确保可序列化的隔离[54]。\n中止率显著影响SSI的整体性能。对于一个长时间的事务来说，可能只需要很长的时间来执行读操作，所以可能只需要很长时间的读操作。但是，与两阶段锁定或串行执行相比，SSI对慢事务的敏感度可能更低。\n总结 事务是一个抽象层，允许应用程序假装某些并发问题和某些类型的硬件和软件故障不存在。一大类错误被简化为一个简单的事务中止，应用程序只需要再试一次。\n在本章中，我们看到了许多事务有助于预防的问题的示例。并非所有的应用程序都容易受到这些问题的影响：具有非常简单的访问模式（例如只读取和写入单个记录）的应用程序可能不需要事务就可以进行管理。但是，对于更复杂的访问模式，事务可以极大地减少您需要考虑的潜在错误情况的数量。\n如果没有事务，各种错误场景（进程崩溃、网络中断、电源中断、磁盘已满、意外并发等）意味着数据可能会以各种方式变得不一致。例如，可以很容易地与非规范化数据源同步。如果没有事务，就很难推断复杂的交互访问对数据库的影响。\n在本章中，我们特别深入地讨论了并发控制的主题。我们讨论了几种广泛使用的隔离级别，特别是提交读取、快照隔离（有时称为可重复读取）和可序列化。我们通过讨论各种种族状况的例子来描述这些孤立程度：\n 脏读：一个客户端在另一个客户端的写操作提交之前读取它们。read committed 隔离级别和更高级别可防止脏读。 肮写：一个客户端覆盖另一个客户端已写入但尚未提交的数据。几乎所有事务实现都防止脏写。 读倾斜（不可重复读取）：客户端在不同的时间点看到数据库的不同部分。此问题通常通过快照隔离来防止，快照隔离允许事务在一个时间点从一致的快照中读取。它通常通过多版本并发控制（MVCC）来实现。 丢失的更新：两个客户端同时执行一个读-改-写循环。一个重写另一个的写操作而不合并其更改，因此数据丢失。快照隔离的一些实现可以自动防止这种异常，而其他实现则需要手动锁定（选择进行更新）。 写倾斜：事务读取某些内容，根据所看到的值做出决策，然后将决策写入数据库。然而，到了发稿的时候，这个决定的前提已经不成立了。只有可序列化的隔离才能防止这种异常。 幻影读：事务读取与某些搜索条件匹配的对象。另一个客户端执行的写入操作会影响搜索结果。快照隔离可防止直接的幻影读取，但写倾斜上下文中的幻影需要特殊处理，例如索引范围锁。  弱隔离级别可防止某些异常，但让应用程序开发人员手动处理其他异常（例如，使用显式锁定）。只有可序列化的隔离才能防止所有这些问题。我们讨论了实现可序列化事务的三种不同方法：\n 按顺序执行事务：如果你能让每一个事务执行得非常快、吞吐量足够低，可以在单个CPU内核上处理，这是一个简单而有效的选择。 两阶段锁定：几十年来，这一直是实现串行化的标准方法，但由于其性能特点，许多应用程序都避免使用它。 可序列化快照隔离（SSI）：一个相当新的算法，它避免了以前算法的大部分缺点。它使用了一种乐观的方法，允许事务在不阻塞的情况下继续进行。当一个事务需要提交时，它会被检查，如果执行不可序列化，它将被中止。  本章中的示例使用关系数据模型。然而，正如第231页“多对象事务的需要”中所讨论的，无论使用哪种数据模型，事务都是一种有价值的数据库特性。\n在这一章中，我们主要在数据库运行在一台机器上的环境中探索思想和算法。分布式数据库中的事务打开了一系列新的难题，我们将在下两章中讨论这些挑战。\n"});index.add({'id':220,'href':'/docs/programmer-interview/front-end/css-jiugongge/','title':"CSS 九宫格",'content':"CSS 九宫格 公共 CSS 属性 .square{ position: relative; width: 100%; height: 0; padding-bottom: 100%; /* padding百分比是相对父元素宽度计算的 */ margin-bottom: 30px; } .square-inner{ position: absolute; top: 0; left: 0; width: 100%; height: 100%; /* 铺满父元素容器，这时候宽高就始终相等了 */ } .square-inner\u0026gt;li{ width: calc(98% / 3); /* calc里面的运算符两边要空格 */ height: calc(98% / 3); margin-right: 1%; margin-bottom: 1%; overflow: hidden; } FlexBox HTML：\n\u0026lt;div class=\u0026#34;square\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;square-inner flex\u0026#34;\u0026gt; \u0026lt;li\u0026gt;1\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;2\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;3\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;4\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;5\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;6\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;7\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;8\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;9\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt;   CSS：\n.flex { display: flex; flex-wrap: wrap; } .flex \u0026gt; li{ flex-grow: 1; /* 子元素按1/n的比例进行拉伸 */ background-color: #4d839c; text-align: center; color: #fff; font-size: 50px; line-height: 2; } .flex \u0026gt; li:nth-of-type(3n) { /* 选择个数是3的倍数的元素 */ margin-right: 0; } .flex \u0026gt; li:nth-of-type(n+7) { /* 选择倒数的三个元素，n可以取0 */ margin-bottom: 0; }    Grid HTML \u0026lt;div class=\u0026#34;square\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;square-inner grid\u0026#34;\u0026gt; \u0026lt;div\u0026gt;1\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;2\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;3\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;4\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;5\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;6\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;7\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;8\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;9\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   CSS .grid { display: grid; grid-template-columns: repeat(3, 1fr); /* 相当于 1fr 1fr 1fr */ grid-template-rows: repeat(3, 1fr); /* fr单位可以将容器分为几等份 */ grid-gap: 1%; /* grid-column-gap 和 grid-row-gap的简写 */ grid-auto-flow: row; } .grid \u0026gt; div { color: #fff; font-size: 50px; line-height: 2; text-align: center; background: linear-gradient(to bottom, #f5f6f6 0%,#dbdce2 21%,#b8bac6 49%,#dddfe3 80%,#f5f6f6 100%); }    Float HTML \u0026lt;div class=\u0026#34;square\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;square-inner float\u0026#34;\u0026gt; \u0026lt;li\u0026gt;1\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;2\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;3\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;4\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;5\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;6\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;7\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;8\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;9\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt;   CSS .float::after { content: \u0026#34;\u0026#34;; display: block; clear: both; visibility: hidden; } .float \u0026gt; li { float: left; background-color: #42a59f; text-align: center; color: #fff; font-size: 50px; line-height: 2; } .float \u0026gt; li:nth-of-type(3n) { margin-right: 0; } .float \u0026gt; li:nth-of-type(n+7) { margin-bottom: 0; }    Table HTML \u0026lt;div class=\u0026#34;square\u0026#34;\u0026gt; \u0026lt;table class=\u0026#34;square-inner table\u0026#34;\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;3\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;4\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;5\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;6\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;7\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;8\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;9\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt;   CSS .table { border-collapse: separate; border-spacing: 0.57em; font-size: 14px; empty-cells: hide; table-layout: fixed; } .table \u0026gt; tbody \u0026gt; tr \u0026gt; td { text-align: center; background-color: #889esd8; overflow: hidden; }    参考  CSS实现自适应九宫格布局  "});index.add({'id':221,'href':'/docs/programmer-interview/java/hashmap/','title':"HashMap",'content':"HashMap get 方法如何实现的 public V get(Object key) { Node\u0026lt;K,V\u0026gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node\u0026lt;K,V\u0026gt; getNode(int hash, Object key) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; first, e; int n; K k; if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (first = tab[(n - 1) \u0026amp; hash]) != null) { if (first.hash == hash \u0026amp;\u0026amp; // always check first node  ((k = first.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode\u0026lt;K,V\u0026gt;)first).getTreeNode(hash, key); do { if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 首先使用 (n - 1) \u0026amp; key.hash 定位到这个 key 应该落到 table 的哪一个桶里面，其中 n 是指 table 的长度。然后看这个桶里面的 first 元素是否是目标值，如果是，直接返回。否则看 first 是否属于 TreeNode，如果是 TreeNode，则调用 getTreeNode 方法；否则沿着 e.next 往下遍历，并依次检查条件 key.hash == k.hash 并且 key.equals(k)。\n"});index.add({'id':222,'href':'/docs/books/ddia/ddia-chapter8/','title':"设计数据密集型应用程序 - 分布式系统的难点",'content':"设计数据密集型应用程序 - 分布式系统的难点 在最后几章中，一个反复出现的主题是系统如何处理出错的事情。例如，我们讨论了副本故障转移（第156页的“处理节点中断”）、复制延迟（“复制延迟的问题”，第161页）和事务的通用控制（“弱隔离级别”，第233页）。随着我们逐渐了解实际系统中可能出现的各种边缘情况，我们会更好地处理它们。\n然而，尽管我们谈了很多关于错误的话题，但最后几章仍然过于乐观。现实更加黑暗。我们现在将把我们的悲观情绪最大化，并假设任何可能出错的事情都会出错。我（经验丰富的系统运营商会告诉你这是一个合理的假设。如果你问得好的话，他们可能会一边给你讲一些可怕的故事，一边抚摸着过去战争留下的伤疤。）\n在分布式系统中工作与在一台计算机上编写软件有着根本的不同，主要的区别在于有许多新的和令人兴奋的方法来解决问题[1,2]。在这一章中，我们将领略到实践中出现的问题，并理解我们可以依赖和不能依赖的东西。\n最后，作为工程师，我们的任务是构建能够完成其工作的系统（即，满足用户期望的保证），尽管一切都出了问题。在第9章中，我们将看到一些在分布式系统中可以提供这种保证的算法的例子。但首先，在本章中，我们必须了解我们面临的挑战。\n本章对分布式系统中可能出现的问题进行了彻底的悲观和令人沮丧的概述。我们将研究网络问题（第277页“不可靠的网络”）；时钟和计时问题（“287页不可靠的时钟”）；并讨论它们在多大程度上是可以避免的。所有这些问题的后果都会让人迷失方向，所以我们将探讨如何思考一个分散的系统的状态，以及如何对已经发生的事情进行推理（“知识、真理和谎言”，第300页）\n故障和部分故障 当您在单台计算机上编写程序时，它通常会以相当可预测的方式运行：要么起作用，要么不起作用。 Buggy软件可能会显示出计算机有时“日子不好过”（此问题通常通过重新启动得以解决），但这主要是软件编写不当造成的。\n单台计算机上的软件应该是片状的并没有根本原因：当硬件正常工作时，相同的操作总是产生相同的结果（这是确定性的）。如果出现硬件问题（例如内存损坏或连接器松动），其后果通常是整个系统故障（例如，kernel panic，“蓝屏死机”，“启动失败”）。一台拥有良好软件的个人计算机通常要么功能完全正常，要么完全坏掉，但不能介于两者之间。\n在设计计算机时，这是一个经过深思熟虑的选择：如果发生内部故障，我们宁愿计算机完全崩溃，而不是返回错误的结果，因为错误的结果很难处理并且令人困惑。因此，计算机隐藏了实现它们的模糊物理现实，并提供了一个数学上完美运作的理想化系统模型。CPU指令总是做同样的事情；如果您将一些数据写入内存或磁盘，这些数据将保持完整，不会随机损坏。这种始终正确计算的设计目标可以追溯到第一台数字计算机[3]。\n当你写的软件运行在多台计算机上，通过一个网络连接起来，情况就完全不同了。在分布式系统中，我们不再是在一个理想化的系统模型中运行，我们别无选择，只能面对物理世界的混乱现实。在现实世界中，很多事情都可能出问题，正如这则轶事所说明的那样：\n 以我有限的经验，我曾处理过单个数据中心（DC）中长期存在的网络分区，PDU [配电单元]故障，交换机故障，整个机架的意外重启，整个DC骨干网故障，整个DC 停电，降血糖的司机将他的福特皮卡车砸到DC的HVAC（加热，通风和空调）系统中。 而且我甚至都不是行动主义者。 \u0026ndash; Coda Hale\n 在分布式系统中，即使系统的其他部分工作正常，也很有可能以某些无法预测的方式破坏了系统的某些部分。 这称为部分故障。 困难在于部分故障是不确定的：如果您尝试执行涉及多个节点和网络的任何操作，则该故障有时可能会工作，并且有时会出现无法预测的故障。\n正如我们将看到的，您甚至可能不知道某事是否成功，因为消息在网络上传输所花费的时间也是不确定的！ 这种不确定性和部分故障的可能性使分布式系统难以使用[5]。\n云计算和超级计算 关于如何构建大型计算系统，存在一系列哲学：\n 一方面是高性能计算（HPC）领域。 具有数千个CPU的超级计算机通常用于计算密集型科学计算任务，例如天气预报或分子计算 动力学（模拟原子和分子的运动）。 另一个极端是云计算，它的定义不是很明确[6]，但通常与多租户数据中心，与IP网络连接的商用计算机（通常是以太网），弹性/按需资源分配和计量相关帐单。 传统企业数据中心位于这些极端之间。  这些哲学带来了处理错误的不同方法。在超级计算机中，作业通常会不时地将其计算状态检查点确定为持久存储。 如果一个节点发生故障，一种常见的解决方案是简单地停止整个集群工作负载。 修复故障节点后，从最后一个检查点[7，8]重新开始计算。 因此，与分布式系统相比，超级计算机更像是单节点计算机：超级计算机通过使其逐步升级为完全故障来处理部分故障-如果系统的任何部分发生故障，则只需让所有程序崩溃（就像单个计算机上的内核崩溃一样） 机）。\n在本书中，我们重点介绍用于实现Internet服务的系统，这些系统通常看起来与超级计算机有很大的不同：\n 从某种意义上讲，许多与Internet相关的应用程序都是在线的，它们需要能够随时为用户提供低延迟的服务。使服务不可用（例如，停止群集进行修复）是不可接受的。相反，可以停止和重新启动离线（批处理）作业（如天气模拟），而影响却很小。 超级计算机通常由专用硬件构建，其中每个节点都非常可靠，并且节点通过共享内存和远程进行通信 直接内存访问（RDMA）。另一方面，云服务中的节点是由商用机器构建，由于规模经济，它们可以以较低的成本提供同等的性能，但故障率也更高。 大型数据中心网络通常基于IP和以太网，以Clos拓扑排列以提供较高的对等带宽[9]。超级计算机通常使用专门的网络拓扑，例如多维网格和圆环[10]，对于具有已知通信模式的HPC工作负载，它们会产生更好的性能。 系统越大，其组件之一损坏的可能性就越大。随着时间的流逝，破碎的事物会得到修复，而新的事物会破碎，但是在具有数千个节点的系统中，可以合理地假设某些事物总是破碎的[7]。当错误处理策略仅由放弃组成时，大型系统最终可能会花费大量时间从故障中恢复，而不是做有用的工作[8]。 如果系统可以容忍发生故障的节点并且仍然可以整体正常工作，那么这对于操作和维护是非常有用的功能：例如，您可以执行滚动升级（请参见第4章），一次重新启动一个节点，服务将继续为用户提供服务而不会中断。在云环境中，如果一台虚拟机性能不佳，则可以将其杀死并请求新的虚拟机（希望新的虚拟机更快）。 在地理上分散的部署中（将数据保持在地理上靠近您的用户以减少访问延迟），通信很可能通过Internet进行，与本地网络相比，这种通信速度慢且不可靠。超级计算机通常假定其所有节点都靠近在一起。  如果要使分布式系统正常工作，我们必须接受部分故障的可能性，并在软件中建立容错机制。 换句话说，我们需要使用不可靠的组件来构建可靠的系统。 （如第6页上的“可靠性”所述，没有完美的可靠性之类的东西，因此我们需要理解我们可以实际承诺的限制。）\n即使在仅包含几个节点的小型系统中，也要考虑部分故障。 在小型系统中，大多数组件很可能在大多数时间都能正常工作。 但是，系统的某些部分迟早会出现故障，软件必须以某种方式进行处理。 故障处理必须是软件设计的一部分，并且您（作为软件的操作员）需要知道发生故障时软件会带来什么行为。\n认为错误很少发生并希望每次获得的都是最好的结果是不明智的。 重要的是要考虑各种可能的故障，甚至是不太可能的故障，并在测试环境中人为地创建此类情况以查看会发生什么。 在分布式系统中，怀疑，悲观和偏执会产生回报。\n从不可靠的组件构建可靠的系统\n您可能想知道这是否有意义—直观上看，系统似乎只能与最不可靠的组件（最薄弱的环节）一样可靠。事实并非如此：实际上，从较不可靠的基础结构构建更可靠的系统是计算中的旧想法[11]。例如：\n 纠错码允许通过通信信道准确传输数字数据，例如由于无线网络上的无线电干扰，这种通信信道偶尔会出错一些[12]。 IP（Internet协议）不可靠：它可能会丢弃，延迟，重复或重新排序数据包。 TCP（传输控制协议）提供了更可靠的 IP上的传输层：它确保丢失的数据包被重新传输，重复数据被消除以及数据包按照发送的顺序重新组装。  尽管系统可能比其基础部分更可靠，但始终可以限制其可靠性。例如，纠错码可以处理少量的单位错误，但是如果您的信号被干扰淹没，则可以通过通信通道获取多少数据就受到了根本的限制[13]。 TCP可以向您隐藏数据包丢失，重复和重新排序的过程，但是它不能神奇地消除网络中的延迟。\n尽管可靠性更高的高级系统并不完美，但它仍然有用，因为它可以解决一些棘手的低级故障，因此通常可以更容易地推理和处理其余的故障。我们将在第519页的“端到端参数”中进一步探讨此问题。\n不可靠的网络 发送请求到最终获取到响应之间，可能出现很多问题：\n处理这种问题的一般方式就是：timeout。\n实践中的网络故障 必须处理网络中出现的各种故障。\n检查故障 系统需要自动检测故障：\n 负载均衡器不要将请求转发到 dead 节点上 分不是系统中，leader fail 后，其中一个 follower 需要晋升为 leader  特定场景下，或许可以知道出现了故障：\n 端口没有进程监听：操作系统自动发送 RST 或 FIN 包 进程 Crash，但是操作系统运行，脚本可以通知其他节点挂掉了，以便迅速接管 IP 不可达，会返回 ICMP 目标不可达报文  超时或没有上限的延迟 （1）网络拥塞 \u0026amp; 排队\n 交换机忙碌的时候，各个包需要排队；满了，会丢包 操作系统 CPU 正在忙碌，包到了，需要在操作系统层面排队 虚拟化环境，另外一个 VM 使用 CPU 的时候，此时当前 VM 什么事情也干不了 TCP 流量控制：自动调控发送的速率 TCP 未收到 Ack，就会重传包  Akka 和 Cassandra 使用了 Phi Accrual failure detector，根据响应时间的分布图，自动调整 timeout 时间，TCP 重传 timeout 也是类似的。\n不可靠的时钟 使用 Network Time Protocol (NTP) 可以同步时钟。\n单调与 Time-of-day 时钟 Java 的 System.currentTimeMillis()，返回自 epoch 以来的毫秒数，Time-of-day 时钟经常与 NTP 同步，所以有时候可能会向前或向后跳，以矫正时间。\n单调时钟\n适合测量一个 duration，例如 Java 的 System.nanoTime()，这个方法保证它总是向前走的。调用两次这个函数，得到的差值就是这一段 duration。\n时钟同步 \u0026amp; 精确 我们获取时间的函数返回的也不精确：\n 时钟 drift 受到计算机的温度影响 一台电脑的时钟与 NTP 相差太多，就会拒绝同步 NTP 同步也会收到网络拥塞影响 部分 NTP 服务器本身可能配置错误 VM 中，时钟是虚拟的  依赖同步的时钟 软件需要应对时钟的不精确。依赖 Time-of-day 时钟，会出现各种问题：\n对于时序性的事件，逻辑时钟或许是更好的选择。\n时钟有置信区间\n对于 NTP Server，或许 10 毫秒的精度算比较好了，网络拥塞的时候，还有可能超过 100 毫秒。所以不要再把一个 Clock 想象为一个时间点了，它更像一个拥有置信区间的时间段：一个系统可能有 95% 的信心认为现在的时间是 10.3 到 10.5秒。在 Google 的 Spanner 中的 TrueTime API，当获取本地时间的时候，你获取到两个值 [earliest, latest]，代表着最早可能是哪个时间戳，最晚可能是哪个时间戳。\n针对全局 snapshot 的时钟\n数据库的 snapshot 隔离，需要一个递增的事务 ID，当放到多台机器上的时候，一个全局的递增的事务 ID 该如何生成？Spanner 采用如下方式在跨数据中心中实现 snapshot 隔离。它采用了 TrueTime API，如果有两个置信区间， (A = [Aearliest, Alatest] and B = [Bearliest, Blatest])，这两个区间没有重叠，那么 B 肯定是后发生于 A 的，只有重叠的时候才无法判断谁先发生。\n为了确保事务时间戳反映因果关系，在提交读写事务之前，Spanner 故意等待置信区间的长度。通过这样做，它可以确保任何可能读取数据的事务都在一个足够晚的时间，因此它们的置信区间不会重叠。为了使等待时间尽可能短，Spanner 需要使时钟的不确定性尽可能小；为此，谷歌在每个数据中心都部署了 GPS 接收器或原子钟，允许时钟在大约 7 毫秒内同步。\n进程暂停 一个节点如何知道自己仍然是 leader，并可以接受写请求呢？\n一种方式是 leader 可以从其他节点获取一个 lease，相当于一个带有 timeout 的 lock，同一时刻只有一个节点可以获取 lease，所以 leader 需要不停的去获取 lese：\nwhile (true) { request = getIncomingRequest(); // Ensure that the lease always has at least 10 seconds remaining \tif (lease.expiryTimeMillis - System.currentTimeMillis() \u0026lt; 10000) { lease = lease.renew(); } if (lease.isValid()) { process(request); } } 上述代码有什么问题：\n 依赖同步的时钟，expiry 时间是由不同机器设置的 (当前时间 + 30 秒之类的算法)，然后与本地时钟进行对比，如果时钟不同步，那么会发生奇怪的行为。 即使采用本地时钟也不可行，假如 lease.isValid() 耗费 15 秒，会怎样？它直到下一次循环才知道自己获取到了超时的 lease，而这期间可能已经做了某些不正确的事情。  代码可能会暂停如此之久吗？\n GC 的 stop-the-world VM 可能会被挂起 手机上运行的程序切到后台，可能会被暂停 执行 I/O 的时候 系统执行 paging 的时候，页 swapping 到磁盘，鉴于此，服务器通常会禁掉 paging 发送 SIGSTOP 信号给 LINUX 程序  Knowledge, Truth, and Lies The Truth Is Defined by the Majority 节点不一定必须相信自己的判断，分布式系统不能仅依赖于单个节点，因为一个节点可能随时发生故障，从而可能使系统卡住并且无法恢复。取而代之的是，许多分布式算法都依赖仲裁，即节点之间的投票（请参阅第179页的“读写仲裁”）：决策需要来自多个节点的最小投票数，以减少对任何一个节点的依赖。\nThe leader and the lock\n一个系统，可能经常面临如下问题：\n 只能有一个 leader，以防止脑裂 只能有一个事务获取锁 只有一个用户注册成功某个唯一的用户昵称  在分布式系统中实现此功能需要谨慎：即使节点认为它是“选定的对象”（分区的负责人，锁的持有人，成功获取用户名的用户的请求处理程序），也不会这样做。 不一定意味着大多数的节点同意！ 一个节点以前可能是领导者，但是如果其他节点在此期间宣布其死亡（例如，由于网络中断或GC暂停），则可能已降级，并且可能已经选择了另一个领导者。\n如果一个节点继续充当所选节点，即使大多数节点都宣布它已死亡，也可能在未精心设计的系统中引起问题。 这样的节点可以以自己指定的能力将消息发送到其他节点，如果其他节点相信，则整个系统可能会做一些不正确的事情。\n例如，图8-4显示了由于不正确的锁定实现而导致的数据损坏错误。 （该错误不是理论上的：HBase曾经存在此问题[74，75]。）假设您要确保一次只能由一个客户端访问存储服务中的文件，因为如果多个客户端尝试写入到它，该文件将被损坏。您尝试通过要求客户端在访问文件之前从锁定服务获得租约来实现此目的。\nfencing token\n当使用锁或租约来保护对某些资源的访问时，例如图8-4中的文件存储，我们需要确保错误地认为自己是“所选对象”的节点不会破坏系统的其余部分。实现此目标的一种相当简单的技术称为 fencing，如图8-5所示。\n假设锁服务器每次授予锁或租用权时，也会返回一个防护令牌，该令牌的数量在每次授予锁时都会增加（例如，由锁服务递增）。 然后，我们可以要求客户端每次向存储服务发送写请求时，都必须包括其当前的隔离令牌。\n如果 ZooKeeper 用于提供锁服务，那么它的事务 ID zxid 或者节点的版本 cversion 可以用作 fencing token，因为它们都是单调递增的。\n请注意，此机制要求资源本身在检查令牌时发挥积极作用，拒绝使用比已处理的令牌旧的任何写入-仅依靠客户端自己检查其锁状态是不够的。对于不显式支持围栏令牌的资源，您可能仍然可以绕过该限制（例如，对于文件存储服务，您可以在文件名中包含围栏令牌）。但是，为了避免在锁的保护之外处理请求，需要进行某种检查。\n拜占庭 Faults 拜占庭 Fault：一个节点说谎。在这种不真实的环境下达成的协议，称之为拜占庭将军问题。即使某些节点出现故障且未遵守协议，或者恶意攻击者正在干扰网络，该系统仍可继续正常运行，则该系统具有拜占庭容错功能。 在某些特定情况下，这种关注是相关的。 例如：\n 在航空航天环境中，计算机内存或CPU寄存器中的数据可能会因辐射而损坏，从而导致其以任意无法预测的方式对其他节点做出响应。 由于系统故障非常昂贵（例如，飞机坠毁并杀死机上所有人，或者火箭与国际空间站相撞），因此飞行控制系统必须容忍拜占庭故障 在具有多个参与组织的系统中，一些参与者可能试图欺骗或欺骗其他人。 在这种情况下，一个节点仅信任另一个节点的消息是不安全的，因为它们可能是出于恶意目的发送的。 例如，点对点网络（如比特币和其他区块链）可以被认为是一种使相互不信任的各方就交易是否发生达成一致而无需依赖中央机构的一种方式。  能抵抗拜占庭 Fault 的算法也比较复杂，一般系统不用考虑，不过有一些简单的步骤可以采用：\n 网络数据包可能由于硬件问题损坏，App 级别的校验和可以防止这种损坏。 Web 需要处理好用户的输入、SQL 攻击、限制字符串大小等。 NTP 可以配置多个服务器  系统模型与 Reality 分布式系统模型都有一些假设：\n 同步模型假定网络延迟、系统暂停执行时间、时钟漂移不会超过某个过分的上限。 并行同步模型，像同步模型一样工作，只不过可以超出某个上限。 异步模型，不作任何时间假设  Node 失败：\n Crash-stop faults: Node 只有一种方式 fail，那就是 crashing，死掉之后就不会再回来 Crash-recovery faults: Node 死掉之后可能过会儿还会继续响应 拜占庭 faults: Node 的行为不受控制了，可以欺骗其他 Node  "});index.add({'id':223,'href':'/docs/programmer-interview/java/treemap/','title':"TreeMap",'content':"TreeMap 如何保证有序 只需要看 put 方法的部分细节即可：\nComparator\u0026lt;? super K\u0026gt; cpr = comparator; if (cpr != null) { do { parent = t; cmp = cpr.compare(key, t.key); if (cmp \u0026lt; 0) t = t.left; else if (cmp \u0026gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } "});index.add({'id':224,'href':'/docs/programmer-interview/front-end/clear-float/','title':"清除浮动",'content':"清除浮动 clear:both 给浮动元素后面的元素添加 clear: both 属性。\n.element { clear: both; } 空 DIV \u0026lt;div style=\u0026#34;clear: both;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; overflow：触发 BFC 给浮动元素的容器添加 overflow:hidden; 或 overflow:auto; 可以清除浮动，另外在 IE6 中还需要触发 hasLayout ，例如为父元素设置容器宽高或设置 zoom:1。\n:after 添加一个 :after 伪元素来清除浮动。\n.clearfix:after { content: \u0026#34;.\u0026#34;; visibility: hidden; display: block; height: 0; clear: both; } 参考  All About Floats CSS - 清除浮动  "});index.add({'id':225,'href':'/docs/books/ddia/ddia-chapter9/','title':"设计数据密集型应用程序 - 一致性和共识",'content':"设计数据密集型应用程序 - 一致性和共识  本章我们看下场景的算法或协议是如何构建 fault-tolerant 分布式系统的。\n 一致性保证 多数 replicated 数据库提供了 最终一致性，但是并未强调需要等待多久才会达成一致。\n线性化 线性化系统，一个 Client 写入，其他 Client 立即可以读取到最新的值。图 9-1 展示的是非线性的系统：\n什么让那个系统线性化？ 读请求和写请求同时发出，那么返回的可能是最新值也可能是旧值，如图 9-2 所示，x 可以是寄存器里面的值，也可以是 key-value store 里面的某个键，也可以是关系型数据库的某一行，文档数据库的某一个文档：\n如何使其线性化？必须添加限制：某个 Client 读取返回 1 的时候，随后的读取也必须都返回 1，即使 write 操作还没有完成：\n我们可以进一步精简，如图 9-4:\n图 9-4 ，我们在 read 和 write 之外增加了新的操作：cas(x, v_old, v_new) =\u0026gt; r。记录下所有请求和响应，看是否位于一个合法的顺序的线上面，可以检测是否是线性化的。\n依赖线性化 哪些系统必须依赖线性化？\n single-leader 依靠锁来选举 leader，这个锁的实现必须是线性化的。 数据库中某条记录是唯一的，必须依赖线性化。 跨通道时序依赖，message queue 必须快于 storage service，否则可能看到的是旧的图片、或看不到图片，因为使用的是两个 Channel。  实现线性化系统 线性化：对外表现就好像只有一份数据，并且所有操作都是原子的。使一个系统 fault-tolerant 的最常见的方式是 replication：\n Single-leader replication: 如果所有 read 都从 leader 或者已经追上 leader 的 follower 读取的话，那么他们自然是线性化的。 一致性算法：一致性协议提供了防止脑裂和陈旧的副本的方法，这正是 ZooKeeper 和 etcd 所做的。 Multi-leader replication: 肯定是非线性化的，因为数据会被异步的同步到其他节点上。 Leaderless replication：大概率也是非线性  线性化与法定人数 (w + r \u0026gt; n)\n线性化的代价 CAP 理论\n任何线性化数据库都有这个问题，不管如何实现：\n 如果你的系统需要线性化，出现网络错误，必须返回错误，或者等待网络自动修复 无需线性化，那么可以继续处理，系统仍然可用，但不是线性化的，所以此种系统更能忍受网络的问题，这就是证明的 CAP 理论。   CAP 更好的叫法：Consistent or Available when Partitioned\n 线性化与网络延迟\n许多系统都不提供线性化，因为它们主要为了增加性能，而非 fault-tolerance。\n顺序保证 分布式事务和共识 "});index.add({'id':226,'href':'/docs/programmer-interview/front-end/css-triangle/','title':"CSS 画三角形",'content':"CSS 画三角形 上三角形 #triangle-up { width: 0; height: 0; border-left: 50px solid transparent; border-right: 50px solid transparent; border-bottom: 100px solid red; } 下三角形 #triangle-down { width: 0; height: 0; border-left: 50px solid transparent; border-right: 50px solid transparent; border-top: 100px solid red; } 左三角形 #triangle-left { width: 0; height: 0; border-top: 50px solid transparent; border-right: 100px solid red; border-bottom: 50px solid transparent; } 右三角形 #triangle-right { width: 0; height: 0; border-top: 50px solid transparent; border-left: 100px solid red; border-bottom: 50px solid transparent; } 左上朝向的直角三角形 #triangle-topleft { width: 0; height: 0; border-top: 100px solid red; border-right: 100px solid transparent; } 右上朝向的直角三角形 #triangle-topright { width: 0; height: 0; border-top: 100px solid red; border-left: 100px solid transparent; } 左下朝向的直角三角形 #triangle-bottomleft { width: 0; height: 0; border-bottom: 100px solid red; border-right: 100px solid transparent; } 右下朝向的直角三角形 #triangle-bottomright { width: 0; height: 0; border-bottom: 100px solid red; border-left: 100px solid transparent; } 参考  The Shape of CSS  "});index.add({'id':227,'href':'/docs/programmer-interview/front-end/css-selector/','title':"CSS 选择器",'content':"CSS 选择器 选择器类型    选择器 示例     ID #id   class .class   标签 p   通用 *   属性 [type=\u0026quot;text\u0026quot;]   伪类 :hover   伪元素 ::first-line   子选择器、相邻选择器     权重优先级 important \u0026gt; 内嵌样式 \u0026gt; ID \u0026gt; 类 \u0026gt; 标签 | 伪类 | 属性选择 \u0026gt; 伪元素 \u0026gt; 继承 \u0026gt; 通配符\n权重赋值  内联样式如 style=XXX：1000 ID 选择器：100 类、伪类、属性选择器：10 标签、伪元素选择器：1 通配符、子选择器、相邻选择器等：0  比较规则：逐级比较。权重一样的 CSS 表达式，在样式表中后声明的优先。\n"});index.add({'id':228,'href':'/docs/programmer-interview/front-end/fixed-side-responsive-middle/','title':"CSS 两边固定中间自适应布局",'content':"CSS 两边固定中间自适应布局 中间 float BFC 布局 \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;浮动布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container \u0026gt; div{ height: 200px; } .left { float: left; width: 300px; background: red; } .right { float: right; width: 300px; background: blue; } .center { overflow: hidden; background: yellow; }  缺点：中间浮动布局的内容最后才加载\n 圣杯布局  父元素：需要内边距 三个孩子全部是：float: left 左右分别是相对布局  \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;圣杯布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container { padding: 0 300px; } .container \u0026gt; div { height: 200px; } .center { float: left; width: 100%; background: yellow; } .left { float: left; width: 300px; background: red; margin-left: -100%; position: relative; left: -300px; } .right { float: left; width: 300px; background: blue; margin-left: -300px; position: relative; right: -300px; } 双飞翼布局 \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;center-container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;双飞翼布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container div { height: 200px; } .center-container { float: left; width: 100%; height: 100px; background: yellow; } .center { margin: 0 300px; } .left { background: red; float: left; width: 300px; margin-left: -100%; } .right { background: blue; float: left; width: 300px; margin-left: -300px; } flex \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;flexbox\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container { display: flex; } .container \u0026gt; div { height: 200px; } .left { width: 300px; background: red; } .center { flex: 1; background: yellow; } .right { width: 300px; background: blue; } table \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;表格布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container { width: 100%; display: table; height: 200px; } .container \u0026gt; div { display: table-cell; } .left { width: 300px; background: red; } .center { background: yellow; } .right { width: 300px; background: blue; } 绝对定位 \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;绝对定位布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container \u0026gt; div { position: absolute; height: 200px; } .left { left: 0; width: 300px; background: red; } .center { left: 300px; right: 300px; background: yellow; } .right { right: 0; width: 300px; background: blue; } Grid \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;网格布局\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .container { display: grid; width: 100%; grid-template-rows: 200px; grid-template-columns: 300px auto 300px; } .left { background: red; } .center { background: yellow; } .right { background: blue; } 选哪个  浮动布局：middle 最后才能加载出来 圣杯：先加载 middle，但是响应效果差 双飞翼：比圣杯简洁、影响性好。  圣杯布局和双飞翼布局都是在浮动布局时代的比较经典的布局方式，对旧的浏览器有很好的兼容性。不过事实上，现代浏览器已经大量普及，需要适配旧浏览器的场景已经开始变少，加上移动端开发越来越盛行，于是有了比较新的布局方式。\n flex：自适应布局最佳方案 table：缺点不灵活 绝对定位：脱离文档流、可维护性差 Grid：浏览器兼容性  参考  css两边固定中间自适应布局  "});index.add({'id':229,'href':'/docs/programmer-interview/front-end/css3-new-feature/','title':"CSS3 新特性",'content':"CSS3 新特性 新特性概览    新特性 说明     新的选择器    阴影 Box 阴影、文本阴影   圆角    渐变    透明度    Transitions    Transformations 旋转、缩放、扭曲、平移等   动画    多列布局    Flexbox 构建 Flex 布局   Grids 构建二维布局   @font face 嵌入更多字体   @media 响应式设计    阴影：box-shadow 圆角：border-radius .class { border-radius: 20px; } 渐变 透明度 .img { opacity: 0.8 } 渐变：Transitions  Transitions: 元素的某个属性发生渐变。\n .class { triansition-duration: 1s transition-delay: 500ms transition-property: color transition-timing-function: ease-out } 示例，鼠标移动到 btn2 按钮上的时候，1秒之内按钮颜色渐变为红色：\n.btn2{ transition-duration:1s} .btn2:hover{ background:red} 参考  CSS3 Tutorial  "});index.add({'id':230,'href':'/docs/programmer-interview/front-end/es6-new-feature/','title':"ES6 新特性",'content':"ES6 新特性 var =\u0026gt; let/const var 是 function-scoped 的 var x = 3; function func(randomize) { if (randomize) { var x = Math.random(); // (A) scope: whole function  return x; } return x; // accesses the x from line A } func(false); // undefined   let/const 是 block-scoped 的 let x = 3; function func(randomize) { if (randomize) { let x = Math.random(); return x; } return x; } func(false); // 3    IIFE =\u0026gt; block ES5 使用 IIFE 限制 tmp 作用域 (function () { // open IIFE  var tmp = ···; ··· }()); // close IIFE  console.log(tmp); // ReferenceError   ES6 使用 let { // open block  let tmp = ···; ··· } // close block  console.log(tmp); // ReferenceError    字符串连接 =\u0026gt; 模板变量 ES5 使用 + 号连接字符串 function printCoord(x, y) { console.log(\u0026#39;(\u0026#39;+x+\u0026#39;, \u0026#39;+y+\u0026#39;)\u0026#39;); }   ES6 使用 ${} 解释字符串 function printCoord(x, y) { console.log(`(${x}, ${y})`); }    多行字符串 ES5 多行字符串 var HTML5_SKELETON = \u0026#39;\u0026lt;!doctype html\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;html\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;head\u0026gt;\\n\u0026#39; + \u0026#39; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt;\\n\u0026#39; + \u0026#39; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;/head\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;body\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;/body\u0026gt;\\n\u0026#39; + \u0026#39;\u0026lt;/html\u0026gt;\\n\u0026#39;;   ES6 多行字符串 const HTML5_SKELETON = ` \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;`;    function =\u0026gt; 箭头函数 ES5 普通函数 var arr = [1, 2, 3]; var squares = arr.map(function (x) { return x * x });   ES6 引入箭头函数 const arr = [1, 2, 3]; const squares = arr.map(x =\u0026gt; x * x);    多个返回值 for =\u0026gt; forEach =\u0026gt; for-of ES5 之前的 for 循环 var arr = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]; for (var i=0; i\u0026lt;arr.length; i++) { var elem = arr[i]; console.log(elem); }   ES5 的 forEach arr.forEach(function (elem) { console.log(elem); });   ES6 的 for-of const arr = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]; for (const elem of arr) { console.log(elem); }    参数默认值 ES5 给参数设置默认值 function foo(x, y) { x = x || 0; y = y || 0; // ··· }   ES6 的参数默认值 function foo(x=0, y=0) { // ... }    命名参数 ES5 多个参数封装为 Object function selectEntries(options) { var start = options.start || 0; var end = options.end || -1; var step = options.step || 1; // ··· }   ES6 精简了 Object 拆封 function selectEntries({ start=0, end=-1, step=1 }) { // ··· }    任意个数的参数 ES5 接受多个参数 function logAllArguments() { for (var i=0; i \u0026lt; arguments.length; i++) { console.log(arguments[i]); } }   ES6 接受多个参数 function logAllArguments(...args) { for (const arg of args) { console.log(arg); } }    apply =\u0026gt; (\u0026hellip;) Math.max() ES5 \u0026gt; Math.max.apply(Math, [-1, 5, 11, 3]) 11   ES6 \u0026gt; Math.max(...[-1, 5, 11, 3]) 11    Array.prototype.push() ES5 var arr1 = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]; var arr2 = [\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]; arr1.push.apply(arr1, arr2); // arr1 is now [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]   ES6 const arr1 = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]; const arr2 = [\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]; arr1.push(...arr2); // arr1 is now [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]    concat() =\u0026gt; (\u0026hellip;) ES5 ar arr1 = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]; var arr2 = [\u0026#39;c\u0026#39;]; var arr3 = [\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;]; console.log(arr1.concat(arr2, arr3)); // [ \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39; ]   ES6 const arr1 = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]; const arr2 = [\u0026#39;c\u0026#39;]; const arr3 = [\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;]; console.log([...arr1, ...arr2, ...arr3]); // [ \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39; ]    object 常量中的 function ES5 var obj = { foo: function () { // ···  }, bar: function () { this.foo(); }, // trailing comma is legal in ES5 }   ES6 const obj = { foo() { // ···  }, bar() { this.foo(); }, }    构造器 =\u0026gt; class 基类 ES5 实现自己的构造器 function Person(name) { this.name = name; } Person.prototype.describe = function () { return \u0026#39;Person called \u0026#39;+this.name; };   ES6 使用 constructor 构造器 class Person { constructor(name) { this.name = name; } describe() { return \u0026#39;Person called \u0026#39;+this.name; } }    继承 ES5 模拟继承 function Employee(name, title) { Person.call(this, name); // super(name)  this.title = title; } Employee.prototype = Object.create(Person.prototype); Employee.prototype.constructor = Employee; Employee.prototype.describe = function () { return Person.prototype.describe.call(this) // super.describe()  + \u0026#39; (\u0026#39; + this.title + \u0026#39;)\u0026#39;; };   ES6 使用 extends 继承 class Employee extends Person { constructor(name, title) { super(name); this.title = title; } describe() { return super.describe() + \u0026#39; (\u0026#39; + this.title + \u0026#39;)\u0026#39;; } }    自定义 error 构造器 =\u0026gt; 继承 Error ES5 实现自己的 Error 构造器 function MyError() { // Use Error as a function  var superInstance = Error.apply(null, arguments); copyOwnPropertiesFrom(this, superInstance); } MyError.prototype = Object.create(Error.prototype); MyError.prototype.constructor = MyError; function copyOwnPropertiesFrom(target, source) { Object.getOwnPropertyNames(source) .forEach(function(propKey) { var desc = Object.getOwnPropertyDescriptor(source, propKey); Object.defineProperty(target, propKey, desc); }); return target; };   ES6 继承 Error class MyError extends Error { }    object 用作 map ES5 使用 object 作为 map var dict = Object.create(null); function countWords(word) { var escapedWord = escapeKey(word); if (escapedWord in dict) { dict[escapedWord]++; } else { dict[escapedWord] = 1; } } function escapeKey(key) { if (key.indexOf(\u0026#39;__proto__\u0026#39;) === 0) { return key+\u0026#39;%\u0026#39;; } else { return key; } }   ES6 引入 Map 对象 const map = new Map(); function countWords(word) { const count = map.get(word) || 0; map.set(word, count + 1); }    string 新的方法  startsWith endsWith includes repeat  Array 新的方法  findIndex Array.from(arguments)：将 Array-like 对象转为 Array arr4 = new Array(2).fill('x')  CommonJS =\u0026gt; ES6 模块 多个 exports CommonJS //------ lib.js ------ var sqrt = Math.sqrt; function square(x) { return x * x; } function diag(x, y) { return sqrt(square(x) + square(y)); } module.exports = { sqrt: sqrt, square: square, diag: diag, }; //------ main1.js ------ var square = require(\u0026#39;lib\u0026#39;).square; var diag = require(\u0026#39;lib\u0026#39;).diag; console.log(square(11)); // 121 console.log(diag(4, 3)); // 5   ES6 //------ lib.js ------ export const sqrt = Math.sqrt; export function square(x) { return x * x; } export function diag(x, y) { return sqrt(square(x) + square(y)); } //------ main1.js ------ import { square, diag } from \u0026#39;lib\u0026#39;; console.log(square(11)); // 121 console.log(diag(4, 3)); // 5    单个 exports CommonJS //------ myFunc.js ------ module.exports = function () { /** ··· */ }; //------ main1.js ------ var myFunc = require(\u0026#39;myFunc\u0026#39;); myFunc();   ES6 //------ myFunc.js ------ export default function () { /** ··· */ } // no semicolon!  //------ main1.js ------ import myFunc from \u0026#39;myFunc\u0026#39;; myFunc();    Map、Set、WeakMap、WeakSet // Sets var s = new Set(); s.add(\u0026#34;hello\u0026#34;).add(\u0026#34;goodbye\u0026#34;).add(\u0026#34;hello\u0026#34;); s.size === 2; s.has(\u0026#34;hello\u0026#34;) === true; // Maps var m = new Map(); m.set(\u0026#34;hello\u0026#34;, 42); m.set(s, 34); m.get(s) == 34; // Weak Maps var wm = new WeakMap(); wm.set(s, { extra: 42 }); wm.size === undefined // Weak Sets var ws = new WeakSet(); ws.add({ data: 42 }); // Because the added object has no other references, it will not be held in the set Promise  Promise 用于异步编程。\n function timeout(duration = 0) { return new Promise((resolve, reject) =\u0026gt; { setTimeout(resolve, duration); }) } var p = timeout(1000).then(() =\u0026gt; { return timeout(2000); }).then(() =\u0026gt; { throw new Error(\u0026#34;hmm\u0026#34;); }).catch(err =\u0026gt; { return Promise.all([timeout(100), timeout(200)]); }) Symbol  Symbol 可以用来访问对象的内部状态。\n var MyClass = (function() { // module scoped symbol  var key = Symbol(\u0026#34;key\u0026#34;); function MyClass(privateData) { this[key] = privateData; } MyClass.prototype = { doStuff: function() { ... this[key] ... } }; return MyClass; })(); var c = new MyClass(\u0026#34;hello\u0026#34;) c[\u0026#34;key\u0026#34;] === undefined Proxy  Proxy：用于拦截、丢向虚拟化、日志记录/分析等\n // Proxying a normal object var target = {}; var handler = { get: function (receiver, name) { return `Hello, ${name}!`; } }; var p = new Proxy(target, handler); p.world === \u0026#39;Hello, world!\u0026#39;; // Proxying a function object var target = function () { return \u0026#39;I am the target\u0026#39;; }; var handler = { apply: function (receiver, ...args) { return \u0026#39;I am the proxy\u0026#39;; } }; var p = new Proxy(target, handler); p() === \u0026#39;I am the proxy\u0026#39;; Unicode 码位放入 {} 就可以正确解读该字符：\n// same as ES5.1 \u0026#34;𠮷\u0026#34;.length == 2 // new RegExp behaviour, opt-in ‘u’ \u0026#34;𠮷\u0026#34;.match(/./u)[0].length == 2 // new form \u0026#34;\\u{20BB7}\u0026#34;==\u0026#34;𠮷\u0026#34;==\u0026#34;\\uD842\\uDFB7\u0026#34; // new String ops \u0026#34;𠮷\u0026#34;.codePointAt(0) == 0x20BB7 // for-of iterates code points for(var c of \u0026#34;𠮷\u0026#34;) { console.log(c); } generator function* 返回的是一个 Generator 对象：\nfunction* generator(i) { yield i; yield i + 10; } const gen = generator(10); console.log(gen.next().value); // 10 console.log(gen.next().value); // 20 参考  4. Core ES6 features ECMAScript 6  "});index.add({'id':231,'href':'/docs/programmer-interview/front-end/display/','title':"display",'content':"display 常设置的值 div { display: inline; display: inline-block; display: block; display: run-in; display: none; } Flexbox .header { display: flex; } Flow-Root  flow-root 创建了一个新的 BFC 。\n .group { display: flow-root; } Grid body { display: grid; } Table div { display: table; display: table-cell; display: table-column; display: table-colgroup; display: table-header-group; display: table-row-group; display: table-footer-group; display: table-row; display: table-caption; } "});index.add({'id':232,'href':'/docs/programmer-interview/front-end/es2020/','title':"ES2020 新特性",'content':"ES2020 新特性 BigInt JavaScript 最大数值 Number.MAX_SAFE_INTEGER，那么如何表达比这个数值更大的数字呢？\n// 注意后缀有 n：表示 large number const bigNum = 10000000000000n; console.log(bigNum * 2n); 动态 import const doMath = async (num1, num2) =\u0026gt; { if (num1 \u0026amp;\u0026amp; num2) { const math = await import(\u0026#39;./math.js\u0026#39;) console.log(math.add(5, 10)) } } doMath(4, 2) Nullish Coalescing let person = { profile: { name: \u0026#34;\u0026#34;, age: 0 } }; // 默认值 console.log(person.profile.name || \u0026#34;Anonymous\u0026#34;) console.log(person.profile.age || 18) 现在可以不使用 ||，而用 ?? 来指定默认值了：\nconsole.log(person.profile.name ?? \u0026#34;Anonymous\u0026#34;) console.log(person.profile.age ?? 18) 更多示例：\nOptional Chaining person 是 undefined，还是 profile 是 undefined，还是 name 是 undefined 的时候，才使用 Anonymous 这个默认值？\nlet person = {}; console.log(person.profile.name ?? \u0026#34;Anonymous\u0026#34;) console.log(person?.profile?.name ?? \u0026#34;Anonymous\u0026#34;) console.log(person?.profile?.age ?? 18) 类私有变量 class Message { #message = \u0026#34;Hello\u0026#34; sayHi() { console.log(this.#message) } } Promise.allSettled const p1 = new Promise((res, rej) =\u0026gt; setTimeout(res, 1000)); const p2 = new Promise((res, rej) =\u0026gt; setTimeout(rej, 1000)); Promise.allSettled([p1, p2]).then(data =\u0026gt; console.log(data)); String.matchAll 匹配字符串，并且打印出每个匹配上的子串的 index：\nconst string = \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39;; const regex = /\\b\\p{ASCII_Hex_Digit}+\\b/gu; let match; while (match = regex.exec(string)) { console.log(match); } // Output: // // [ \u0026#39;DEADBEEF\u0026#39;, index: 19, input: \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39; ] // [ \u0026#39;CAFE\u0026#39;, index: 28, input: \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39; ] 现在直接这样写，即可：\nconst string = \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39;; const regex = /\\b\\p{ASCII_Hex_Digit}+\\b/gu; for (const match of string.matchAll(regex)) { console.log(match); } // Output: // // [ \u0026#39;DEADBEEF\u0026#39;, index: 19, input: \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39; ] // [ \u0026#39;CAFE\u0026#39;, index: 28, input: \u0026#39;Magic hex numbers: DEADBEEF CAFE\u0026#39; ] globalThis JavaScript 的全局 this 可能是 window，可能是 Node.js 的 global，也可能是 self，想要获取全局 this 需要经过逐个 if 条件去判断才行：\nif (typeof globalThis !== \u0026#39;undefined\u0026#39;) return globalThis; if (typeof self !== \u0026#39;undefined\u0026#39;) return self; if (typeof window !== \u0026#39;undefined\u0026#39;) return window; if (typeof global !== \u0026#39;undefined\u0026#39;) return global; 现在引入 globalThis 这个变量，可以直接返回全局 this：\nconst theGlobalThis = globalThis; 模块命名空间导出 以前，JavaScript 仅支持如下 import 写法：\nimport * as utils from \u0026#39;./utils.js\u0026#39;; 现在 export 也可以这样写了：\nexport * as utils from \u0026#39;./utils.js\u0026#39;; 更为明确的 for-in 顺序 for (x in y) 以怎样的顺序遍历，将会在 ES2020 中以官方指定的标准化的形式得到统一。\nimport.meta 考虑如下 module：\n\u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;module.js\u0026#34;\u0026gt; \u0026lt;/script\u0026gt; 你可以通过 import.meta 来获取到它的 meta 信息：\nconsole.log(import.meta); // { url: \u0026#34;file:///home/user/module.js\u0026#34; } 参考  10 New JavaScript Features in ES2020 That You Should Know Features tagged \u0026ldquo;ES2020\u0026rdquo;  "});index.add({'id':233,'href':'/docs/programmer-interview/front-end/create-object/','title':"创建对象",'content':"创建对象 new Object() var d = new Object(); Object.create() var a = Object.create(null); 内部原理 Object.create = function (proto, propertiesObject) { if (typeof proto !== \u0026#39;object\u0026#39; \u0026amp;\u0026amp; typeof proto !== \u0026#39;function\u0026#39;) { throw new TypeError(\u0026#39;Object prototype may only be an Object: \u0026#39; + proto); } else if (proto === null) { throw new Error(\u0026#34;This browser\u0026#39;s implementation of Object.create is a shim and doesn\u0026#39;t support \u0026#39;null\u0026#39; as the first argument.\u0026#34;); } if (typeof propertiesObject != \u0026#39;undefined\u0026#39;) { throw new Error(\u0026#34;This browser\u0026#39;s implementation of Object.create is a shim and doesn\u0026#39;t support a second argument.\u0026#34;); } function F() {} F.prototype = proto; return new F(); }; {} var b = {}; Object.create(null) vs {} 默认情况下，JavaScript 的对象继承自 Object，所以 {}.constructor.prototype == Object.prototype。但是如果你主动指定了 null 作为 prototype（Object.create(null)），那么这个对象不会继承任何属性，也就是没有 __proto__ 这个属性。\n{} 和 Object.create(Object.prototype) 是等价的。\nfunction 构造器 var Obj = function(name) { this.name = name } var c = new Obj(\u0026#34;hello\u0026#34;); function 构造器 + prototype function myObj(){}; myObj.prototype.name = \u0026#34;hello\u0026#34;; var k = new myObj(); ES6 class class myObject { constructor(name) { this.name = name; } } var e = new myObject(\u0026#34;hello\u0026#34;); 单例 var l = new function(){ this.name = \u0026#34;hello\u0026#34;; } 参考  Different ways of creating an Object in javascript Creating JS object with Object.create(null)? Object.create  "});index.add({'id':234,'href':'/docs/programmer-interview/front-end/object-freeze/','title':"Object.freeze()",'content':"Object.freeze() 作用  可以冻结一个对象。一个被冻结的对象再也不能被修改；冻结了一个对象则不能向这个对象添加新的属性，不能删除已有属性，不能修改该对象已有属性的可枚举性、可配置性、可写性，以及不能修改已有属性的值。此外，冻结一个对象后该对象的原型也不能被修改。\n 行为 Object.freeze() 是浅冻结，浅不可变。\nvar ob1 = { foo : 1, bar : { value : 2 } }; Object.freeze( ob1 ); const ob2 = { foo : 1, bar : { value : 2 } } ob1.foo = 4; // (frozen) ob1.foo not modified ob2.foo = 4; // (const) ob2.foo modified  ob1.bar.value = 4; // (frozen) modified, because ob1.bar is nested ob2.bar.value = 4; // (const) modified  ob1.bar = 4; // (frozen) not modified, bar is a key of obj1 ob2.bar = 4; // (const) modified  ob1 = {}; // (frozen) ob1 redeclared ob2 = {}; // (const) ob2 not redeclared 内部原理  Object.definedProperty() 方法可以定义对象的属性的特性。如可不可以删除、可不可以修改等 Object.seal() 方法可以让对象不能被扩展、删除属性等  function myFreeze(obj) { if (obj instanceof Object) { Object.seal(obj); let p; for (p in obj) { if (obj.hasOwnProperty(p)) { Object.defineProperty(obj, p, { writable: false }); // myFreeze(obj[p]);// 递归，实现更深层次的冻结  } } } } 参考  Object.freeze() vs const JavaScript内置一些方法的实现原理\u0026ndash;Object.freeze()、instanceof  "});index.add({'id':235,'href':'/docs/programmer-interview/front-end/cookie-session/','title':"Cookie 和 Session",'content':"Cookie 和 Session Cookie 作用  登录、购物车、游戏分数 用户个性化信息、主题、其他设置 记录和分析用户行为  创建 Cookie 服务器在返回网页内容的时候，使用 Set-Cookie 头来创建 Cookie：\nHTTP/2.0 200 OK Content-Type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] Cookie 的有效期  Session：浏览器关闭当前 Tab 页就过期 Expires 和 Max-Age  Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Expires=\u0026lt;date\u0026gt; Set-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt;; Max-Age=\u0026lt;non-zero-digit\u0026gt; Cookie 访问权限 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2021 07:28:00 GMT; Secure; HttpOnly  Secure：只有 HTTPS 协议的时候，浏览器端的 Cookie 才会上传到 Server HttpOnly：声明这个的 Cookie，使用 Document.cokie 无法读和写  JavaScript 读写 Cookie document.cookie = \u0026#34;yummy_cookie=choco\u0026#34;; document.cookie = \u0026#34;tasty_cookie=strawberry\u0026#34;; console.log(document.cookie); Session Java 服务器端的 Session 对象 如下是在 Java 中 javax.servlet.http.HttpSession 接口中的两个方法：\npublic interface HttpSession { void setAttribute(String name, Object value); Object getAttribute(String key); } 在 Tomcat 中的 StandardSession 内部实现：\npublic class StandardSession implements HttpSession, Session, Serializable { protected ConcurrentHashMap\u0026lt;String, Object\u0026gt; attributes = new ConcurrentHashMap(); @Override public void setAttribute(String name, Object value) { this.attributes.put(name, value); } @Override public Object getAttribute(String name) { return this.attributes.get(name); } } 可见 Session 其实就是一个存储在服务器端的 Map ，它可以存储的对象为 Object。\nSession 默认有效期 在 Tomcat 中，Session 默认有效期是 30 秒：\n\u0026lt;session-config\u0026gt; \u0026lt;session-timeout\u0026gt;30\u0026lt;/session-timeout\u0026gt; \u0026lt;/session-config\u0026gt; Session vs Cookie  安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，需要将其转换成字符串，Session 可以存任意数据类型。 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，但是当访问量过多，会占用过多的服务器资源。  参考  傻傻分不清之 Cookie、Session、Token、JWT  "});index.add({'id':236,'href':'/docs/programmer-interview/front-end/call-apply-bind/','title':"call、apply、bind",'content':"call、apply、bind call func.call([thisArg[, arg1, arg2, ...argN]]) 底层原理 /** * 每个函数都可以调用call方法，来改变当前这个函数执行的this关键字，并且支持传入参数 */ Function.prototype.myCall = function(context) { //第一个参数为调用call方法的函数中的this指向  var context = context || global; //将this赋给context的fn属性  context.fn = this;//此处this是指调用myCall的function  var arr = []; for (var i=0,len=arguments.length;i\u0026lt;len;i++) { arr.push(\u0026#34;arguments[\u0026#34; + i + \u0026#34;]\u0026#34;); } //执行这个函数，并返回结果  var result = eval(\u0026#34;context.fn(\u0026#34; + arr.toString() + \u0026#34;)\u0026#34;); //将this指向销毁  delete context.fn; return result; } apply func.apply(thisArg, [ argsArray]) 底层原理 /** * apply函数传入的是this指向和参数数组 */ Function.prototype.myApply = function(context, arr) { var context = context || global; context.fn = this; var result; if (!arr) { result = context.fn(); //直接执行  } else { var args = []; for (var i=0,len=arr.length;i\u0026lt;len;i++) { args.push(\u0026#34;arr[\u0026#34; + i + \u0026#34;]\u0026#34;); } result = eval(\u0026#34;context.fn([\u0026#34; + args.toString() + \u0026#34;])\u0026#34;); } //将this指向销毁  delete context.fn; return result; } bind let boundFunc = func.bind(thisArg[, arg1[, arg2[, ...argN]]]) 底层原理 Function.prototype.myBind = function() { var _this = this; var context = [].shift.call(arguments);// 保存需要绑定的this上下文  var args = [].slice.call(arguments); //剩下参数转为数组  console.log(_this, context, args); return function() { return _this.apply(context, [].concat.call(args, [].slice.call(arguments))); } }; 区别 call 和 apply 它们两个是改变 this 的指向之后立即调用该函数，而 bind 则不同，它是创建一个新函数，我们必须手动去调用它。\n参考  bind、call、apply的区别与实现原理  "});index.add({'id':237,'href':'/docs/programmer-interview/front-end/css-square/','title':"画正方形",'content':"画正方形 实现一个正方形，拖拽窗口，正方形等比例缩放\nvw \u0026lt;div class=\u0026#34;square\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;This is a Square\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; .square { background: #000; width: 50vw; height: 50vw; } .square h1 { color: #fff; } padding-bottom \u0026lt;div style=\u0026#34;height:0;width:20%;padding-bottom:20%;background-color:red\u0026#34;\u0026gt; \u0026lt;div\u0026gt; Content goes here \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 参考  [How to style a div to be a responsive square?(https://stackoverflow.com/questions/19068070/how-to-style-a-div-to-be-a-responsive-square)  "});index.add({'id':238,'href':'/docs/programmer-interview/front-end/css-draw-circle/','title':"CSS 画圆",'content':"CSS 画圆 border-radius #circle { width: 200px; height: 200px; background: #f00; border-radius: 50%; } "});index.add({'id':239,'href':'/docs/programmer-interview/front-end/css-sector/','title':"CSS 画扇形",'content':"CSS 画扇形 #cone { width: 0; height: 0; border-left: 70px solid transparent; border-right: 70px solid transparent; border-top: 100px solid red; border-radius: 50%; } "});index.add({'id':240,'href':'/docs/programmer-interview/front-end/three-equal-layout/','title':"三栏等宽布局",'content':"三栏等宽布局 \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;111\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;222\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;333\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; float ul { width: 500px; background: #ccc; overflow: hidden; } li { list-style: none; width: 33.33%; float: left; background: red; } flex ul { width: 500px; background: #ccc; display: flex; } li { list-style: none; flex: 1; background: red; } "});index.add({'id':241,'href':'/docs/programmer-interview/front-end/css-draw-half-circle/','title':"CSS 画半圆",'content':"CSS 画半圆 $size: 45px; div { background: #9e978e; display: inline-block; margin: 0 1em 1em 0; } .top, .bottom { height: $size; width: $size * 2; } .right, .left { height: $size * 2; width: $size; } .top { border-top-left-radius: $size * 2; border-top-right-radius: $size * 2; } .right { border-bottom-right-radius: $size * 2; border-top-right-radius: $size * 2; } .bottom { border-bottom-left-radius: $size * 2; border-bottom-right-radius: $size * 2; } .left { border-bottom-left-radius: $size * 2; border-top-left-radius: $size * 2; } 效果：\n"});index.add({'id':242,'href':'/docs/programmer-interview/front-end/animation/','title':"动画",'content':"动画 CSS animation .element { animation: pulse 5s infinite; } @keyframes pulse { 0% { background-color: #001F3F; } 100% { background-color: #FF4136; } } 在 @keyframes 中，0% 代表动画的开始，100% 代表动画的结束。animation 可用的子属性：\n.element { animation-name: stretch; animation-duration: 1.5s; animation-timing-function: ease-out; animation-delay: 0s; animation-direction: alternate; animation-iteration-count: infinite; animation-fill-mode: none; animation-play-state: running; } CSS transition transition 控制的是从一种状态/阶段/样式，转变为另外一种状态/阶段/样式，animation 控制的是整个动画的每一帧。\n.example { transition: [transition-property] [transition-duration] [transition-timing-function] [transition-delay]; } 鼠标悬浮在 div 的时候，转变 background 和 padding 的状态：\ndiv { transition: all 0.5s ease; background: red; padding: 10px; } div:hover { background: green; padding: 20px; } 或者你也可以使用逗号单独指定每一个属性的动作：\ndiv { transition: background 0.2s ease, padding 0.8s linear; } 如何触发 Transition  使用伪类 :hover、:focus、:active 添加或移除 class  .button { background-color: #33ae74; transition: background-color 0.5s ease-out; } .button.is-active { background-color: #1ce; } const button = document.querySelector(\u0026#39;.button\u0026#39;) button.addEventListener(\u0026#39;click\u0026#39;, _ =\u0026gt; button.classList.toggle(\u0026#39;is-active\u0026#39;)) JavaScript 动画 setInterval 示例代码如下：\nlet start = Date.now(); // remember start time  let timer = setInterval(function() { // how much time passed from the start?  let timePassed = Date.now() - start; if (timePassed \u0026gt;= 2000) { clearInterval(timer); // finish the animation after 2 seconds  return; } // draw the animation at the moment timePassed  draw(timePassed); }, 20); // as timePassed goes from 0 to 2000 // left gets values from 0px to 400px function draw(timePassed) { train.style.left = timePassed / 5 + \u0026#39;px\u0026#39;; } requestAnimationFrame function animate({timing, draw, duration}) { let start = performance.now(); requestAnimationFrame(function animate(time) { // timeFraction goes from 0 to 1  let timeFraction = (time - start) / duration; if (timeFraction \u0026gt; 1) timeFraction = 1; // calculate the current animation state  let progress = timing(timeFraction) draw(progress); // draw it  if (timeFraction \u0026lt; 1) { requestAnimationFrame(animate); } }); } 参考  animation JavaScript animations  "});index.add({'id':243,'href':'/docs/programmer-interview/front-end/css-units/','title':"CSS 单位",'content':"CSS 单位 绝对单位    单位 描述     cm 厘米   mm 毫米   in 英寸   px 像素   pt points   pc picas (1pc = 12pt)    相对单位    单位 描述     em 相对于当前元素的 font-size，2em 意味着两倍 font-size 的大小   ex 相对于当前字体的 x-height   ch 相对于 \u0026lsquo;0\u0026rsquo; 的宽度   rem 相对于根元素的字体大小   vw 相当于 viewport 宽度的 1%   vh 相当于 viewport 高度的 1%   vmin 相当于 viewport 较短边的 1%   vmax 相当于 viewport 较长边的 1%   % 相对于父元素    参考  CSS Units  "});index.add({'id':244,'href':'/docs/programmer-interview/front-end/prototype/','title':"prototype、__proto__、[[prototype]]",'content':"prototype、proto、[[prototype]] prototype 谁创建的 当你创建一个 Function object 的时候，一个名字为 prototype 的属性也会自动创建，并附着在 function object 上。\nfunction Foo() { this.name = \u0026#34;Zhao Kun\u0026#34;; } Foo.hasOwnProperty(\u0026#39;prototype\u0026#39;); // true [[prototype]] 谁创建的 使用 new 关键字创建一个新的对象，那么这个对象本身会有一个内部的/私有的或指针指向 Foo 的 prototype：\nfunction Foo() { this.name = \u0026#34;Zhao Kun\u0026#34;; } let b = new Foo(); b.[[Prototype]] === Foo.prototype // true proto 谁创建的 __proto__ 是 [[prototype]] 的 public 形式的指针：\nlet b = new Foo(); b.__proto___ === Foo.prototype // true 自 ECMAScript5 起，你有另外一种选择，可以拿到这个对象内部的私有的 [[prototype]] 的这个指针：\nObject.getPrototypeOf(b) === b.__proto__ // true 参考  proto VS. prototype in JavaScript  "});index.add({'id':245,'href':'/docs/programmer-interview/front-end/html-semantic/','title':"HTML 语义化",'content':"HTML 语义化 作用 Web语义化是指使用恰当语义的html标签、class类名等内容，让页面具有良好的结构与含义，从而让人和机器都能快速理解网页内容。语义化的web页面一方面可以让机器在更少的人类干预情况下收集并研究网页的信息，从而可以读懂网页的内容，然后将收集汇总的信息进行分析，结果为人类所用；另一方面它可以让开发人员读懂结构和用户以及屏幕阅读器（如果访客有视障）能够读懂内容。 简单来说就是利于 SEO，便于阅读维护理解。\n常见语义化标签  i：专业术语 em：强调文本 strong：这个文本非常重要 section：文档中的一个区域、一节 article：文档、页面、网站中的独立结构 aside：附属信息 nav：页面的导航链接区域 footer：页脚 hgroup：文章的标题 header：页眉  dl、dt、dd \u0026lt;dl\u0026gt; 代表 description list，这个 list 封装了若干个 terms (\u0026lt;dt\u0026gt;) 以及 descriptions (\u0026lt;dd\u0026gt;) 信息。\n\u0026lt;dl\u0026gt; \u0026lt;dt\u0026gt;火狐浏览器\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; 由 Mozilla 组织以及数百个志愿者一起开发的一款免费、开源、跨平台的 Web 浏览器。 \u0026lt;/dd\u0026gt; \u0026lt;dt\u0026gt;Chrome 浏览器\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; 谷歌浏览器，是一个由Google（谷歌）公司开发的开放源代码网页浏览器。 \u0026lt;/dd\u0026gt; \u0026lt;/dl\u0026gt; "});index.add({'id':246,'href':'/docs/programmer-interview/algorithm/delete-node-in-bst/','title':"二叉搜索树中删除一个节点",'content':"二叉搜索树中删除一个节点  微软\n // https://leetcode.com/problems/delete-node-in-a-bst/ // // 微软面试题 // // 这道题的删除节点，就是把这个节点所有的孩子都给删除了 public class DeleteNodeinaBST { public TreeNode deleteNode(TreeNode root, int key) { // ================  // search the node  // ================  TreeNode curr = root; TreeNode predecessor = null; // 我们是从 root 的 left 树进去的吗?  boolean left = false; while (curr != null) { if (curr.val == key) { break; } else if (curr.val \u0026gt; key) { predecessor = curr; curr = curr.left; left = true; } else if (curr.val \u0026lt; key) { predecessor = curr; curr = curr.right; left = false; } } if (curr == null) { // ====================  // we cannot find the node  // ====================  return root; } // leaf node  if (curr.left == null \u0026amp;\u0026amp; curr.right == null) { // Case 1. 没有祖先  // The tree has only one node:  //  // 1  //  if (predecessor == null) { // root.val == key  return null; } // Case 2.  //  // 6  // / \\  // 4 8 (删除 4)  // / \\ / \\  // 3 5 7 9  if (left) { predecessor.left = null; } else { predecessor.right = null; } return root; } // has one child  // 8  // \\  // 10 \u0026lt;-- delete  // \\  // 14  // / \\  // 12 15  if (curr.left == null) { // 1 \u0026lt;- 删除 1  // \\  // 2  if (predecessor == null) { return curr.right; } if (left) { predecessor.left = curr.right; } else { predecessor.right = curr.right; } return root; } // 8  // \\  // 15 \u0026lt;-- delete  // /  // 9  //  if (curr.right == null) { if (predecessor == null) { return curr.left; } if (left) { predecessor.left = curr.left; } else { predecessor.right = curr.left; } return root; } // has two child  // 用右树上第一个比 curr 大的节点来替代  TreeNode inorderSuccessor = deletefirstInorderSuccessor(curr); if (predecessor == null) { inorderSuccessor.left = curr.left; inorderSuccessor.right = curr.right; return inorderSuccessor; } if (left) { predecessor.left = inorderSuccessor; } else { predecessor.right = inorderSuccessor; } inorderSuccessor.left = curr.left; inorderSuccessor.right = curr.right; return predecessor == null ? inorderSuccessor : root; } // 删除 root 开始的第一个中序遍历的后继  private TreeNode deletefirstInorderSuccessor(TreeNode root) { TreeNode predecessor = root; // ===================  // 5  // \\  // 7 \u0026lt;-- predecessor  // \\  // 8  // ===================  root = root.right; if (root.left == null) { predecessor.right = root.right; return root; } while (root.left != null) { predecessor = root; root = root.left; } // 5  // \\  // 7  // /  // 6  // \\  // 6.5  predecessor.left = root.right; return root; } } "});index.add({'id':247,'href':'/docs/tutorial/unix-optimize/memory/','title':"内存",'content':"内存  作者：赵坤\n 内存分配 首先给出 32 位系统虚拟内存空间分布图：\n在 C 语言中，内存分配采用 malloc() 函数进行分配。底层实现：\n 申请的内存小于 128K，使用 brk() 函数完成，也就是从上图中的堆中分配的内存 申请的内存大于 128K，使用 mmap() 内存映射函数完成，也就是从上图中的文件映射中分配的内存  内存回收 应用程序应通过 free() 或 unmap() 来释放内存。\n当然，系统也会监管进程的内存，当发现系统内存不足时，会采取措施：\n 使用 LRU 算法回收缓存 回收不常访问的内存，写到 Swap 区（位于硬盘上） 杀死进程  查看整个系统的内存 $ free total used free shared buff/cache available Mem: 6030036 2312004 266488 624252 3451544 2911700 Swap: 2097148 256 2096892 查看某个进程的内存 使用 top 或 ps：\n$ top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 8883 zk 20 0 16.7g 477004 223504 S 4.7 7.9 2:02.69 chrome 2075 zk 20 0 1233356 99568 53584 S 4.0 1.7 4:49.63 Xorg 2681 zk 20 0 865052 56464 42040 S 2.7 0.9 0:11.70 gnome-terminal- 2247 zk 20 0 4335224 252364 99052 S 1.3 4.2 6:50.33 gnome-shell Buffer vs Cache free 命令中的 buff/cache 一列的 buff 和 cache 分别指代什么？\n buff 即 Buffer，缓存磁盘数据，是对原始磁盘块的缓存，内核可以将分散写改为集中写。 cache 缓存从磁盘读取的数据，缓存的是磁盘读取文件的页缓存。  内存回收机制 系统可以回收的内存：\n 文件页（Buffer、Cache、文件映射页） 匿名页（堆内存）  其中，文件页如果被修改过，那么就会变为脏页，必须先写入磁盘，才能内存释放。脏页写入磁盘有两种方式：\n 调用 fsync() 写入脏页到磁盘 内核线程定期 pdflush 刷新这些脏页到磁盘  而匿名页虽无法直接回收，但是通过 Swap 机制可以将不常用的内存写到磁盘中，然后释放内存，给其他程序用。\n通过调节 /proc/sys/vm/swappiness 值（0 - 100）可以控制回收匿名页的优先级，值越大，越优先回收匿名页。\n"});index.add({'id':248,'href':'/docs/programmer-interview/java/finally/','title':"基础 - finally",'content':"finally 常见疑惑代码片段 （1）下面代码中的 finally 块会执行吗？\ntry { // do something  System.exit(1); } finally{ System.out.println(“Print from finally”); } 答案：不会。\n（2）finally 会执行吗？\npublic static int test() { try { return 0; } finally { System.out.println(\u0026#34;finally trumps return.\u0026#34;); } } 答案：\nfinally trumps return. 0 （3）这个方法返回的值是 10 还是 12？\npublic static int getMonthsInYear() { try { return 10; } finally { return 12; } } 答案：12\n（4）执行顺序\ntry{ int divideByZeroException = 5 / 0; } catch (Exception e){ System.out.println(\u0026#34;catch\u0026#34;); return; // also tried with break; in switch-case, got same output } finally { System.out.println(\u0026#34;finally\u0026#34;); } 答案：\ncatch finally （5）赋值\npublic static void main(final String[] args) { System.out.println(test()); } public static int test() { int i = 0; try { i = 2; return i; } finally { i = 12; System.out.println(\u0026#34;finally trumps return.\u0026#34;); } } 结果：\nfinally trumps return. 2 （6）执行顺序\npublic static void main(String[] args) { System.out.println(Test.test()); } public static int printX() { System.out.println(\u0026#34;X\u0026#34;); return 0; } public static int test() { try { return printX(); } finally { System.out.println(\u0026#34;finally trumps return... sort of\u0026#34;); } } 结果：\nX finally trumps return... sort of 0 不会执行 finally 的情况  触发了 System.exit() 触发了 Runtime.getRuntime().halt(exitStatus) JVM Crash  "});index.add({'id':249,'href':'/docs/programmer-interview/java/char/','title':"char",'content':"char char 能存储中文字符吗 在 Java 中，char 类型占 2 个字节，而且 Java 默认采用 Unicode 编码，一个 Unicode 码是 16 位，所以一个 Unicode 码占两个字节，Java 中无论汉字还是英文字母都是用 Unicode 编码来表示的。所以，在Java中，char 类型变量可以存储一个中文汉字。\n"});index.add({'id':250,'href':'/docs/programmer-interview/front-end/html5-new-feature/','title':"HTML5 新特性",'content':"HTML5 新特性 新的 HTML5 语义 章节、轮廓等语义标签 \u0026lt;section\u0026gt;、\u0026lt;article\u0026gt;、\u0026lt;nav\u0026gt;、\u0026lt;header\u0026gt;、\u0026lt;footer\u0026gt;、\u0026lt;aside\u0026gt;\n音频、视频 \u0026lt;audio\u0026gt;、\u0026lt;video\u0026gt;\ninput 校验 \u0026lt;input\u0026gt; 标签引入了新的属性：\n required：表单的这个字段不为空 minlength 和 maxlength：文本的长度要求 min 和 max：数值类型的大小值约束 type：是否是数值？邮件地址？或其它类型 pattern：输入的内容必须符合整个正则表达式  新的语义元素 \u0026lt;mark\u0026gt;、\u0026lt;figure\u0026gt;、\u0026lt;figcaption\u0026gt;、\u0026lt;data\u0026gt;、\u0026lt;time\u0026gt;、\u0026lt;output\u0026gt;、\u0026lt;progress\u0026gt;、\u0026lt;meter\u0026gt;、\u0026lt;main\u0026gt;\niframe 安全  sandbox 属性：附加更多限制 srcdoc 属性：内嵌的 HTML 内容  MathML 数学公式 使用 MathML 直接插入数学公式\n数据传输 Web Sockets 浏览器和服务器建立一个长久连接，双方都可以发送信息给对方。\nServer-sent events Server 推送 event 给客户端：\nconst evtSource = new EventSource(\u0026#39;ssedemo.php\u0026#39;) WebRTC RTC：Real-Time Communication\n存储 IndexedDB 选择多个文件 \u0026lt;input type = 'file'\u0026gt; 新增 multiple 属性，可选择多个文件。\n多媒体 Camera API \u0026lt;input type=\u0026#34;file\u0026#34; id=\u0026#34;take-picture\u0026#34; accept=\u0026#34;image/*\u0026#34;\u0026gt; Graphics canvas 画布\nWebGL 提供 3D 能力\nSVG 可以内嵌在 HTML 中的基于 XML 的向量图片\n性能  Web Workers：繁重的 JavaScript 计算任务可以放到后台线程 History API：管理浏览器记录 Drag 和 Drop：拖拽文件 requestAnimationFrame：控制动画渲染 Fullscreen API：提供全屏 API  设备可访问性  Touch Events：感应用户手指触感事件 geolocation：提供地理位置信息  参考  HTML5  "});index.add({'id':251,'href':'/docs/programmer-interview/java/java-exception/','title':"Java 异常",'content':"Java 异常 异常架构图 "});index.add({'id':252,'href':'/docs/programmer-interview/algorithm/insert-into-bst/','title':"二叉搜索树中新增一个节点",'content':"二叉搜索树中新增一个节点  微软\n // https://leetcode.com/problems/insert-into-a-binary-search-tree/ // 保证二叉树原来没有值为 val 的节点 // // 微软面试题 public class InsertintoaBinarySearchTree { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) { return new TreeNode(val); } TreeNode backupRoot = root; TreeNode predecessor = null; while (root != null) { predecessor = root; if (root.val \u0026gt; val) { root = root.left; } else if (root.val \u0026lt; val) { root = root.right; } } if (predecessor.val \u0026lt; val) { predecessor.right = new TreeNode(val); } else { predecessor.left = new TreeNode(val); } return backupRoot; } } "});index.add({'id':253,'href':'/docs/programmer-interview/java/classload/','title':"类加载",'content':"类加载 "});index.add({'id':254,'href':'/docs/programmer-interview/front-end/meta/','title':"meta",'content':"meta 作用 提供 Document 的元信息。\ncharset 属性 \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; content 属性 \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Free Web tutorials\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;HTML,CSS,XML,JavaScript\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; http-equiv \u0026lt;!-- Redirect page after 3 seconds --\u0026gt; \u0026lt;meta http-equiv=\u0026#34;refresh\u0026#34; content=\u0026#34;3;url=https://www.mozilla.org\u0026#34;\u0026gt; name \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Free Web tutorials\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;HTML,CSS,JavaScript\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;John Doe\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; "});index.add({'id':255,'href':'/docs/programmer-interview/algorithm/diameter-of-binary-tree/','title':"二叉树的直径",'content':"二叉树的直径  微软\n // https://leetcode.com/problems/diameter-of-binary-tree/ // // 二叉树的直径 // // Given a binary tree // 1 // / \\ // 2 3 // / \\ // 4 5 // Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. public class DiameterofBinaryTree { public int diameterOfBinaryTree(TreeNode root) { if (root == null) { return 0; } int rootDiameter = depth(root.left) + depth(root.right); int leftDiameter = diameterOfBinaryTree(root.left); int rightDiameter = diameterOfBinaryTree(root.right); return Math.max(rootDiameter, Math.max(leftDiameter, rightDiameter)); } private int depth(TreeNode root) { if (root == null) { return 0; } return 1 + Math.max(depth(root.left), depth(root.right)); } } "});index.add({'id':256,'href':'/docs/programmer-interview/algorithm/next-node-of-inorder-traverse/','title':"中序遍历的下一个节点",'content':"中序遍历的下一个节点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。\n// https://www.nowcoder.com/practice/9023a0c988684a53960365b889ceaf5e // // 牛客网 // 二叉树中包含指向父节点的指针 next public class NextNodeOfBinaryTree { public TreeLinkNode GetNext(TreeLinkNode pNode) { if (pNode == null) { return null; } // 1  // 2 3  // 4 5 6 7  // 8 9  if (pNode.right != null) { // 2 的下一个节点:  //  // 如果 8 存在，那么最终是 8  // 如果 8 不存在，那么最终是 5  TreeLinkNode p = pNode.right; while (p.left != null) { p = p.left; } return p; } else { // 1  // 2 3  // [4] 5 6 7  //  TreeLinkNode parent = pNode.next; while (parent != null) { if (parent.left == pNode) { return parent; } pNode = parent; parent = parent.next; } } return null; } } "});index.add({'id':257,'href':'/docs/programmer-interview/algorithm/binary-tree-maximum-path-sum/','title':"二叉树最大路径和",'content':"二叉树最大路径和 解法一 // // 从树的任何一个节点开始，到任何一个节点结束，路径和最长 // 至少包含一个节点 // // Hard 级别 // public class BinaryTreeMaximumPathSum { int result = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) { helper(root); return result; } private int helper(TreeNode root) { if (root == null) { return 0; } int left = helper(root.left); int right = helper(root.right); // - 只选择 root  // - 选择 root + left  // - 选择 root + right  // - 为什么没有 left ? 因为我们现在是站在 root 节点上进行考虑的，root 必选  int returnResult = Math.max(root.val + right, Math.max(root.val, root.val + left)); // 实际上这个地方一旦像这样 root.val + left + right 串起来的话，那么子树就只能沿着单路径走了  // 下面这棵树，最大是 42，而不是 41，因为用 20 做根，就不能串 -10 了，只能串一次  // 所以串的这个过程要借助其他变量 result 额外更新  //  // -10  // / \\  // 9 20  // / \\  // 15 7  //  // - 选择 root.val + left + right 连起来  // 更新  result = Math.max(Math.max(result, root.val + left + right), returnResult); // 返回最大的  return returnResult; } } 解法二 public class BinaryTreeMaximumPathSum { private int maxValue = Integer.MIN_VALUE; public int maxPathSum0(TreeNode root) { maxPathDown(root); return maxValue; } private int maxPathDown(TreeNode node) { if (node == null) return 0; int left = Math.max(0, maxPathDown(node.left)); int right = Math.max(0, maxPathDown(node.right)); // 选择 root，选择 left，选择 right  // 选择 root，选择 left，不选择 right，即 right = 0  // 选择 root，不选择 left，选择 right，即 left = 0  maxValue = Math.max(maxValue, left + right + node.val); // 选择 left 或者 right 最大的  return Math.max(left, right) + node.val; } } "});index.add({'id':258,'href':'/docs/programmer-interview/algorithm/binary-tree-inorder-traversal/','title':"二叉树非递归中序遍历",'content':"二叉树非递归中序遍历 public class BinaryTreeInorderTraversal { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { if (root == null) { return Collections.emptyList(); } List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); TreeNode curr = root; // 1  // / \\  // 2 3  // / \\  // 4 5  // / \\  // 6 7  //  // 4 2 6 5 7 1 3  while (true) { // =========================  // 这个地方容易出错：  //  // stack.push(root);  //  // while (true) {  // if (root.left != null) {  // root = root.left;  // stack.push(root);  // }  // }  //  // 这样，会漏掉左树的最后一个节点  //  // 第二个出错点，就是不设置指针，使用 root 很容易出错  // =========================  if (curr != null) { stack.push(curr); curr = curr.left; } else { while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek().right == null) { curr = stack.pop(); res.add(curr.val); } if (!stack.isEmpty()) { curr = stack.pop(); res.add(curr.val); // ============================  // 注意这个地方，不是把 curr.right push 进去栈了  // 而是用指针指向这个右树了  // ============================  curr = curr.right; } else { break; } } } return res; } } "});index.add({'id':259,'href':'/docs/programmer-interview/algorithm/lowest-common-ancestor-of-a-binary-search-tree/','title':"二叉树的公共祖先",'content':"二叉树的公共祖先 二叉树的公共祖先 // 所有 NODE 节点的值都是唯一的 // p 和 q 一定存在于二叉树里面 public class LowestCommonAncestorofaBinaryTree { // =========================  // 这个方法最优  // =========================  public TreeNode lowestCommonAncestor0(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) { return root; } TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if (left != null \u0026amp;\u0026amp; right != null) { return root; } return left != null ? left : right; } // =========================  // 方法二  // =========================  public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root != null) { // 递归调用自身  // 实际上，是从 root 这棵树的最后一排元素开始倒着  // 寻找 p 和 q 的公共祖先的  //  // 也就是尽量保证祖先尽量靠近底层  TreeNode leftRoot = lowestCommonAncestor(root.left, p, q); if (leftRoot != null) { return leftRoot; } TreeNode rightRoot = lowestCommonAncestor(root.right, p, q); if (rightRoot != null) { return rightRoot; } if (isGrandSonNode(root, p) \u0026amp;\u0026amp; isGrandSonNode(root, q)) { return root; } } return null; } // t 是否是 root 的孙子节点  private boolean isGrandSonNode(TreeNode root, TreeNode t) { if (root == null) { return false; } if (root == t || root.left == t || root.right == t) { return true; } return isGrandSonNode(root.left, t) || isGrandSonNode(root.right, t); } } 二叉搜索树的公共祖先 public class LowestCommonAncestorofaBinarySearchTree { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root != null) { // 都在 root 的左树  if (p.val \u0026lt; root.val \u0026amp;\u0026amp; q.val \u0026lt; root.val) { return lowestCommonAncestor(root.left, p, q); } // 都在 root 的右树  if (p.val \u0026gt; root.val \u0026amp;\u0026amp; q.val \u0026gt; root.val) { return lowestCommonAncestor(root.right, p, q); } return root; } return null; } // ===============================  // 另外一个版本  // ===============================  public TreeNode lowestCommonAncestor0(TreeNode root, TreeNode p, TreeNode q) { if (root == null) { return null; } // 6  // / \\  // [2] 8  // / \\ / \\  // 0 [4] 7 9  // / \\  // 3 5  // =======================  // 必须是等于  // =======================  if (p.val \u0026lt;= root.val \u0026amp;\u0026amp; q.val \u0026gt;= root.val) { return root; } // =======================  // 必须是等于  // =======================  if (q.val \u0026lt;= root.val \u0026amp;\u0026amp; p.val \u0026gt;= root.val) { return root; } if (p.val \u0026lt; root.val \u0026amp;\u0026amp; q.val \u0026lt; root.val) { return lowestCommonAncestor(root.left, p, q); } if (p.val \u0026gt; root.val \u0026amp;\u0026amp; q.val \u0026gt; root.val) { return lowestCommonAncestor(root.right, p, q); } return null; } } "});index.add({'id':260,'href':'/docs/programmer-interview/algorithm/is-subtree-another-tree/','title':"一颗二叉树是否是另外一颗的子树",'content':"一颗二叉树是否是另外一颗的子树 // O(N^2) public class SubtreeOfAnotherTree { public boolean isSubtree(TreeNode s /** root tree */, TreeNode t /** sub tree */) { return isSameTree(s, t) || (s != null ? (isSubtree(s.left, t) || isSubtree(s.right, t)) : false); } private boolean isSameTree(TreeNode a, TreeNode b) { if (a == null \u0026amp;\u0026amp; b == null) { return true; } if (a == null || b == null) { return false; } if (a.val != b.val) { return false; } return isSameTree(a.left, b.left) \u0026amp;\u0026amp; isSameTree(a.right, b.right); } } "});index.add({'id':261,'href':'/docs/programmer-interview/algorithm/binary-tree-right-side-view/','title':"二叉树右视图",'content':"二叉树右视图  微软问的其实是左视图。 原题\n 递归 public class Solution { public List\u0026lt;Integer\u0026gt; rightSideView(TreeNode root) { List\u0026lt;Integer\u0026gt; result = new ArrayList\u0026lt;Integer\u0026gt;(); rightView(root, result, 0); return result; } public void rightView(TreeNode curr, List\u0026lt;Integer\u0026gt; result, int currDepth){ if (curr == null){ return; } if (currDepth == result.size()){ result.add(curr.val); } rightView(curr.right, result, currDepth + 1); rightView(curr.left, result, currDepth + 1); } } 层次遍历 public class Solution { public List\u0026lt;Integer\u0026gt; rightSideView(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; rst = new ArrayList\u0026lt;\u0026gt;(); if (root == null) return rst; queue.offer(root); while (!queue.isEmpty()){ int levelNum = queue.size(); for (int i = 0; i \u0026lt; levelNum; i++){ if (queue.peek().left != null) queue.offer(queue.peek().left); if (queue.peek().right != null) queue.offer(queue.peek().right); if (i == levelNum - 1) rst.add(queue.poll().val); else queue.poll(); } } return rst; } } "});index.add({'id':262,'href':'/docs/programmer-interview/java/collections/','title':"Java 集合类框架图",'content':"Java 集合类框架图 Collections 框架图 List 框架图 Set 框架图 Map 框架图 Queue 框架图 参考  Overview of Java Collections Framework API (UML diagram)  "});index.add({'id':263,'href':'/docs/programmer-interview/front-end/js-detect-array/','title':"JavaScript 检测数组",'content':"JavaScript 检测数组 Array.isArray Array.isArray(obj) 兼容旧版本：\nif (typeof Array.isArray === \u0026#39;undefined\u0026#39;) { Array.isArray = function(obj) { return Object.prototype.toString.call(obj) === \u0026#39;[object Array]\u0026#39;; } } constructor function isArray(obj) { return !!obj \u0026amp;\u0026amp; obj.constructor === Array; } "});index.add({'id':264,'href':'/docs/programmer-interview/java/kafka-high-throughput/','title':"Kafka 高吞吐量怎么实现的",'content':"Kafka 高吞吐量怎么实现的 顺序读写 Producer 发送的消息顺序追加到文件中，Consumer 从 Broker 自带偏移量读取消息。这两者可以充分利用磁盘的顺序写和顺序读性能，速度远快于随机读写。\n   零拷贝 mmap 持久化文件 Broker 写入数据，并非真正的 flush 到磁盘上了，而是写入到 mmap 中。\nsendfile 读取 Customer 从 Broker 读取数据，采用 sendfile，将磁盘文件读到 OS 内核缓冲区后，直接转到 socket buffer 进行网络发送。\n分区 Kafka 将消息分成多个 partition，增加了并行处理的能力。\n批量发送 Producer 发送多个消息到同一分区，通过批量发送可以减少系统性能开销。\n batch.size：默认积压到 16K 就会批量发送 linger.ms：设置一定延迟来收集更多消息。默认 0ms ，即有消息就立马发送。  上述两个条件有任一条件满足，就会触发批量发送。\n数据压缩 Kafka 支持三种压缩算法：\n gzip snappy lz4  /*compressType有四种取值:none lz4 gzip snappy*/ props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressType); 参考  Kafka为什么吞吐量大、速度快？  "});index.add({'id':265,'href':'/docs/programmer-interview/algorithm/word-ladder/','title':"WordLadder",'content':"WordLadder  微软、阿里巴巴\n // 常见面试题 // 阿里巴巴、微软都问过 // https://leetcode.com/problems/word-ladder/ // public class WordLadder { // Runtime: 135 ms, faster than 39.64% of Java online submissions for Word Ladder.  public int ladderLength(String beginWord, String endWord, List\u0026lt;String\u0026gt; wordList) { Set\u0026lt;String\u0026gt; wordSet = new HashSet\u0026lt;\u0026gt;(wordList); if (!wordSet.contains(endWord)) { return 0; } Queue\u0026lt;String\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(beginWord); int level = 0; while (!queue.isEmpty()) { level++; int size = queue.size(); for (int i = 0; i \u0026lt; size; i++) { String curr = queue.poll(); if (curr.equals(endWord)) { return level; } for (int j = 0; j \u0026lt; curr.length(); j++) { for (char c = \u0026#39;a\u0026#39;; c \u0026lt;= \u0026#39;z\u0026#39;; c++) { String newWord = curr.substring(0, j) + c + curr.substring(j + 1); if (!wordSet.contains(newWord)) { continue; } queue.add(newWord); wordSet.remove(newWord); } } } } return 0; } // =========================  // 尝试优化，优化失败  // =========================  // Runtime: 163 ms, faster than 36.47% of Java online submissions for Word Ladder.  public int ladderLength0(String beginWord, String endWord, List\u0026lt;String\u0026gt; wordList) { Set\u0026lt;String\u0026gt; wordSet = new HashSet\u0026lt;\u0026gt;(wordList); if (!wordSet.contains(endWord)) { return 0; } Map\u0026lt;Integer, Set\u0026lt;Character\u0026gt;\u0026gt; charMap = buildCharMap(wordList); Queue\u0026lt;String\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(beginWord); int level = 0; while (!queue.isEmpty()) { level++; int size = queue.size(); for (int i = 0; i \u0026lt; size; i++) { String curr = queue.poll(); if (curr.equals(endWord)) { return level; } for (int j = 0; j \u0026lt; curr.length(); j++) { for (char c: charMap.get(j)) { String newWord = curr.substring(0, j) + c + curr.substring(j + 1); if (!wordSet.contains(newWord)) { continue; } queue.add(newWord); wordSet.remove(newWord); } } } } return 0; } private Map\u0026lt;Integer, Set\u0026lt;Character\u0026gt;\u0026gt; buildCharMap(List\u0026lt;String\u0026gt; wordSet) { Map\u0026lt;Integer, Set\u0026lt;Character\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (String word: wordSet) { for (int i = 0; i \u0026lt; word.length(); i++) { Set\u0026lt;Character\u0026gt; charSet = map.getOrDefault(i, new HashSet\u0026lt;Character\u0026gt;()); charSet.add(word.charAt(i)); map.put(i, charSet); } } return map; } } "});index.add({'id':266,'href':'/docs/tutorial/unix-optimize/io/','title':"磁盘 I/O",'content':"磁盘 I/O  作者：赵坤\n 虚拟文件系统 I/O 调度 为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。\nLinux 内核支持四种 I/O 调度算法，分别是 NONE、NOOP、CFQ 以及 DeadLine。\n NONE，不使用 I/O 调度算法 NOOP，先入先出 CFQ（Completely Fair Scheduler），为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求 DeadLine，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理  每块磁盘 I/O 性能 $ iostat -d -x 1 Linux 5.4.0-42-generic (zk) 2020年09月02日 _x86_64_\t(4 CPU) Device r/s rkB/s rrqm/s %rrqm r_await rareq-sz w/s wkB/s wrqm/s %wrqm w_await wareq-sz d/s dkB/s drqm/s %drqm d_await dareq-sz aqu-sz %util loop0 0.67 0.73 0.00 0.00 0.59 1.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 loop1 0.01 0.06 0.00 0.00 0.87 7.43 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 loop2 0.23 0.28 0.00 0.00 0.40 1.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 loop3 1.49 1.55 0.00 0.00 0.40 1.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 loop4 0.87 1.05 0.00 0.00 0.46 1.21 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 loop5 0.01 0.06 0.00 0.00 0.64 8.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 loop6 2.20 2.25 0.00 0.00 0.47 1.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 loop7 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sda 0.06 1.92 0.00 0.00 30.65 31.56 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 sdb 6.13 277.71 1.45 19.16 1.17 45.31 8.24 166.67 6.65 44.65 0.88 20.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.03 其中：\n %util，就是磁盘 I/O 使用率； r/s + w/s ，就是 IOPS； rkB/s+wkB/s ，就是吞吐量； r_await+w_await ，就是响应时间。  进程 I/O 性能 $ pidstat -d 1 Average: UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command Average: 0 282 -1.00 -1.00 -1.00 0 jbd2/sdb5-8 Average: 1000 2524 0.00 11.96 0.00 0 chrome 追踪系统调用 如何知道某个进程当前正在读写哪一个文件？\n$ strace -p 18940 strace: Process 18940 attached ... mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f7aee9000 mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f682e8000 write(3, \u0026#34;2018-12-05 15:23:01,709 - __main\u0026#34;..., 314572844 ) = 314572844 munmap(0x7f0f682e8000, 314576896) = 0 write(3, \u0026#34;\\n\u0026#34;, 1) = 1 munmap(0x7f0f7aee9000, 314576896) = 0 close(3) = 0 stat(\u0026#34;/tmp/logtest.txt.1\u0026#34;, {st_mode=S_IFREG|0644, st_size=943718535, ...}) = 0 查看进程打开的文件列表 $ lsof -p 18940 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python 18940 root cwd DIR 0,50 4096 1549389 / python 18940 root rtd DIR 0,50 4096 1549389 / … python 18940 root 2u CHR 136,0 0t0 3 /dev/pts/0 python 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt "});index.add({'id':267,'href':'/docs/programmer-interview/java/design-seckilling/','title':"秒杀系统设计",'content':"秒杀系统设计 秒杀其实主要解决两个问题，一个是并发读，一个是并发写。并发读的核心优化理念是尽量减少用户到服务端来“读”数据，或者让他们读更少的数据；并发写的处理原则也一样，它要求我们在数据库层面独立出来一个库，做特殊的处理。另外，我们还要针对秒杀系统做一些保护，针对意料之外的情况设计兜底方案，以防止最坏的情况发生。\n秒杀系统架构原则  数据尽量少: 可以简化秒杀页面的大小，去掉不必要的页面装修效果，等等。 请求数尽量少: 减少请求数最常用的一个实践就是合并 CSS 和 JavaScript 文件，把多个 JavaScript 文件合并成一个文件，在 URL 中用逗号隔开。 路径要尽量短: 缩短访问路径有一种办法，就是多个相互强依赖的应用合并部署在一起，把远程过程调用（RPC）变成 JVM 内部之间的方法调用。 依赖要尽量少: 0 级系统要尽量减少对 1 级系统的强依赖，防止重要的系统被不重要的系统拖垮。例如支付系统是 0 级系统，而优惠券是 1 级系统的话，在极端情况下可以把优惠券给降级，防止支付系统被优惠券这个 1 级系统给拖垮。 不要有单点: 应用无状态化。  动静分离 热点数据 流量削峰 排队 答题 系统优化 配置线程数 很多多线程的场景都有一个默认配置，即 “线程数 = 2 * CPU 核数 + 1” 。除去这个配置，还有一个根据最佳实践得出来的公式：线程数 = [(线程等待时间 + 线程 CPU 时间) / 线程 CPU 时间] × CPU 数量。\n 最好的办法是通过性能测试来发现最佳的线程数。\n 发现 CPU 瓶颈 JProfiler 和 Yourkit 这两个工具。\n减库存 兜底方案 降级 当秒杀流量达到 5w/s 时，把成交记录的获取从展示 20 条降级到只展示 5 条。“从 20 改到 5”这个操作由一个开关来实现，也就是设置一个能够从开关系统动态获取的系统参数。\n限流 限流既可以是在客户端限流，也可以是在服务端限流。此外，限流的实现方式既要支持 URL 以及方法级别的限流，也要支持基于 QPS 和线程的限流。\n在限流的实现手段上来讲，基于 QPS 和线程数的限流应用最多，最大 QPS 很容易通过压测提前获取，例如我们的系统最高支持 1w QPS 时，可以设置 8000 来进行限流保护。线程数限流在客户端比较有效，例如在远程调用时我们设置连接池的线程数，超出这个并发线程请求，就将线程进行排队或者直接超时丢弃。\n拒绝服务 当系统负载达到一定阈值时，例如 CPU 使用率达到 90% 或者系统 load 值达到 2 * CPU 核数时，系统直接拒绝所有请求，这种方式是最暴力但也最有效的系统保护方式。\n 在最前端的 Nginx 上设置过载保护，当机器负载达到某个值时直接拒绝 HTTP 请求并返回 503 错误码，在 Java 层同样也可以设计过载保护。\n "});index.add({'id':268,'href':'/docs/programmer-interview/front-end/js-extend/','title':"JavaScript 继承",'content':"JavaScript 继承 原型链 function Parent() {} function Child() {} Child.prototype = new Parent(); Child.prototype.constructor = Child; 构造器 function Parent() {} function Child() { Parent.call(this); } 原型链 + 构造器 function Parent() {} function Child() { Parent.call(this); } Child.prototype = new Parent(); 原型式 function extendObject(obj) { function F() {} F.prototype = obj; return new F(); } 其实上述代码就是 Object.create() 的兼容方法。\n寄生组合 function inheritPrototype(child, parent) { var prototype = extendObject(parent.prototype); prototype.constructor = child; child.prototype = prototype; } function Child() { Parent.call(this); } inheritPrototype(Child, Parent) ES6 extends class Parent {} class Child extends Parent {]} let child = new Child();  child instanceof Parent 返回 true child instanceof Child 返回 true  对比  可复用：可复用父类的构造函数 会共享：子类的实例会共享父类的引用属性 可传参：创建子类的时候能否传递参数     方法 优点 缺点     原型链 可复用 会共享、不可传参   构造器 不会被共享、可传递参数 不可复用   原型链 + 构造器 可复用、不会被共享、可传参 调用了两次父类构造函数   原型式 可复用 会共享、不可传参   寄生组合 完美的方案    ES6 extends  兼容性    "});index.add({'id':269,'href':'/docs/programmer-interview/java/why-develop-rocketmq/','title':"为什么阿里要自研 RocketMQ ?",'content':"为什么阿里要自研 RocketMQ ? 为什么要重写一个类似于 Kafka 的消息队列，而非基于 Kafka 作二次开发？\n初衷 Kafka is a distributed streaming platform, which was born from logging aggregation cases. 它并不需要太高的并发. In some large scale cases in alibaba, we found that the original model 无法满足我们的实际需求.\n无法支持更多分区  Each partition stores the whole message data. 尽管单个分区是顺序写的, 随着越来越多的针对不同分区的写入, 在操作系统层面已经变为随机写了. Due to the scattered data files, it is difficult to use the Linux IO Group Commit mechanism.  RocketMQ 支持更多分区  所有消息数据都存储在 Commit Log 文件中。所有写入都是完全顺序的，而读取是随机的。对磁盘的访问是完全顺序的，这避免了磁盘锁争用，并且在创建大量队列时不会导致高磁盘 IO 等待。 ConsumeQueue 存储实际的用户消费位置信息，这些信息也以顺序方式刷新到磁盘。  参考  How to Support More Queues in RocketMQ?  "});index.add({'id':270,'href':'/docs/programmer-interview/algorithm/longest-increasing-path-in-a-matrix/','title':"二维数组寻找最长的单调递增序列",'content':"二维数组寻找最长的单调递增序列  微软\n // Input: nums = // [ // [9,9,4], // [6,6,8], // [2,1,1] // ] // Output: 4 // Explanation: The longest increasing path is [1, 2, 6, 9]. // // 微软面试题: 二维数组寻找最长的单调递增序列 // // =========================== // 时间复杂度分析: // 每个单元格，尝试它的四个相邻单元格 // 每个单元格都当做起始单元格，使用一个 Cache 来存储结果 // O(M * N) // =========================== public class LongestIncreasingPathinaMatrix { public int longestIncreasingPath(int[][] matrix) { if (matrix.length == 0) { return 0; } int[][] cached = new int[matrix.length][matrix[0].length]; int max = 0; for (int i = 0; i \u0026lt; matrix.length; i++) { for (int j = 0; j \u0026lt; matrix[i].length; j++) { max = Math.max(max, dfs(matrix, i, j, cached)); } } return max; } private int dfs(int[][] matrix, int i, int j, int[][] cached) { if (cached[i][j] != 0) { return cached[i][j]; } int max = 1; for (int d = 0; d \u0026lt; DIR.length; d++) { int ni = i + DIR[d][0]; int nj = j + DIR[d][1]; if (ni \u0026lt; 0 || ni \u0026gt;= matrix.length || nj \u0026lt; 0 || nj \u0026gt;= matrix[ni].length) { continue; } if (matrix[ni][nj] \u0026lt;= matrix[i][j]) { continue; } max = Math.max(max, dfs(matrix, ni, nj, cached) + 1); } return cached[i][j] = max; } static int[][] DIR = new int[][] { {-1,0}, {0,1}, {1,0}, {0,-1} }; } 时间复杂度 时间复杂度分析: 每个单元格，尝试它的四个相邻单元格，每个单元格都当做起始单元格，使用一个 Cache 来存储结果，O(M * N)\n"});index.add({'id':271,'href':'/docs/programmer-interview/front-end/array-shuffle/','title':"数组乱序",'content':"数组乱序 Fisher-Yates 乱序算法 /** * Shuffles array in place. * @param {Array} a items An array containing the items. */ function shuffle(a) { var j, x, i; for (i = a.length - 1; i \u0026gt; 0; i--) { j = Math.floor(Math.random() * (i + 1)); x = a[i]; a[i] = a[j]; a[j] = x; } return a; } ES6 /** * Shuffles array in place. ES6 version * @param {Array} a items An array containing the items. */ function shuffle(a) { for (let i = a.length - 1; i \u0026gt; 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [a[i], a[j]] = [a[j], a[i]]; } return a; } 参考  乱序  "});index.add({'id':272,'href':'/docs/programmer-interview/front-end/typeof/','title':"typeof",'content':"typeof 用法 typeof operand typeof(operand) typeof null \u0026gt; typeof null \u0026#34;object\u0026#34; 为什么 typeof null 是 object 根据 Why is typeof null “object”?，这是 JavaScript 实现上的一个 Bug，如果修正这个 Bug，会导致现有代码出现更多的 Bug。\nJavaScript 底层的 object 的 type 是使用 0 来表示的，而 null 在多数平台上也是使用 0 来表示，所以 null 的 type 也是 0，因此返回 object。\nnull 是 object 吗 null 不是 object，它是 primitive value 。\ntypeof typeof null \u0026gt; typeof typeof null \u0026#34;string\u0026#34; typeof Array \u0026gt; typeof Array \u0026#34;function\u0026#34; typeof [1,2,3] \u0026gt; typeof [1,2,3] \u0026#34;object\u0026#34; typeof 5 \u0026gt; typeof 5 \u0026#34;number\u0026#34; typeof false \u0026#34;boolean\u0026#34; typeof undefined typeof undefined \u0026#34;undefined\u0026#34; typeof String(\u0026ldquo;asdfasdf\u0026rdquo;) typeof String(\u0026#34;asdfasdf\u0026#34;) \u0026#34;string\u0026#34; 更多 "});index.add({'id':273,'href':'/docs/programmer-interview/front-end/array-unique/','title':"数组去重",'content':"数组去重 indexOf function onlyUnique(value, index, self) { return self.indexOf(value) === index; } filter var myArray = [\u0026#39;a\u0026#39;, 1, \u0026#39;a\u0026#39;, 2, \u0026#39;1\u0026#39;]; var unique = myArray.filter((v, i, a) =\u0026gt; a.indexOf(v) === i); Set function uniqueArray(a) { return [...new Set(a)]; } 参考  Get all unique values in a JavaScript array (remove duplicates)  "});index.add({'id':274,'href':'/docs/programmer-interview/front-end/flattern-array/','title':"数组扁平化",'content':"数组扁平化 递归实现 const flatten = function(arr, result = []) { for (let i = 0, length = arr.length; i \u0026lt; length; i++) { const value = arr[i]; if (Array.isArray(value)) { flatten(value, result); } else { result.push(value); } } return result; }; ES2015 reduce function flatten(arr) { return arr.reduce(function (flat, toFlatten) { return flat.concat(Array.isArray(toFlatten) ? flatten(toFlatten) : toFlatten); }, []); } 示例：\nflatten([[1, 2, 3], [4, 5]]); // [1, 2, 3, 4, 5] flatten([[[1, [1.1]], 2, 3], [4, 5]]); // [1, 1.1, 2, 3, 4, 5] ECMA 2019 flat const arr1 = [1, 2, [3, 4]]; arr1.flat(); // [1, 2, 3, 4]  const arr2 = [1, 2, [3, 4, [5, 6]]]; arr2.flat(); // [1, 2, 3, 4, [5, 6]]  // Flatten 2 levels deep const arr3 = [2, 2, 5, [5, [5, [6]], 7]]; arr3.flat(2); // [2, 2, 5, 5, 5, [6], 7];  // Flatten all levels const arr4 = [2, 2, 5, [5, [5, [6]], 7]]; arr4.flat(Infinity); // [2, 2, 5, 5, 5, 6, 7]; 参考  Merge/flatten an array of arrays  "});index.add({'id':275,'href':'/docs/programmer-interview/front-end/new/','title':"new 关键字",'content':"new 关键字 new 做了什么  创建一个对象 设置 __proto__ 属性 设置 this 指向这个对象 执行构造函数 返回对象  function New(func) { var res = {}; if (func.prototype !== null) { res.__proto__ = func.prototype; } var ret = func.apply(res, Array.prototype.slice.call(arguments, 1)); if ((typeof ret === \u0026#34;object\u0026#34; || typeof ret === \u0026#34;function\u0026#34;) \u0026amp;\u0026amp; ret !== null) { return ret; } return res; } 参考  What is the \u0026lsquo;new\u0026rsquo; keyword in JavaScript?  "});index.add({'id':276,'href':'/docs/programmer-interview/front-end/data-types/','title':"数据类型",'content':"数据类型 8 种数据类型  Primitive values 原始类型: Boolean、Null、Undefined、Number、BigInt、String、Symbol Object：Object  null vs undefined undefined 指已经声明，但是未赋值：\nlet testVar; alert(testVar); // undefined null 是已经赋值的变量：\nlet tetVar = null; alert(testVar); // null "});index.add({'id':277,'href':'/docs/programmer-interview/front-end/instanceof/','title':"instanceof",'content':"instanceof 作用 检测构造器函数的 prototype 是否位于某个对象的 __proto__ 原型链上。\n原理 function instance_of(V, F) { var O = F.prototype; V = V.__proto__; while (true) { if (V === null) return false; if (O === V) return true; V = V.__proto__; } } 为什么下列 instanceof 返回 false console.log(true instanceof Boolean); // false console.log(0 instanceof Number); // false console.log(\u0026#34;\u0026#34; instanceof String); // false console.log(new Boolean(true) instanceof Boolean); // true console.log(new Number(0) instanceof Number); // true console.log(new String(\u0026#34;\u0026#34;) instanceof String); // true 参考 ECMAScript 的标准：\n If type(o) is not object, return false.\n 因为在 JavaScript 的世界中，一切皆是对象 (除了 boolean、number、string、null、undefined、symbol、bigint 这几个原始类型)。所以这意味着 true、0、\u0026quot;\u0026quot;、undefined 等这些是非对象。\n参考  Why does instanceof return false for some literals?  "});index.add({'id':278,'href':'/docs/programmer-interview/front-end/let-vs-const-vs-var/','title':"let、var、const",'content':"let、var、const var  var 作用域：在整个 function 内有效 在声明之前就可以引用  function run() { console.log(foo) // 声明之前就可以引用，值：undefined \tvar foo = \u0026#34;Foo\u0026#34;; }  function 外定义会创建全局对象  var foo = \u0026#34;Foo\u0026#34;; console.log(window.foo) // Foo，附着在 window 对象  可以再次定义相同变量  \u0026#39;use strict\u0026#39; var foo = \u0026#34;foo1\u0026#34; var foo = \u0026#34;foo2\u0026#34; // foo 值替换为 foo2  闭包引用问题  // 打印 3 次 3 for (var i = 0; i \u0026lt; 3; i++) { setTimeout(() =\u0026gt; console.log(i), 0); } let (ES6)  let 作用域；在整个 block 内有效  { let a = 123 } console.log(a) // ReferenceError  声明之后才能引用  function run() { console.log(foo) // ReferenceError \tlet foo = \u0026#34;Foo\u0026#34;; }  function 外定义不会创建全局对象  let foo = \u0026#34;Foo\u0026#34; console.log(window.foo) // undefined  不可再次定义重复变量  \u0026#39;use strict\u0026#39; let bar = \u0026#34;bar1\u0026#34; let bar = \u0026#34;bar2\u0026#34; // SyntaxError  闭包引用问题  // 打印 1 2 3 for (let j = 0; j \u0026lt; 3; j++) { setTimeout(() =\u0026gt; console.log(j), 0); } const (ES6)  const 同 let 很像\n  无法再次赋值  const a = 42 a = 43 // TypeError  声明的时候就得初始化  const a; // SyntaxError 参考  What\u0026rsquo;s the difference between using “let” and “var”?  "});index.add({'id':279,'href':'/docs/programmer-interview/front-end/currying/','title':"柯里化 - Currying",'content':"柯里化 - Currying 作用 将 f(a, b, c) 调用形式转为 f(a)(b)(c) 调用形式，它对函数只做转换，不做执行。\n实现 function curry(func) { return function curried(...args) { if (args.length \u0026gt;= func.length) { return func.apply(this, args); } else { return function(...args2) { return curried.apply(this, args.concat(args2)); } } }; } 优点  多参数复用性 函数式编程  参考  Currying  "});index.add({'id':280,'href':'/docs/programmer-interview/front-end/settimeout/','title':"setTimeout",'content':"setTimeout 用 setTimeout 实现 setInterval { const intervals = new Map(); function setInterval(fn, time, context, ...args) { // 随机生成一个 ID  const id = Math.floor(Math.random() * 10000); intervals.set(id, setTimeout(function next() { intervals.set(id, setTimeout(next, time)); fn.apply(context, args); }, time)); return id; } function clearInterval(id) { clearTimeout(intervals.get(id)); } } 如何使用：\nconst interval = setInterval(console.log, 100, console, \u0026#34;hi\u0026#34;); clearInterval(interval); requestAnimationFrame  作用：告诉浏览器在下一次 repaint 的时候，更新你的动画，也就是说这个是转为动画设计的 API requestAnimationFrame 的调用时机：浏览器的 repaint 阶段 使用 requestAnimationFrame，只有你的网站页面的 Tab 页处于 visible 的时候，浏览器才会去运行你的动画。更省 CPU、更省 GPU、更省内存、更节约电量。 动画至少 60帧/秒，看起来才更流畅：  setInterval(function() { // animiate something }, 1000/60); "});index.add({'id':281,'href':'/docs/programmer-interview/front-end/strict-mode/','title':"Strict Mode",'content':"Strict Mode 作用  Strict Mode is a new feature in ECMAScript 5 that allows you to place a program, or a function, in a \u0026ldquo;strict\u0026rdquo; operating context. 这种严格的上下文能够禁掉一些行为以及抛出更多地错误.\n 为什么需要它  Strict mode makes it easier to write \u0026ldquo;secure\u0026rdquo; JavaScript.\n 如何开启  文件顶部：  // File: myscript.js  \u0026#39;use strict\u0026#39;; var a = 2; ...  function 顶部：  function doSomething() { \u0026#39;use strict\u0026#39;; ... } 约束  禁止全局变量  \u0026#39;use strict\u0026#39;; // Assignment to a non-writable global var undefined = 5; // throws a TypeError var Infinity = 5; // throws a TypeError  delete 不可删除的属性，会抛出异常。例如尝试 delete Object.prototype  \u0026#39;use strict\u0026#39;; delete Object.prototype; // throws a TypeError  object literal 中的所有属性名称应该是唯一的。反例 var x = { \u0026quot;x1\u0026quot;: 1, \u0026quot;x1\u0026quot;: 2 }  \u0026#39;use strict\u0026#39;; var o = { p: 1, p: 2 }; // !!! syntax error  函数参数的名字必须唯一。反例 function sum(x, x) {}  function sum(a, a, c) { // !!! syntax error  \u0026#39;use strict\u0026#39;; return a + a + c; // wrong if this code ran }  禁用八进制声明 var x = 023  \u0026#39;use strict\u0026#39;; var sum = 015 + // !!! syntax error  197 + 142;  禁用 with  \u0026#39;use strict\u0026#39;; var x = 17; with (obj) { // !!! syntax error  // If this weren\u0026#39;t strict mode, would this be var x, or  // would it instead be obj.x? It\u0026#39;s impossible in general  // to say without running the code, so the name can\u0026#39;t be  // optimized.  x; }  eval 不会产生新的变量 禁止 delete 普通变量。反例 delete x  \u0026#39;use strict\u0026#39;; var x; delete x; // !!! syntax error  Forbids binding or assignment of the names eval and arguments in any form Strict mode does not alias properties of the arguments object with the formal parameters. (i.e. in function sum (a,b) { return arguments[0] + b;} This works because arguments[0] is bound to a and so on. ) 不支持 arguments.callee  \u0026#39;use strict\u0026#39;; function fun(a, b) { \u0026#39;use strict\u0026#39;; var v = 12; return arguments.caller; // throws a TypeError } fun(1, 2); // doesn\u0026#39;t expose v (or a or b)  function restricted() { \u0026#39;use strict\u0026#39;; restricted.caller; // throws a TypeError  restricted.arguments; // throws a TypeError } 参考  Strict Mode What does “use strict” do in JavaScript, and what is the reasoning behind it?  "});index.add({'id':282,'href':'/docs/programmer-interview/front-end/implement-sleep/','title':"实现 sleep 函数",'content':"实现 sleep 函数 const sleep = (milliseconds) =\u0026gt; { return new Promise(resolve =\u0026gt; setTimeout(resolve, milliseconds)) } 如何使用：\nsleep(500).then(() =\u0026gt; { //do stuff }) 在 async 函数中使用：\nconst doSomething = async () =\u0026gt; { await sleep(2000) //do stuff } doSomething() "});index.add({'id':283,'href':'/docs/programmer-interview/front-end/js-copy/','title':"JS 深浅拷贝",'content':"JS 深浅拷贝 浅拷贝 ES6 Object assign var A1 = { a: \u0026#34;2\u0026#34; }; var A2 = Object.assign({}, A1); Object assign 的兼容写法 if (!Object.assign) { Object.defineProperty(Object, \u0026#39;assign\u0026#39;, { enumerable: false, configurable: true, writable: true, value: function(target) { \u0026#39;use strict\u0026#39;; if (target === undefined || target === null) { throw new TypeError(\u0026#39;Cannot convert first argument to object\u0026#39;); } var to = Object(target); for (var i = 1; i \u0026lt; arguments.length; i++) { var nextSource = arguments[i]; if (nextSource === undefined || nextSource === null) { continue; } nextSource = Object(nextSource); var keysArray = Object.keys(nextSource); for (var nextIndex = 0, len = keysArray.length; nextIndex \u0026lt; len; nextIndex++) { var nextKey = keysArray[nextIndex]; var desc = Object.getOwnPropertyDescriptor(nextSource, nextKey); if (desc !== undefined \u0026amp;\u0026amp; desc.enumerable) { to[nextKey] = nextSource[nextKey]; } } } return to; } }); } ES6 Spread Syntax var A1 = { a: \u0026#34;2\u0026#34; }; var A3 = { ...A1 }; // Spread Syntax 深拷贝 stringfy 假设你的对象只包含变量，没有 functions：\nvar newObject = JSON.parse(JSON.stringify(oldObject)); 使用 library  loadash.clonedeep angular.copy jQuery.extend(true, {}, oldObject)  "});index.add({'id':284,'href':'/docs/programmer-interview/front-end/implement-promise-all/','title':"实现 Promise.all",'content':"实现 Promise.all 借助 async/await Promise.all = async (promises) =\u0026gt; { const results = []; for (p of promises) { results.push(await p); } return results; } 不借助 async/await Promise.all = (promises) =\u0026gt; { let resolved = 0; let results = []; return new Promise((resolve, reject) =\u0026gt; { for (let promise of promises) { promise .then((result) =\u0026gt; { results.push(result); if (++resolved === promises.length) resolve(results); }) .catch((e) =\u0026gt; { reject(e); }); } }) } 参考  How to Implement Promise.all in JavaScript  "});index.add({'id':285,'href':'/docs/programmer-interview/front-end/implement-retry/','title':"实现 retry",'content':"实现 retry 只要没有 resolve，就一直 retry\n方法 const wait = ms =\u0026gt; new Promise(r =\u0026gt; setTimeout(r, ms)); const retryOperation = (operation, delay, times) =\u0026gt; new Promise((resolve, reject) =\u0026gt; { return operation() .then(resolve) .catch((reason) =\u0026gt; { if (times - 1 \u0026gt; 0) { return wait(delay) .then(retryOperation.bind(null, operation, delay, times - 1)) .then(resolve) .catch(reject); } return reject(reason); }); }); 如何使用：\n如果没有 resolve 或 reject，那么就每隔 1 秒重试一次，最多重试 5 秒：\nretryOperation(func, 1000, 5) .then(console.log) .catch(console.log); 参考  Promise Retry Design Patterns  "});index.add({'id':286,'href':'/docs/programmer-interview/algorithm/pancake-sorting/','title':"PancakeSorting",'content':"PancakeSorting 煎饼排序  微软\n // 微软面试题 // https://leetcode.com/problems/pancake-sorting/ // // https://www.1point3acres.com/bbs/forum.php?mod=viewthread\u0026amp;tid=518795\u0026amp;extra=page%3D1%26filter%3Dsortid%26sortid%3D327%26sortid%3D327 // 给出算法题，完成一个只能flip subarray的sort功能。 // 然后面试官提示看代码XX行，可以优化。 然后在她的提示下进行了两次优化。 // 然后要写flip的功能，时间来不及了，就匆匆用python的 reverse()完成了个简单的flip。 // 然后面试官说能不能不用reverse，我说可以，用forloop和stack就行了。她说可以用swap。 // 然后说时间到了，问我有没有想问的，我就问了能不能有更快的算法完成这个sort，她说没有了。 // // 看起来 flip 的意思是指只能从 0 ~ index 的位置整体翻转 // // Any valid answer that sorts the array within 10 * A.length flips will be judged as correct. // A[i] is a permutation of [1, 2, ..., A.length] public class PancakeSorting { public List\u0026lt;Integer\u0026gt; pancakeSort(int[] A) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); int end = A.length; while (end \u0026gt; 0) { int largestIndex = findLargestIndex(A, end); int largest = A[largestIndex]; // =====================  // 最大位于最后一个  // =====================  if (largestIndex == end - 1) { end--; continue; } // =====================  // 最大位于第一个的话，我们不用翻转  // =====================  if (largestIndex \u0026gt; 0) { flip(A, largestIndex); res.add(largestIndex + 1); } flip(A, end - 1); res.add(end); end--; } return res; } private int findLargestIndex(int[] A, int end/**exclusive */) { int index = 1; int maxIndex = 0; while (index \u0026lt; end) { if (A[index] \u0026gt; A[maxIndex]) { maxIndex = index; } index++; } return maxIndex; } private void flip(int[] A, int end/**index */) { int start = 0; while (start \u0026lt; end) { int t = A[start]; A[start] = A[end]; A[end] = t; start++; end--; } } } "});index.add({'id':287,'href':'/docs/programmer-interview/java/sentinel-vs-hystrix/','title':"Sentinel 与 Hystrix 的对比",'content':"Sentinel 与 Hystrix 的对比 对比     Sentinel Hystrix     隔离策略 信号量隔离 线程池隔离/信号量隔离   熔断降级策略 基于响应时间或失败比率 基于失败比率   实时指标实现 滑动窗口 滑动窗口（基于 RxJava）   规则配置 支持多种数据源 支持多种数据源   扩展性 多个扩展点 插件的形式   基于注解的支持 支持 支持   限流 基于 QPS，支持基于调用关系的限流 有限的支持   流量整形 支持慢启动、匀速器模式 不支持   系统负载保护 支持 不支持   控制台 开箱即用，可配置规则、查看秒级监控、机器发现等 不完善   常见框架的适配 Servlet、Spring Cloud、Dubbo、gRPC 等 Servlet、Spring Cloud Netflix    参考  Sentinel 与 Hystrix 的对比  "});index.add({'id':288,'href':'/docs/programmer-interview/front-end/cors/','title':"跨域",'content':"跨域 定义 跨域：指一个 domain 下的 HTML 或脚本试图去请求另一个 domain 下的资源。\n同源策略 同源策略 （Same origin policy）：两个 URL 的协议、域名、端口相同。\n同源限制访问资源  Cookie、LocalStorage 和 IndexDB 无法读取 DOM 和 Js 对象无法获得 AJAX 请求不能发送  JSONP \u0026lt;script\u0026gt; var script = document.createElement(\u0026#39;script\u0026#39;); script.type = \u0026#39;text/javascript\u0026#39;; // 传参一个回调函数名给后端，方便后端返回时执行这个在前端定义的回调函数  script.src = \u0026#39;http://www.domain2.com:8080/login?user=admin\u0026amp;callback=handleCallback\u0026#39;; document.head.appendChild(script); // 回调执行函数  function handleCallback(res) { alert(JSON.stringify(res)); } \u0026lt;/script\u0026gt; 后端返回的内容如下所示，即返回后立即执行 handleCallback：\nhandleCallback({\u0026#34;status\u0026#34;: true, \u0026#34;user\u0026#34;: \u0026#34;admin\u0026#34;})  JSONP 只能进行 GET 调用\n 跨域资源共享 (CORS) 服务端设置 Access-Control-Allow-Origin:* 头即可，前端无须设置。如果想要进行 cookie 的读写，那么前端需要设置这个属性为 true：\nvar xhr = new XMLHttpRequest(); // 前端开关：浏览器是否读写cookie xhr.withCredentials = true; 预请求 (preflight) （1）为什么需要预请求\n为了让浏览器知道，它所请求的服务器是一个对 CORS 有感知 (CORS-aware) 的服务器，也就是说让浏览器知道这个服务器是能够对 CORS 做出正确处理的服务器。\n（2）触发方式\n浏览器发送 OPTIONS 请求。\n（3）触发 规则\n 简单请求（GET、HEAD、POST）并且没有设置 force preflight flag 位，那么不触发  WebSocket WebSocket 协议允许跨域通信。\n心跳怎么实现的 任意一方，都可以发送 ping/pong control frame 来检测是否还处于连接状态。\npostMessage 这是 HTML5 XMLHttpRequest Level 2 引入的新的 API，可以用于前端多个打开的窗口之间的数据传递。\nA 网站发送消息给 B 网站 \u0026lt;iframe src=\u0026#34;http://a.com\u0026#34; name=\u0026#34;a\u0026#34;\u0026gt; \u0026lt;script\u0026gt; let a = window.frames.a; a.postMessage(\u0026#34;message\u0026#34;, \u0026#34;http://b.com\u0026#34;); \u0026lt;/script\u0026gt; B 网站接受消息 window.addEventListener(\u0026#34;message\u0026#34;, function(event) { if (event.origin != \u0026#39;http://a.com\u0026#39;) { // something from an unknown domain, let\u0026#39;s ignore it  return; } alert( \u0026#34;received: \u0026#34; + event.data ); // can message back using event.source.postMessage(...) }); 代理 直接访问 login 和 api 域名 通过代理访问 login 和 api 域名 代理服务器可以设置上 Access-Control-Allow-Origin:* 这个响应头，taobao.com 需要访问 api.taobao.com 和 login.taobao.com 上的资源，就请求代理服务器去访问，代理服务器作为中间人，在中间帮助转发请求和响应。\n通过代理访问资源，需要重写 URL 和进行 URL 访问映射：\n 之前访问的是 api.taobao.com，现在改为访问 proxy.taobao.com/api，代理服务器收到这个请求后，再去请求 api.taobao.com，并将响应转发给 taobao.com 请求 login.taobao.com 也是同样的道理，同样可以将 URL 改为 proxy.taobao.com/login  这个代理服务器，可以是流行的 Nginx 代理服务器，也可以是其他的用于转发请求和响应的服务器，这个代理服务器可以拦截流量，可以添加自定义头，可以统计信息等，总之针对浏览器限制的行为在服务器端是不受限制的。\n 跨域是浏览器的行为，不是服务器的行为。\n 参考  Cross-window communication CORS - What is the motivation behind introducing preflight requests? Cross-Origin Resource Sharing (CORS)  "});index.add({'id':289,'href':'/docs/programmer-interview/algorithm/heap-sort/','title':"堆排序",'content':"堆排序 // - 建堆，不同节点运行 heapify 的时间与节点所处的高度相关 // https://www.cnblogs.com/LzyRapx/p/9565305.html // // 树高 h = lgn // // 第 0 层，只有根节点，它需要最多向下调整 h * 2^0 次 // 第 1 层，有 2 个节点，它需要向下调整 (h - 1) * 2^1 次 // 第 2 层，有 4 个节点，它需要向下调整 (h - 2) * 2^2 次 // // 第 h 层，有 2^h 个节点，它需要向下调整 (h - h) * 2^h 次 // // O(n) = h * 2^0 + (h - 1) * 2^1 + (h - 2) * 2^2 + ... + (h - h) * 2^h // ① ② ③ // O(n) = ∑((h - i) * 2^i), i: 0 -\u0026gt; h // 将上式乘以 2 // // 2O(n) = 0 + h * 2^1 + (h - 1) * 2^2 + (h - 2) * 2^3 + ... + (h - h) * 2^(h + 1) // ① ② ③ ④ // // 按照 ①、②、③、依次减 // 2O(n) - O(n) = -h + 2^1 + 2^2 + 2^3 + ... + 2^h = -h + 2 * ((1 - 2^h) / (1 - 2)) = 2 ^ (h + 1) - 2 - h // 所以复杂度就是 2 ^ (h + 1) - 2 - h // // 总结点数 N = 2^0 + 2^1 + 2^2 + ... + 2^h = 1 * ((1 - 2^(h + 1)) / (1 - 2)) = 2 ^ (h + 1) - 1 ≈ 2 ^ (h + 1) // // 所以复杂度 2 ^ (h + 1) - 2 - h ≈ N - 2 - h = N - 2 - lgn = O(N) public class HeapSort { // 1. 建立最大堆  // 2. 堆顶元素是最大的，放在末尾  // 3. 末尾的放在堆顶，重新进行调整  public void sort(int[] array) { // 建堆的时间复杂度 O(n)  //  // n 个节点高度 logn  // 最后一层，每个父节点最多下调一次  // 倒数第二层，每个父节点最多下调两次  // ...  // 只有根节点需要比较 logn 次  //  // 建立最大堆，小的都下沉了  for (int i = array.length / 2; i \u0026gt;= 0; i--) { heapify(array, array.length, i); } // O(N) * O(logN) = O(nlogn)  for (int i = array.length - 1; i \u0026gt;= 0; i--) { swap(array, 0, i); heapify(array, i, 0); } } private void heapify(int[] array, int heapSize, int rootIndex) { int largest = rootIndex; int leftChildIndex = rootIndex * 2; int rightChildIndex = rootIndex * 2 + 1; if (leftChildIndex \u0026lt; heapSize \u0026amp;\u0026amp; array[leftChildIndex] \u0026gt; array[largest]) { largest = leftChildIndex; } if (rightChildIndex \u0026lt; heapSize \u0026amp;\u0026amp; array[rightChildIndex] \u0026gt; array[largest]) { largest = rightChildIndex; } // =====================  // 将 8 和 3 的位置进行交换，即 largest 一直和自己的两个孩子比较  //  // 3  // / \\  // 6 8  //  // =====================  if (largest != rootIndex) { swap(array, largest, rootIndex); heapify(array, heapSize, largest); } } private void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; } } "});index.add({'id':290,'href':'/docs/programmer-interview/front-end/throttle-and-debounce/','title':"节流和防抖",'content':"节流和防抖 作用 解决页面卡顿等性能问题\n节流函数 一定时间内，某个函数只执行一次。By using throttle function, we don\u0026rsquo;t allow to our function to execute more than once every X milliseconds.\nfunction throttle (callback, limit) { var waiting = false; // 一开始，处于非等待状态  return function () { // 返回一个节流函数  if (!waiting) { // 如果没有等待  callback.apply(this, arguments); // 执行函数  waiting = true; // 等待置位 true  setTimeout(function () { // limit 时间之后  waiting = false; // 重新置位 false  }, limit); } } }  callback 函数：哪个函数需要节流？ limit：多长时间之后可以重新调用  也可以基于时间判断来实现：\nfunction throttle(callback, limit) { let lastTime = 0; return function() { let now = new Date(); if (now - lastTime \u0026gt;= limit) { callback(); lastTime = now; } } } 节流应用场景：\n 滚动加载 表单重复提交 搜索联想功能  防抖 频繁触发时，不执行函数，等最后一次触发延迟一定时间再执行函数，前面的 n 次触发都被忽略掉。（The Debounce technique allow us to \u0026ldquo;group\u0026rdquo; multiple sequential calls in a single one.）常用于：防止按钮频繁点击，比如发送验证码的按钮之类的。\nconst debounce = (callback, delay = 250) =\u0026gt; { let timeoutId; return (...args) =\u0026gt; { clearTimeout(timeoutId); timeoutId = setTimeout(() =\u0026gt; { timeoutId = null; callback(...args); }) } } 防抖应用场景：\n 搜索框搜索输入（注意这和上面的搜索联想是不同的） 手机号、邮箱验证检测输入 窗口大小 Resize  "});index.add({'id':291,'href':'/docs/programmer-interview/algorithm/linkedlist-mergesort/','title':"链表归并排序",'content':"链表归并排序 // ============================= // MergeSort // // 切分为三部分: // - small // - equal // - large // ============================= public class SortList { public ListNode sortList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode lessOrEqualThanPivot = new ListNode(-1); ListNode pivot = head; ListNode greatThanPivot = new ListNode(-1); ListNode equalPivot = new ListNode(-1); ListNode p = head; ListNode pLess = lessOrEqualThanPivot; ListNode pGreat = greatThanPivot; ListNode pEqual = equalPivot; while (p != null) { if (p.val \u0026lt; pivot.val) { pLess.next = p; pLess = pLess.next; } else if (p.val == pivot.val) { pEqual.next = p; pEqual = pEqual.next; } else { pGreat.next = p; pGreat = pGreat.next; } p = p.next; } pEqual.next = null; pLess.next = null; pGreat.next = null; return merge(merge(sortList(lessOrEqualThanPivot.next), equalPivot.next), sortList(greatThanPivot.next)); } private ListNode merge(ListNode less, ListNode great) { if (less == null) { return great; } if (great == null) { return less; } ListNode last = findLastNode(less); last.next = great; return less; } private ListNode findLastNode(ListNode root) { ListNode p = root; while (p != null \u0026amp;\u0026amp; p.next != null) { p = p.next; } return p; } } "});index.add({'id':292,'href':'/docs/programmer-interview/algorithm/quicksort/','title':"快排序",'content':"快排序 // T(n) = T(n - 1) + T(0)，每次都分为 n - 1 个和 0 个元素 // T(n - 1) = T(n - 2) + T(0) // ... // 迭代想加 // T(n) = O(n^2) // // - 最坏情况: // T(n) = 2T(n / 2) + O(n) // T(n / 2) = 2T(n / 4) + O(n) // // 画出树，整颗树高 log2^n 然后每次都是 O(n) 所以 nlogn // // n.................O(n) // n/2 n/2.............O(n) // n/4 n/4 n/4 n/4..........O(n) // public class QuickSort { public void sort(int[] array) { sort(array, 0, array.length - 1); } private void sort(int[] array, int begin, int end) { if (begin \u0026lt; end) { int index = partition(array, begin, end); // ================================  // begin ~ index - 1  // ================================  sort(array, begin, index - 1); // ================================  // index + 1 ~ end  // ================================  sort(array, index + 1, end); } } private int partition(int[] array, int begin, int end) { int i = begin - 1; int pivot = array[end]; for (int j = begin; j \u0026lt; end; j++) { if (array[j] \u0026lt;= pivot) { i++; swap(array, i, j); } } swap(array, i + 1, end); // 返回的是 pivot 所在的索引  return i + 1; } private void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; } } "});index.add({'id':293,'href':'/docs/programmer-interview/algorithm/mergesort/','title':"归并排序",'content':"归并排序 public class MergeSort { public void sort(int[] array) { sort(array, 0, array.length - 1); } private void sort(int[] array, int begin, int end) { if (end \u0026gt; begin) { int middle = begin + ((end - begin) \u0026gt;\u0026gt; 1); // ===========================  // begin ~ middle  // ===========================  sort(array, begin, middle); // ===========================  // middle + 1 ~ end  // ===========================  sort(array, middle + 1, end); merge(array, begin, middle, end); } } private void merge(int[] array, int begin, int middle, int end) { int[] temp = new int[end - begin + 1]; int i = begin; int j = middle + 1; int k = 0; while (i \u0026lt;= middle \u0026amp;\u0026amp; j \u0026lt;= end) { if (array[i] \u0026lt; array[j]) { temp[k++] = array[i++]; } else { temp[k++] = array[j++]; } } while (i \u0026lt;= middle) { temp[k++] = array[i++]; } while (j \u0026lt;= end) { temp[k++] = array[j++]; } k = 0; while (k \u0026lt; temp.length) { array[begin++] = temp[k++]; } } } "});index.add({'id':294,'href':'/docs/programmer-interview/algorithm/sort-stack/','title':"栈排序",'content':"栈排序 // 美团面试题: https://www.nowcoder.com/discuss/268612?type=2\u0026amp;order=0\u0026amp;pos=49\u0026amp;page=1 public class SortStack { public static void stackSorting(Stack\u0026lt;Integer\u0026gt; stack) { Stack\u0026lt;Integer\u0026gt; t = new Stack\u0026lt;\u0026gt;(); while(!stack.isEmpty()) { int item = stack.pop(); while(!t.isEmpty() \u0026amp;\u0026amp; t.peek() \u0026gt; item) stack.push(t.pop()); t.push(item); } while(!t.isEmpty()) stack.push(t.pop()); } } "});index.add({'id':295,'href':'/docs/programmer-interview/algorithm/disk-merge-sort/','title':"磁盘多路归并排序",'content':"磁盘多路归并排序 // 磁盘多路归并排序 // https://mp.weixin.qq.com/s?__biz=MzI0MzQyMTYzOQ==\u0026amp;mid=2247484900\u0026amp;idx=1\u0026amp;sn=a120748f4c1229dbb851732e4cd5f47c\u0026amp;pass_ticket=TEG93hpjf7gdkxzIPcHay9NH%2FprQkpCtcZYYI4NcTDeNiCpFQAsK%2Bh4x9M1mviQ8 // // 1 2 3 4 5 (文件 1 2 3 4 5) // ---------- // 2 5 5 7 12 // 3 6 5 8 13 // 4 6 6 9 13 // // 内存中维护的数组变化，方括号内代表这个数字属于哪个文件 // 2[1] 5[2] 5[3] 7[4] 12[5] // 3[1] 5[2] 5[3] 7[4] 12[5] // 4[1] 5[2] 5[3] 7[4] 12[5] // 5[2] 5[3] 6[2] 7[4] 12[5] // 5[3] 6[2] 6[2] 7[4] 12[5] // ... import java.io.BufferedReader; import java.io.Closeable; import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.FileReader; import java.io.IOException; import java.io.PrintWriter; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.concurrent.ThreadLocalRandom; public class DiskMergeSort implements Closeable { public static List\u0026lt;String\u0026gt; generateFiles(int n, int minEntries, int maxEntries) { List\u0026lt;String\u0026gt; files = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { String filename = \u0026#34;input-\u0026#34; + i + \u0026#34;.txt\u0026#34;; PrintWriter writer; try { writer = new PrintWriter(new FileOutputStream(filename)); int entries = ThreadLocalRandom.current().nextInt(minEntries, maxEntries); List\u0026lt;Integer\u0026gt; nums = new ArrayList\u0026lt;\u0026gt;(); for (int k = 0; k \u0026lt; entries; k++) { int num = ThreadLocalRandom.current().nextInt(10000000); nums.add(num); } Collections.sort(nums); for (int num : nums) { writer.println(num); } writer.close(); } catch (FileNotFoundException e) { } files.add(filename); } return files; } private List\u0026lt;MergeSource\u0026gt; sources; private MergeOut out; public DiskMergeSort(List\u0026lt;String\u0026gt; files, String outFilename) { this.sources = new ArrayList\u0026lt;\u0026gt;(); for (String filename : files) { this.sources.add(new MergeSource(filename)); } this.out = new MergeOut(outFilename); } static class MergeOut implements Closeable { private PrintWriter writer; public MergeOut(String filename) { try { this.writer = new PrintWriter(new FileOutputStream(filename)); } catch (FileNotFoundException e) { } } public void write(Bin bin) { writer.println(bin.num); } @Override public void close() throws IOException { writer.flush(); writer.close(); } } // 这里假设了: 输入文件是有序的  static class MergeSource implements Closeable { private BufferedReader reader; private String cachedLine; public MergeSource(String filename) { try { FileReader fr = new FileReader(filename); this.reader = new BufferedReader(fr); } catch (FileNotFoundException e) { } } public boolean hasNext() { String line; try { line = this.reader.readLine(); if (line == null || line.isEmpty()) { return false; } this.cachedLine = line.trim(); return true; } catch (IOException e) { } return false; } public int next() { if (this.cachedLine == null) { if (!hasNext()) { throw new IllegalStateException(\u0026#34;no content\u0026#34;); } } int num = Integer.parseInt(this.cachedLine); this.cachedLine = null; return num; } @Override public void close() throws IOException { this.reader.close(); } } static class Bin implements Comparable\u0026lt;Bin\u0026gt; { int num; MergeSource source; Bin(MergeSource source, int num) { this.source = source; this.num = num; } @Override public int compareTo(Bin o) { return this.num - o.num; } } public List\u0026lt;Bin\u0026gt; prepare() { List\u0026lt;Bin\u0026gt; bins = new ArrayList\u0026lt;\u0026gt;(); for (MergeSource source : sources) { Bin newBin = new Bin(source, source.next()); bins.add(newBin); } // 先把每隔文件的第一个数字放到这个数组里面  // 先排第一次序  //  Collections.sort(bins); return bins; } public void sort() { // 内存维护一个数组  List\u0026lt;Bin\u0026gt; bins = prepare(); //  // 2 | 7 | 15 | 30 | 36  // file-1 | file-2 | file-3 | file-4 | file-5  // ↑  // current  //  while (true) { MergeSource current = bins.get(0).source; if (current.hasNext()) { Bin newBin = new Bin(current, current.next()); int index = Collections.binarySearch(bins, newBin); // 如果取出来的元素和当前数组中的最小元素相等，那么就可以直接将这个元素输出。  // 再继续下一轮循环。  // 不可能取出比当前数组最小元素还要小的元素，因为输入文件本身也是有序的。  if (index == 0 || index == -1) { this.out.write(newBin); if (index == -1) { throw new IllegalStateException(\u0026#34;impossible\u0026#34;); } } else { if (index \u0026lt; 0) { index = -index - 1; } // 否则就需要将元素插入到当前的数组中的指定位置，继续保持数组有序。  // 然后将数组中当前最小的元素输出并移除。  bins.add(index, newBin); Bin minBin = bins.remove(0); this.out.write(minBin); } } else { // 遇到文件尾  Bin minBin = bins.remove(0); this.out.write(minBin); if (bins.isEmpty()) { break; } } } } @Override public void close() throws IOException { for (MergeSource source : sources) { source.close(); } this.out.close(); } public static void main(String[] args) throws IOException { List\u0026lt;String\u0026gt; inputs = DiskMergeSort.generateFiles(100, 10000, 20000); // 运行多次看算法耗时  for (int i = 0; i \u0026lt; 20; i++) { DiskMergeSort sorter = new DiskMergeSort(inputs, \u0026#34;output.txt\u0026#34;); long start = System.currentTimeMillis(); sorter.sort(); long duration = System.currentTimeMillis() - start; System.out.printf(\u0026#34;%dms\\n\u0026#34;, duration); sorter.close(); } } } "});index.add({'id':296,'href':'/docs/programmer-interview/algorithm/evaluate-reverse-polish-notation/','title':"求解逆波兰表达式",'content':"求解逆波兰表达式 - 写一个计算器  微软\n // https://leetcode.com/problems/evaluate-reverse-polish-notation/submissions/ // // Input: [\u0026#34;2\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;+\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;*\u0026#34;] // Output: 9 // Explanation: ((2 + 1) * 3) = 9 // // Input: [\u0026#34;4\u0026#34;, \u0026#34;13\u0026#34;, \u0026#34;5\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;+\u0026#34;] // Output: 6 // Explanation: (4 + (13 / 5)) = 6 // // 微软面试题: 写一个计算器 // 这个原题，给你的已经是一个逆波兰表达式了 public class EvaluateReversePolishNotation { public int evalRPN(String[] tokens) { Stack\u0026lt;Integer\u0026gt; numStack = new Stack\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; tokens.length; i++) { String str = tokens[i]; if (isSign(str)) { int right = numStack.pop(); int left = numStack.pop(); switch (str.charAt(0)) { case \u0026#39;+\u0026#39;: numStack.push(left + right); break; case \u0026#39;-\u0026#39;: numStack.push(left - right); break; case \u0026#39;*\u0026#39;: numStack.push(left * right); break; case \u0026#39;/\u0026#39;: numStack.push(left / right); break; } } else { numStack.push(Integer.parseInt(str)); } } return numStack.pop(); } private boolean isSign(String str) { char c = str.charAt(0); return str.length() == 1 \u0026amp;\u0026amp; (c == \u0026#39;+\u0026#39; || c == \u0026#39;-\u0026#39; || c == \u0026#39;*\u0026#39; || c == \u0026#39;/\u0026#39;); } } "});index.add({'id':297,'href':'/docs/programmer-interview/front-end/cache/','title':"浏览器缓存",'content':"浏览器缓存 强缓存 HTTP 1.0 Expires Expires: Wed, 11 May 2018 07:20:00 GMT  缺点：时间是绝对时间，很难保证用户计算机时间和服务器时间一致\n HTTP 1.1 Cache-Control Cache-Control: max-age=315360000  优先级高于 Expires\n Cache-Control 取值如下：\n no-store：不缓存到本地 public：多用户共享 private：只能被终端浏览器缓存 no-cache：缓存到本地，但是使用这个缓存之前，必须与服务器进行新鲜度验证  （1）禁用缓存\nCache-Control: no-store （2）缓存静态资源\nCache-Control: public, max-age=604800, immutable （3）重新校验资源\nCache-Control: no-cache Cache-Control: no-cache, max-age=0 协商缓存 当浏览器对某个资源的请求没有命中强缓存，就会发一个请求到服务器，验证协商缓存是否命中，如果协商缓存命中，请求响应返回的 HTTP 状态为 304 并且会显示一个 Not Modified 的字符串。\n那么浏览器如何询问服务器？\nIf-Modified-Since 浏览器请求服务器的时候带上这个头 If-Modified-Since，它的值是这个文件上一次服务器返回来的时候携带的 Last-Modified 的 HTTP 头的值。如果服务器有新的资源，那么会返回新的资源，否则响应 304。\nIf-None-Match If-Modified-Since 是根据文件的修改时间定的，而 If-None-Match 携带的值是这个文件的指纹，即上一次服务器返回这个文件携带的 ETag HTTP 头的值。浏览器将这个信息发送给服务器，可以更为精确地知道这个文件有没有变化。如果服务器有新的资源，返回新的，否则响应 304 状态码。\n ETag 优先级高于 Last-Modified\n "});index.add({'id':298,'href':'/docs/programmer-interview/algorithm/minstack/','title':"MinStack",'content':"MinStack 使用两个栈 // 使用了两个栈 // 占用内存大 // // 使用一个栈的解法见 MinStack_Solution_1.java public class MinStack { private Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;Integer\u0026gt;(); private Stack\u0026lt;Integer\u0026gt; minStack = new Stack\u0026lt;Integer\u0026gt;(); /** initialize your data structure here. */ public MinStack() { } public void push(int x) { stack.push(x); if (minStack.isEmpty()) { minStack.push(x); } else { minStack.push(x \u0026lt; minStack.peek() ? x : minStack.peek()); } } public void pop() { stack.pop(); minStack.pop(); } public int top() { return stack.peek(); } public int getMin() { return minStack.peek(); } } 使用一个栈 // 使用一个栈，了解一下吧 // // [5,3,7,8,4,6,2,12] // // |12| // --- // |2| 2 push 之前，的最小值是继续 pop() 一次的值，即 3 // --- // |3| // |6| // |4| // |8| // |7| // --- // |3| 3 push 之前，的最小值是继续 pop() 一次的值，即 5 // |5| // --- // |5| // |Integer.MIN_VALUE| public class MinStack_Solution_1 { // ==================  // 数据大概这么分组  //  // |42|  //  // |x(min) = 4|  // |min = 4|  //  // |6|  // |10|  // |9|  // |7|  //  // |x(min) = 5|  // |min = 5|  // ==================  int min = Integer.MAX_VALUE; Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;Integer\u0026gt;(); // 核心思想：  //  // push 的数，小于等于，已有的 min  // 那么先 push 一下已有的 min  // 然后再正常 push  public void push(int x) { // only push the old minimum value when the current  // minimum value changes after pushing the new value x  if (x \u0026lt;= min) { stack.push(min); min = x; } stack.push(x); } public void pop() { // if pop operation could result in the changing of the current minimum value,  // pop twice and change the current minimum value to the last minimum value.  if (stack.pop() == min) min = stack.pop(); } public int top() { return stack.peek(); } public int getMin() { return min; } } "});index.add({'id':299,'href':'/docs/programmer-interview/front-end/eventloop/','title':"事件循环",'content':"事件循环 Event Loop JS 是单线程的。在 JavaScript 引擎里，取 task 和执行 task 的代码封装在一个死循环里面，JavaScript 引擎等待 tasks 的出现，有则执行，无则 sleep。异步任务分为宏任务和微任务。\nMacro Task 宏任务 宏任务示例：\u0026lt;script\u0026gt;、setTimeout、setInterval、setImmediate、requestAnimationFrame、I/O、UI 渲染\nMicro Task 微任务 微任务示例：process.nextTick、Promises、queueMicrotask、MutationObserver\n Micro is like macro but with higher priority.\n Event Loop 算法 while (true) { // 执行宏任务  let task = macroTaskQueue.getOldestTask(); execute(task); // 执行微任务  while (microTaskQueue.length \u0026gt; 0) { execute(microTaskQueue.getOldestTask()) } // 渲染  if (isRenderTime()) { render(); } } Node EventLoop vs 浏览器 EventLoop  microtask 的执行时机不同。\n Node.js 的 EventLoop 分为 6 个阶段：\n timers: 执行 setTimeout、setInterval I/O callbacks: 处理上一轮剩下来的少数未执行的 I/O 回调 idle, prepare: Node 内部使用 poll 阶段：获取新的 I/O 事件, 适当的条件下 node 将阻塞在这里 check 阶段：执行 setImmediate() 的回调 close callbacks 阶段：执行 socket 的 close 事件回调  浏览器环境下，microtask 的任务队列是每个 macrotask 执行完之后执行。而在 Node.js 中，microtask 会在事件循环的各个阶段之间执行，也就是一个阶段执行完毕，就会去执行 microtask 队列的任务:\n参考  浏览器与Node的事件循环(Event Loop)有何区别?  "});index.add({'id':300,'href':'/docs/programmer-interview/front-end/storage/','title':"浏览器存储",'content':"浏览器存储 Web Storage  Web Storage 也叫 DOM Storage.\n 浏览器提供了两种支持 Web Storage 的对象：\n window.localStorage：数据没有过期时间 window.sessionStorage：仅仅会话期间有效，关闭当前浏览器 Tab 页面的时候，数据消失  注意，这两个 Storage 对象存储的 value 只支持 String 类型，你存储其他类型，浏览器也会自动转为 String 类型存储进去。\nsessionStorage.setItem(\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;) localStorage.setItem(\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;) 存储大小  Cookie 允许 4KB Opera 10.50+ 允许 5MB Safari 8 允许 5MB Firefox 34 允许 10MB Chrome 允许 10MB IE 允许 10MB  数据可见性 （1）LocalStorage\n只有相同协议、相同 Host、相同端口，这三个都相同，才能算作是同一个 Origin。 只要两个页面处于同一 Origin ，那么存储在这一 Origin 的 LocalStorage 数据便可以自由访问。\n（2）SessionStorage\n而 SessionStorage 除了需要同一 Origin，还需要同一 Tab 才可以。\nWebSQL Google Chrome、Opera、Android 浏览器支持。\nCookie Cookie\nIndexedDB 当数据量比较大的时候，可以使用 IndexedDB 来存储。\n"});index.add({'id':301,'href':'/docs/programmer-interview/front-end/dom-operate-api/','title':"DOM 操作 API",'content':"JavaScript 常见原生 DOM 操作 API 总结 节点查找 API    方法 示例 描述     querySelector var el = document.querySelector(\u0026quot;.myclass\u0026quot;); 返回第一个匹配 selector 的元素   querySelectorAll var matches = document.querySelectorAll(\u0026quot;p\u0026quot;); 返回一个匹配的 NodeList 数组   getElementById var elem = document.getElementById('para'); 返回匹配 ID 的 Element   getElementsByClassName document.getElementsByClassName('red test') 返回匹配 class 的一个 array-like 的元素数组   getElementsByTagName var allParas = document.getElementsByTagName('p'); 返回指定 Tag 的 HTMLCollection   getElementsByName var up_names = document.getElementsByName(\u0026quot;up\u0026quot;); 返回一个匹配元素 name 属性的 NodeList    创建节点 API    方法 示例 描述     createElement const newDiv = document.createElement(\u0026quot;div\u0026quot;); 创建一个新的元素   createTextNode var newtext = document.createTextNode(\u0026quot;hello\u0026quot;) 创建一个新的文本节点   createDocumentFragment var fragment = document.createDocumentFragment(); 创建文档片段    修改节点 API    方法 描述     appendChild 添加一个新的节点到这个节点的孩子列表的末尾   insertBefore 在指定节点之前添加一个节点   removeChild 移除某个孩子节点   replaceChild 替换某个孩子节点    父子/兄弟节点 API    方法 描述     Node.parentNode 返回父节点   ParentNode.children 返回孩子节点的 HTMLCollection   Node.firstChild 第一个孩子   Node.lastChild 最后一个孩子   Node.nextSibling 节点的下一个节点   Node.previousSibling 节点的上一个节点    属性 API    方法 描述     Element.setAttribute(name, value) 设置元素属性   Element.getAttribute(name) 获取元素属性    参考  Document  "});index.add({'id':302,'href':'/docs/programmer-interview/front-end/event/','title':"Event",'content':"Event target 和 currentTarget  target：哪个元素触发了事件？ currentTarget：哪个元素监听了这个事件  事件代理/委托 原因 添加的事件监听器数量，影响页面的整体运行性能，因为访问 DOM 的次数越多，引起浏览器的重绘和重排的次数也就越多。通过事件委托，可以减少添加的事件监听器数量，提高页面性能。\n原理 利用事件冒泡。\n示例 \u0026lt;ul id=\u0026#34;list\u0026#34;\u0026gt; \u0026lt;li id=\u0026#34;item1\u0026#34;\u0026gt;item1\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;item2\u0026#34;\u0026gt;item2\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;item3\u0026#34;\u0026gt;item3\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;item4\u0026#34;\u0026gt;item4\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 目的：点击 li 然后弹出这个 li 的内容：\nwindow.onload=function(){ var ulNode=document.getElementById(\u0026#34;list\u0026#34;); ulNode.addEventListener(\u0026#39;click\u0026#39;,function(e){ if(e.target \u0026amp;\u0026amp; e.target.nodeName.toUpperCase() == \u0026#34;LI\u0026#34;){/*判断目标事件是否为li*/ alert(e.target.innerHTML); } },false); }; "});index.add({'id':303,'href':'/docs/programmer-interview/front-end/event-bubble-capture/','title':"冒泡捕获",'content':"冒泡捕获 冒泡 \u0026lt;style\u0026gt; body * { margin: 10px; border: 1px solid blue; } \u0026lt;/style\u0026gt; \u0026lt;form onclick=\u0026#34;alert(\u0026#39;form\u0026#39;)\u0026#34;\u0026gt;FORM \u0026lt;div onclick=\u0026#34;alert(\u0026#39;div\u0026#39;)\u0026#34;\u0026gt;DIV \u0026lt;p onclick=\u0026#34;alert(\u0026#39;p\u0026#39;)\u0026#34;\u0026gt;P\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;  冒泡：点击 p 元素，首先 alert p，其次 alert div，最后 alert form。从最内层的元素冒泡到最外层的元素。\n 阻止冒泡 stopPropagation 阻止向父元素冒泡：\nevent.stopPropagation() 示例：\n\u0026lt;body onclick=\u0026#34;alert(`the bubbling doesn\u0026#39;t reach here`)\u0026#34;\u0026gt; \u0026lt;button onclick=\u0026#34;event.stopPropagation()\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; \u0026lt;/body\u0026gt; 阻止冒泡 stopImmediatePropagation 阻止向父元素冒泡，并且当前元素绑定的其它事件也不会执行：\nevent.stopImmediatePropagation() 举例：\n$(\u0026#39;p\u0026#39;).click(event =\u0026gt; event.stopImmediatePropagation()) $(\u0026#39;p\u0026#39;).click(event =\u0026gt; console.log(\u0026#39;这个事件不会执行\u0026#39;)) 但是如果你调整一下顺序，这个事件就会执行了：\n$(\u0026#39;p\u0026#39;).click(event =\u0026gt; console.log(\u0026#39;这个事件会执行\u0026#39;)) $(\u0026#39;p\u0026#39;).click(event =\u0026gt; event.stopImmediatePropagation()) 捕获 事件传播的三个阶段 DOM Events 描述了事件传播的三个阶段：\n 捕获阶段 (事件从 HTML 方向一层一层地向目标元素方向传递) 目标阶段 (事件到达目标元素) 冒泡阶段 (事件从目标元素向 HTML 方向冒泡)  addEventListener 为 elem 添加捕获事件监听：\nelem.addEventListener(..., {capture: true}) // or, just \u0026#34;true\u0026#34; is an alias to {capture: true} elem.addEventListener(..., true)  如果 capture 值是 false，那么会作为冒泡事件监听。\n 移除事件监听器，同样需要对应的参数。如果你是 addEventListener(..., true) 这样添加的事件，那么\tremoveEventListener(..., true) 需要这样移除。\n示例 \u0026lt;style\u0026gt; body * { margin: 10px; border: 1px solid blue; } \u0026lt;/style\u0026gt; \u0026lt;form\u0026gt;FORM \u0026lt;div\u0026gt;DIV \u0026lt;p\u0026gt;P\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; for(let elem of document.querySelectorAll(\u0026#39;*\u0026#39;)) { elem.addEventListener(\u0026#34;click\u0026#34;, e =\u0026gt; alert(`Capturing: ${elem.tagName}`), true); elem.addEventListener(\u0026#34;click\u0026#34;, e =\u0026gt; alert(`Bubbling: ${elem.tagName}`)); } \u0026lt;/script\u0026gt; 点击 p 元素，网页依次 alert 的是：\n Capturing: HTML -\u0026gt; Capturing: BODY -\u0026gt; Capturing: FORM -\u0026gt; Capturing: DIV -\u0026gt; Capturing: P Bubbling: P -\u0026gt; Bubbling: DIV -\u0026gt; Bubbling: FORM -\u0026gt; Bubbling: BODY -\u0026gt; Bubbling: HTML  参考  Bubbling and capturing  "});index.add({'id':304,'href':'/docs/programmer-interview/front-end/repaint-reflow/','title':"重绘和重排",'content':"重绘和重排 网页渲染  \u0026ldquo;生成布局\u0026rdquo;（flow）和\u0026quot;绘制\u0026rdquo;（paint）这两步，合称为\u0026quot;渲染\u0026rdquo;（render）。\n 重绘 repint 重绘：某种操作改变了某个元素的外观，但并未改变这个元素的布局，从而需要重新绘制。例如对 outline、visibility、background、color 的改变。重绘不一定会引起重排。\n重排/回流 reflow 重排/回流：某种操作改变了某个元素、网页的一部分或整个网页的布局，其对于性能的影响更为严重。重排必会导致重绘。\n重排触发机制 重排发生的根本原理就是元素的几何属性发生了改变，那么我们就从能够改变元素几何属性的角度入手\n 添加或删除可见的 DOM 元素 元素位置改变 元素本身的尺寸发生改变 内容改变 页面渲染器初始化 浏览器窗口大小发生改变  如何优化 浏览器自身优化 现代浏览器大多都是通过队列机制来批量更新布局，浏览器会把修改操作放在队列中，至少一个浏览器刷新（即16.6ms）才会清空队列，但当你获取布局信息的时候，队列中可能有会影响这些属性或方法返回值的操作，即使没有，浏览器也会强制清空队列，触发回流与重绘来确保返回正确的值。\n主要包括以下属性或方法：\noffsetTop、offsetLeft、offsetWidth、offsetHeight scrollTop、scrollLeft、scrollWidth、scrollHeight clientTop、clientLeft、clientWidth、clientHeight width、height getComputedStyle() getBoundingClientRect() 所以，我们应该避免频繁的使用上述的属性，他们都会强制渲染刷新队列。\n合并读操作/写操作 不要这样：\n// bad div.style.left = div.offsetLeft + 10 + \u0026#34;px\u0026#34;; div.style.top = div.offsetTop + 10 + \u0026#34;px\u0026#34;; 读写分离合并：\n// good var left = div.offsetLeft; var top = div.offsetTop; div.style.left = left + 10 + \u0026#34;px\u0026#34;; div.style.top = top + 10 + \u0026#34;px\u0026#34;; 缓存 如果某个样式是通过重排得到的，那么最好缓存结果。避免下一次用到的时候，浏览器又要重排。\n不要一条条改变样式 不要一条条地改变样式，而要通过改变 class，或者 css text 属性，一次性地改变样式。\n// bad var left = 10; var top = 10; el.style.left = left + \u0026#34;px\u0026#34;; el.style.top = top + \u0026#34;px\u0026#34;; // good el.className += \u0026#34; theclassname\u0026#34;; // good el.style.cssText += \u0026#34;; left: \u0026#34; + left + \u0026#34;px; top: \u0026#34; + top + \u0026#34;px;\u0026#34;; 使用离线 DOM 尽量使用离线DOM，而不是真实的网面DOM，来改变元素样式。比如，操作 Document Fragment 对象，完成后再把这个对象加入DOM。再比如，使用 cloneNode() 方法，在克隆的节点上进行操作，然后再用克隆的节点替换原始节点。\n善用 display:none 先将元素设为 display: none（需要1次重排和重绘），然后对这个节点进行100次操作，最后再恢复显示（需要1次重排和重绘）。这样一来，你就用两次重新渲染，取代了可能高达100次的重新渲染。\n善用 absolute 和 fixed position 属性为 absolute 或 fixed 的元素，重排的开销会比较小，因为不用考虑它对其他元素的影响。\n非必要不可见 只在必要的时候，才将元素的 display 属性为可见，因为不可见的元素不影响重排和重绘。另外，visibility:hidden 的元素只对重绘有影响，不影响重排。\n虚拟 DOM 使用虚拟 DOM 的脚本库，比如 React 等。\n善用 RAF、RIC 使用 window.requestAnimationFrame()、window.requestIdleCallback() 这两个方法调节重新渲染。\n（1）分离读写操作：\nfunction doubleHeight(element) { var currentHeight = element.clientHeight; // 下一次重新渲染时执行  window.requestAnimationFrame(function () { element.style.height = (currentHeight * 2) + \u0026#39;px\u0026#39;; }); } elements.forEach(doubleHeight); （2）页面滚动，推动到下一次执行 scroll 函数：\n$(window).on(\u0026#39;scroll\u0026#39;, function() { window.requestAnimationFrame(scrollHandler); }); （3）window.requestIdleCallback()\n它指定只有当一帧的末尾有空闲时间，才会执行回调函数。\n参考  What\u0026rsquo;s the difference between reflow and repaint? 第 22 题：介绍下重绘和回流（Repaint \u0026amp; Reflow），以及如何进行优化 #24  "});index.add({'id':305,'href':'/docs/programmer-interview/front-end/how-browser-render-html/','title':"浏览器如何渲染页面 ?",'content':"浏览器如何渲染页面 ? 渲染主流程 Webkit 渲染流程  Mozilla 的 Gecko 渲染流程   构建 DOM 树 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critical Path\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;\u0026lt;img src=\u0026#34;awesome-photo.jpg\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 对于上述 HTML 片段，浏览器从磁盘或网络读取 HTML 的原始字节，并根据文件的指定编码（例如 UTF-8）将它们转换成各个字符，然后分析出各个 HTML 标签、各个标签对应的属性等。最后，根据标签之间的关系，构建 DOM 树。\n构建 CSSOM 树 在浏览器构建我们这个简单页面的 DOM 时，在文档的 head 部分遇到了一个 link 标记，该标记引用一个外部 CSS 样式表：style.css。由于预见到需要利用该资源来渲染页面，它会立即发出对该资源的请求，并返回以下内容：\nbody { font-size: 16px } p { font-weight: bold } span { color: red } p span { display: none } img { float: right } 与处理 HTML 时一样，我们需要将收到的 CSS 规则转换成某种浏览器能够理解和处理的东西。因此，我们会重复 HTML 过程，不过是为 CSS 而不是 HTML：\nCSS 字节转换成字符，接着转换成标签和节点，最后会构建 CSSOM (CSS Object Model) 树：\n渲染树构建、布局和绘制 CSSOM 树和 DOM 树合并成渲染树，然后用于计算每个可见元素的布局，并输出给绘制流程，将像素渲染到屏幕上。\n合成渲染树 为构建渲染树，浏览器大体上完成了下列工作：\n 从 DOM 树的根节点开始遍历每个可见节点。某些节点不可见（例如脚本标记、元标记等），因为它们不会体现在渲染输出中，所以会被忽略。某些节点通过 CSS 隐藏，因此在渲染树中也会被忽略，例如，上例中的 span 节点就不会出现在渲染树中，因为有一个显式规则在该节点上设置了 display:none 属性。 对于每个可见节点，为其找到适配的 CSSOM 规则并应用到节点上。 将可见节点、内容和计算的样式传递到下一个阶段。   简单提一句，请注意 visibility: hidden 与 display: none 是不一样的。前者隐藏元素，但元素仍占据着布局空间（即将其渲染成一个空框），而后者 (display: none) 将元素从渲染树中完全移除，元素既不可见，也不是布局的组成部分。\n 布局 为弄清每个对象在网页上的确切大小和位置，浏览器从渲染树的根节点开始进行遍历。让我们考虑下面这样一个简单的实例：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Critial Path: Hello world!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;width: 50%\u0026#34;\u0026gt;Hello world!\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 布局流程的输出是一个 “盒模型”，它会精确地捕获每个元素在视口内的确切位置和尺寸：所有相对测量值都转换为屏幕上的绝对像素。\n绘制 我们知道了哪些节点可见、它们的计算样式以及几何信息，我们终于可以将这些信息传递给最后一个阶段：将渲染树中的每个节点转换成屏幕上的实际像素。这一步通常称为 “绘制” 或 “栅格化”。\n提升渲染性能 使用媒体查询避免 CSS 阻塞渲染  CSS 是阻塞渲染的资源。需要将它尽早、尽快地下载到客户端，以便缩短首次渲染的时间。HTML 其实也是阻塞渲染的资源，不过没有 HTML 就没有办法构建 DOM 树，所以 HTML 必须提供。\n 不过，如果我们有一些 CSS 样式只在特定条件下（例如显示网页或将网页投影到大型显示器上时）使用，又该如何？如果这些资源不阻塞渲染，该有多好。\n我们可以通过 CSS “媒体类型” 和 “媒体查询” 来解决这类用例：\n\u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;print.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;print\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;other.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; media=\u0026#34;(min-width: 40em)\u0026#34;\u0026gt; 上述媒体查询定义，\n 第一行的 style.css，未提供任何媒体类型或查询，因此它适用于所有情况，也就是说，它始终会阻塞渲染。 第二行的 print.css，只在打印内容的时候适用，因此在网页首次加载时，该样式表不需要阻塞渲染。 最后一行的 other.css，只在符合条件时，浏览器才会阻塞渲染。  优化 JavaScript 脚本 JavaScript 在 DOM、CSSOM 和 JavaScript 执行之间引入了大量新的依赖关系，从而可能导致浏览器在处理以及在屏幕上渲染网页时出现大幅延迟：\n 脚本在文档中的位置很重要。 当浏览器遇到一个 script 标记时，DOM 构建将暂停，直至脚本完成执行，也就延缓了首次渲染。 如果浏览器尚未完成 CSSOM 的下载和构建，而我们却想在此时运行脚本，浏览器将延迟脚本执行和 DOM 构建，直至其完成 CSSOM 的下载和构建。  默认情况下，所有 JavaScript 都会阻止解析器。由于浏览器不了解脚本计划在页面上执行什么操作，它会作最坏的假设并阻止解析器。为此，我们可以将脚本标记为异步：\n\u0026lt;script src=\u0026#34;app.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; 分析加载时间 使用 Navigation Timing API 和页面加载时发出的其他浏览器事件，您可以捕获并记录任何页面的真实 CRP (Critical Rendering Path) 性能。\n domInteractive 表示 DOM 准备就绪的时间点。 domContentLoaded 一般表示 DOM 和 CSSOM 均准备就绪的时间点。如果没有阻塞解析器的 JavaScript，则 DOMContentLoaded 将在 domInteractive 后立即触发。 domComplete 表示网页及其所有子资源都准备就绪的时间点。  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Critical Path: Measure\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;script\u0026gt; function measureCRP() { var t = window.performance.timing, interactive = t.domInteractive - t.domLoading, dcl = t.domContentLoadedEventStart - t.domLoading, complete = t.domComplete - t.domLoading; var stats = document.createElement(\u0026#39;p\u0026#39;); stats.textContent = \u0026#39;interactive: \u0026#39; + interactive + \u0026#39;ms, \u0026#39; + \u0026#39;dcl: \u0026#39; + dcl + \u0026#39;ms, complete: \u0026#39; + complete + \u0026#39;ms\u0026#39;; document.body.appendChild(stats); } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body onload=\u0026#34;measureCRP()\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Hello \u0026lt;span\u0026gt;web performance\u0026lt;/span\u0026gt; students!\u0026lt;/p\u0026gt; \u0026lt;div\u0026gt;\u0026lt;img src=\u0026#34;awesome-photo.jpg\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 参考  How Browsers Work: Behind the scenes of modern web browsers 关键渲染路径  "});index.add({'id':306,'href':'/docs/programmer-interview/front-end/web_security/','title':"Web 安全",'content':"Web 安全 点击劫持 点击劫持 (Clickjacking) 技术又称为界面伪装攻击 (UI redress attack )，是一种视觉上的欺骗手段。攻击者使用一个或多个透明的 iframe 覆盖在一个正常的网页上，然后诱使用户在该网页上进行操作，当用户在不知情的情况下点击透明的 iframe 页面时，用户的操作已经被劫持到攻击者事先设计好的恶意按钮或链接上。攻击者既可以通过点击劫持设计一个独立的恶意网站，执行钓鱼攻击等；也可以与 XSS 和 CSRF 攻击相结合，突破传统的防御措施，提升漏洞的危害程度。\nCross-site request forgery (CSRF) CSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。\n一个典型的CSRF攻击有着如下的流程：\n 受害者登录 a.com，并保留了登录凭证（Cookie）。 攻击者引诱受害者访问了 b.com。 b.com 向 a.com 发送了一个请求：a.com/act=xx。浏览器会默认携带 a.com 的 Cookie。 a.com 接收到请求后，对请求进行验证，并确认是受害者的凭证，误以为是受害者自己发送的请求。 a.com 以受害者的名义执行了 act=xx。 攻击完成，攻击者在受害者不知情的情况下，冒充受害者，让 a.com 执行了自己定义的操作。  CSRF 的特点  攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。 攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。 整个过程攻击者并不能获取到受害者的登录凭证，仅仅是“冒用”。 跨站请求可以用各种方式：图片URL、超链接、CORS、Form提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。  CSRF通常是跨域的，因为外域通常更容易被攻击者掌控。但是如果本域下有容易被利用的功能，比如可以发图和链接的论坛和评论区，攻击可以直接在本域下进行，而且这种攻击更加危险。\n防护策略 （1）同源检测（Origin 和 Referer 验证）\n既然 CSRF 大多来自第三方网站，那么我们就**直接禁止外域（或者不受信任的域名）**对我们发起请求。在HTTP协议中，每一个异步请求都会携带两个Header，用于标记来源域名：origin 和 referer。\n（2）CSRF Token\nCSRF 攻击之所以能够成功，是因为服务器误把攻击者发送的请求当成了用户自己的请求。那么我们可以要求所有的用户请求都携带一个 CSRF 攻击者无法获取到的 Token。服务器通过校验请求是否携带正确的 Token，来把正常的请求和攻击的请求区分开，也可以防范 CSRF 的攻击。具体步骤：\n 用户打开页面的时候，服务器需要给这个用户生成一个Token，该Token通过加密算法对数据进行加密，一般Token都包括随机字符串和时间戳的组合，显然在提交时Token不能再放在Cookie中了，否则又会被攻击者冒用。因此，为了安全起见Token最好还是存在服务器的Session中，之后在每次页面加载时，使用JS遍历整个DOM树，对于DOM中所有的 a 和 form 标签后加入Token。 页面提交的请求携带这个Token  \u0026lt;input type=”hidden” name=”csrftoken” value=”tokenvalue”/\u0026gt;  当用户从客户端得到了Token，再次提交给服务器的时候，服务器需要判断Token的有效性，验证过程是先解密Token，对比加密字符串以及时间戳，如果加密字符串一致且时间未过期，那么这个Token就是有效的。  这种方法要比之前检查 Referer 或者 Origin 要安全一些，Token可以在产生并放于Session之中，然后在每次请求时把Token从Session中拿出，与请求中的Token进行比对\nCross-site scripting (XSS) 定义 Cross-Site Scripting（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。\n为了和 CSS 区分，这里把攻击的第一个字母改成了 X，于是叫做 XSS。\n XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。\n 示例一 搜索页面，根据 URL 参数决定关键词的内容：\n\u0026lt;input type=\u0026#34;text\u0026#34; value=\u0026#34;\u0026lt;%= getParameter(\u0026#34;keyword\u0026#34;) %\u0026gt;\u0026#34;\u0026gt; \u0026lt;button\u0026gt;搜索\u0026lt;/button\u0026gt; \u0026lt;div\u0026gt; 您搜索的关键词是：\u0026lt;%= getParameter(\u0026#34;keyword\u0026#34;) %\u0026gt; \u0026lt;/div\u0026gt; 用如下链接发起 XSS 攻击，页面中弹出了写着”XSS”的对话框。\nhttp://xxx/search?keyword=\u0026quot;\u0026gt;\u0026lt;script\u0026gt;alert('XSS');\u0026lt;/script\u0026gt; 当浏览器请求 http://xxx/search?keyword=\u0026quot;\u0026gt;\u0026lt;script\u0026gt;alert('XSS');\u0026lt;/script\u0026gt; 时，服务端会解析出请求参数 keyword，得到 \u0026quot;\u0026gt;\u0026lt;script\u0026gt;alert('XSS');\u0026lt;/script\u0026gt;，拼接到 HTML 中返回给浏览器。形成了如下的 HTML：\n\u0026lt;input type=\u0026#34;text\u0026#34; value=\u0026#34;\u0026#34;\u0026gt;\u0026lt;script\u0026gt;alert(\u0026#39;XSS\u0026#39;);\u0026lt;/script\u0026gt;\u0026#34;\u0026gt; \u0026lt;button\u0026gt;搜索\u0026lt;/button\u0026gt; \u0026lt;div\u0026gt; 您搜索的关键词是：\u0026#34;\u0026gt;\u0026lt;script\u0026gt;alert(\u0026#39;XSS\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; 修复示例一漏洞 使用 escapeHTML 对字符进行转义：\n\u0026lt;input type=\u0026#34;text\u0026#34; value=\u0026#34;\u0026lt;%= escapeHTML(getParameter(\u0026#34;keyword\u0026#34;)) %\u0026gt;\u0026#34;\u0026gt; \u0026lt;button\u0026gt;搜索\u0026lt;/button\u0026gt; \u0026lt;div\u0026gt; 您搜索的关键词是：\u0026lt;%= escapeHTML(getParameter(\u0026#34;keyword\u0026#34;)) %\u0026gt; \u0026lt;/div\u0026gt; 示例二 页面某部分源码：\n\u0026lt;a href=\u0026#34;\u0026lt;%= escapeHTML(getParameter(\u0026#34;redirect_to\u0026#34;)) %\u0026gt;\u0026#34;\u0026gt;跳转...\u0026lt;/a\u0026gt; 使用如下链接发起 XSS 攻击：\nhttp://xxx/?redirect_to=javascript:alert('XSS') http://xxx/?redirect_to=jAvascRipt:alert('XSS') http://xxx/?redirect_to=%20javascript:alert('XSS') 服务器响应变为：\n\u0026lt;a href=\u0026#34;javascript:alert(\u0026amp;#x27;XSS\u0026amp;#x27;)\u0026#34;\u0026gt;跳转...\u0026lt;/a\u0026gt; 虽然代码不会立即执行，但一旦用户点击 a 标签时，浏览器会就会弹出“XSS”。\n修复示例二漏洞 禁止以 javascript: 开头的链接，和其他非法的 scheme：\n// 根据项目情况进行过滤，禁止掉 \u0026#34;javascript:\u0026#34; 链接、非法 scheme 等 allowSchemes = [\u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;]; valid = isValid(getParameter(\u0026#34;redirect_to\u0026#34;), allowSchemes); if (valid) { \u0026lt;a href=\u0026#34;\u0026lt;%= escapeHTML(getParameter(\u0026#34;redirect_to\u0026#34;))%\u0026gt;\u0026#34;\u0026gt; 跳转... \u0026lt;/a\u0026gt; } else { \u0026lt;a href=\u0026#34;/404\u0026#34;\u0026gt; 跳转... \u0026lt;/a\u0026gt; } Man-in-the-middle (MitM)  解决中间人攻击的最主要手段：使用 HTTPS\n Session hijacking (cookie hijacking)  hijacking 是 窃取 的意思\n 参考  Types of attacks 前端安全系列（一）：如何防止XSS攻击？ 前端安全系列（二）：如何防止CSRF攻击？  "});index.add({'id':307,'href':'/docs/programmer-interview/front-end/vue-lifecycle/','title':"VUE 生命周期",'content':"VUE 生命周期 "});index.add({'id':308,'href':'/docs/programmer-interview/algorithm/longest-substring-with-at-most-k-distinct-characters/','title':"找出最多 K 个不同字符的最长子串",'content':"找出最多 K 个不同字符的最长子串  微软、网易\n // https://www.lintcode.com/problem/longest-substring-with-at-most-k-distinct-characters/description // 网易 // 微软面试题 // // 给定字符串S，找到最多有k个不同字符的最长子串 T // // 输入: S = \u0026#34;eceba\u0026#34; 并且 k = 3 // 输出: 4 // 解释: T = \u0026#34;eceb\u0026#34; public class LongestSubstringwithAtMostKDistinctCharacters { public int lengthOfLongestSubstringKDistinct(String s, int k) { if (k == 0 || s.length() == 0) { return 0; } Map\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); int longest = 0; int right = 0; int left = 0; while (right \u0026lt; s.length()) { char c = s.charAt(right); if (map.size() == k) { // k = 3  // ↓ ↓  // a b c c c  if (map.containsKey(c)) { map.put(c, map.getOrDefault(c, 0) + 1); right++; } else { longest = Math.max(longest, right - left); // 注意这个地方并没有更新 right 的指针  // k = 3  // ↓ ↓  // a a a b c  char leftC = s.charAt(left); int oldCount = map.get(leftC); if (oldCount == 1) { map.remove(leftC); } else { map.put(leftC, oldCount - 1); } left++; } } else { map.put(c, map.getOrDefault(c, 0) + 1); right++; } } if (map.size() \u0026lt;= k) { longest = Math.max(longest, right - left); } return longest; } } "});index.add({'id':309,'href':'/docs/programmer-interview/front-end/nexttick/','title':"Vue.nextTick",'content':"Vue.nextTick 作用 在 DOM 更新后，执行一个回调。\n// DOM 还没有更新 Vue.nextTick(function () { // DOM 更新了 }) Vue 何时更新 DOM Vue 在修改数据后，DOM 不会立刻更新，而是等同一事件循环中的所有数据变化完成之后，再统一进行 DOM 更新。\n应用场景 created/mounted 操作 DOM mounted: function () { this.$nextTick(function () { // Code that will run only after the  // entire view has been rendered  }) } 显示输入框并获取焦点 showInput() { this.show = true this.$nextTick(function () { // DOM 更新了  document.getElementById(\u0026#34;keywords\u0026#34;).focus() }) } "});index.add({'id':310,'href':'/docs/programmer-interview/algorithm/add-strings/','title':"两个字符串整数相加",'content':"两个字符串整数相加  微软\n // https://leetcode.com/problems/add-strings/ // 没有 leading zeros // 微软面试题 public class AddStrings { // 1 2 3 4  // 7 8 9  public String addStrings(String num1, String num2) { if (num1 == null) { return num2; } if (num2 == null) { return num1; } final StringBuilder sb = new StringBuilder(Math.max(num1.length(), num2.length()) + 1); int index1 = num1.length() - 1; int index2 = num2.length() - 1; int remainder = 0; while (index1 \u0026gt;= 0 \u0026amp;\u0026amp; index2 \u0026gt;= 0) { char a = num1.charAt(index1); char b = num2.charAt(index2); int num = ((a - \u0026#39;0\u0026#39;) + (b - \u0026#39;0\u0026#39;)) + remainder; remainder = num / 10; num %= 10; sb.append(num); index1--; index2--; } while (index1 \u0026gt;= 0) { char a = num1.charAt(index1); int num = (a - \u0026#39;0\u0026#39;) + remainder; remainder = num / 10; num %= 10; sb.append(num); index1--; } while (index2 \u0026gt;= 0) { char b = num2.charAt(index2); int num = (b - \u0026#39;0\u0026#39;) + remainder; remainder = num / 10; num %= 10; sb.append(num); index2--; } if (remainder != 0) { sb.append(remainder); } return sb.reverse().toString(); } } "});index.add({'id':311,'href':'/docs/programmer-interview/algorithm/minimum-path-sum/','title':"二维矩阵数值和最小的路径",'content':"二维矩阵数值和最小的路径  微软\n // 微软面试题: 二维矩阵，没有负值，找出从左上角到右下角，使得路径上的数值和最小的路径 // https://leetcode.com/problems/minimum-path-sum/ // public class MinimumPathSum { public int minPathSum(int[][] grid) { int m = grid.length; int n = grid[0].length; int[][] minSum = new int[m][n]; minSum[0][0] = grid[0][0]; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { if (i == 0 \u0026amp;\u0026amp; j == 0) { continue; } if (i - 1 \u0026lt; 0) { minSum[i][j] = minSum[i][j - 1] + grid[i][j]; } else if (j - 1 \u0026lt; 0) { minSum[i][j] = minSum[i - 1][j] + grid[i][j]; } else { // =======================  // 这一个公式就可以了  // =======================  minSum[i][j] = Math.min(minSum[i - 1][j], minSum[i][j - 1]) + grid[i][j]; } } } return minSum[m - 1][n - 1]; } } "});index.add({'id':312,'href':'/docs/programmer-interview/algorithm/minimum-cost-for-tickets/','title':"最小火车票费用",'content':"最小火车票费用 // 亚马逊电面 // 最小火车票费用 // // 一年有 365 天，第一天编号为 1 // days = [1,4,6,7,8,20] 你的旅游时间必须覆盖到1、4、6、... // costs = [2,7,15] 旅游有 1 日游、7 日游、30 日游 // 问覆盖到所有天数的最小 cost // // https://leetcode.com/problems/minimum-cost-for-tickets/ public class MinimumCostForTickets { public int mincostTickets(int[] days, int[] costs) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; days.length; i++) { set.add(days[i]); } int[] dp = new int[366]; for (int i = 1; i \u0026lt; 366; i++) { if (!set.contains(i)) { // 无需花钱  dp[i] = dp[i - 1]; } else { dp[i] = Math.min( // 昨天购买一张票  dp[i - 1] + costs[0], Math.min( // 7 天前购买一张票  dp[Math.max(i - 7, 0)] + costs[1], // 30 天前购买一张票  dp[Math.max(i - 30, 0)] + costs[2] ) ); } } return dp[365]; } } "});index.add({'id':313,'href':'/docs/programmer-interview/algorithm/lis/','title':"最长递增子序列 (LIS)",'content':"最长递增子序列 (LIS) 方法一 // 未排序 // [10,9,2,5,3,7,101,18] =\u0026gt; [2,3,7,101], length = 4 public class LongestIncreasingSubsequence { public int lengthOfLIS(int[] array) { if (array.length == 0) { return 0; } // [10,9,2,5,3,7,101,18]  // 1(10)  // 1(9)  // 1(2)  // 2(5)  // 2(3)  // 3(7)  // 4(101)  // 4(18)  //  // 每个长度 i + 1 的最小值  // 这是一个有序序列  int[] lisLength = new int[array.length]; lisLength[0] = array[0]; int lo = 0; int hi = 0; // [10,9,2,5,3,4]  // 1(10)  // 1(9)  // 1(2)  // 2(5)  // 2(3)  // 3(4)  for (int i = 1; i \u0026lt; array.length; i++) { if (array[i] \u0026gt; lisLength[hi]) { lisLength[++hi] = array[i]; } else { // [2,3,5] 4,  //  // 我们不是要将 [2,3,5] 中的 3 替换为 4  // 而是要将 5 替换 4  int h = hi; while (h \u0026gt;= 0 \u0026amp;\u0026amp; array[i] \u0026lt;= lisLength[h]) { h--; } if (h == -1) { lisLength[h + 1] = array[i]; } else { lisLength[h + 1] = array[i]; } } } return hi + 1; } } 方法二 // 未排序 // [10,9,2,5,3,7,101,18] =\u0026gt; [2,3,7,101], length = 4 public class LongestIncreasingSubsequence_Solution_1 { public int lengthOfLIS(int[] array) { if (array.length == 0) { return 0; } // [10,9,2,5,3,7,101,18]  // 1(10)  // 1(9)  // 1(2)  // 2(5)  // 2(3)  // 3(7)  // 4(101)  // 4(18)  //  // 每个长度 i + 1 的最小值  // 这是一个有序序列  int[] lisLength = new int[array.length]; lisLength[0] = array[0]; int hi = 0; // [10,9,2,5,3,4]  // 1(10)  // 1(9)  // 1(2)  // 2(5)  // 2(3)  // 3(4)  for (int i = 1; i \u0026lt; array.length; i++) { if (array[i] \u0026gt; lisLength[hi]) { lisLength[++hi] = array[i]; } else { // [2,3,5] 4,  //  // 我们不是要将 [2,3,5] 中的 3 替换为 4  // 而是要将 5 替换 4  //  // =============================  // 二叉树版本  // 找到第一个大于 target 的值的索引  //  // 测试未通过 [18,55,66,2,3,54]  //  // [18,55,66,2,3,54]  // 1(18)  // 2(55)  // 2(66)  // 1(2)  // 2(3)  // 3(54)  // =============================  int l = 0; int h = hi; int target = array[i]; while (l \u0026lt;= h) { int m = l + ((h - l) \u0026gt;\u0026gt; 1); if (target \u0026gt; lisLength[m]) { l = m + 1; } else { if ((m - 1 \u0026gt;= 0 \u0026amp;\u0026amp; target \u0026gt; lisLength[m - 1]) || m == 0) { h = m; break; } h = m - 1; } } lisLength[h] = array[i]; } } return hi + 1; } } DP // https://leetcode.com/problems/longest-increasing-subsequence/solution/ public class LongestIncreasingSubsequence_DP { public int lengthOfLIS(int[] nums) { if (nums.length == 0) { return 0; } int[] dp = new int[nums.length]; dp[0] = 1; int maxans = 1; for (int i = 1; i \u0026lt; dp.length; i++) { int maxval = 0; for (int j = 0; j \u0026lt; i; j++) { if (nums[i] \u0026gt; nums[j]) { maxval = Math.max(maxval, dp[j]); } } dp[i] = maxval + 1; maxans = Math.max(maxans, dp[i]); } return maxans; } } "});index.add({'id':314,'href':'/docs/programmer-interview/algorithm/linkedlist-has-cycle/','title':"链表是否有环",'content':"链表是否有环  微软\n // https://leetcode.com/problems/linked-list-cycle/discuss/44669/Fully-Explained!-why-fast-and-slow-can-meet-in-the-cycle // https://www.cnblogs.com/wuyuegb2312/p/3183214.html // // 解释: 为什么快指针、慢指针能够相遇 // // 在任意时刻，p1 和 p2都在环上。由于 p1 每次向前 1 步，p2 每次向前两步， // 用相对运动的观点来看，把 p1 看作静止，那么 p2 每次相对 p1 向前 1 步， // 二者在顺时针方向上的距离每经过一个时刻就减少 1，直到变为 0，也即二者恰好相遇。 // 这样就证明了在离散情况下，对于有环链表，二者也是必然在某一时刻相遇在某个节点上的。 public class LinkedListCycle { public boolean hasCycle(ListNode head) { if (head == null) { return false; } ListNode slower = head; ListNode faster = head.next; while (slower != null \u0026amp;\u0026amp; faster != null) { if (slower == faster) { return true; } slower = slower.next; if (faster.next == null) { return false; } faster = faster.next.next; } return false; } } "});index.add({'id':315,'href':'/docs/programmer-interview/algorithm/find-linkedlist-cycle-start-node/','title':"找出链表环的入口节点",'content':"找出链表环的入口节点  微软\n // 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 // ↑_________| // // 假设链表长度为 L = 4 // 假设环长度为 C = 3 // 假设相遇时的点，距离环的入口 [逆时针顺序] 长度为 K // // 最终相遇: // - 快指针走了 L + m * C + K 步骤 // - 慢指针走了 L + n * C + K 步骤 // // L + m * C + K = 2 * (L + n * C + K) // // 化简得到 m * C = L + 2n * C + K // 化简得到 (m - 2n) * C = L + K // 化简得到 K = (m - 2n) * C - L = n\u0026#39; * C - L // // 即 K 是常数 // // 此时，相遇点 K 的位置是 n\u0026#39; * C - L，它再走 L 步就能到链表入口处 // 而此时慢指针从头开始走，也需要 L 步才能到链表入口处，所以这个是可以找到入口的 // // 还有就是如下这个代码，faster 从头开始移动 C 个，因为总长度是 L + C 个，所以全程还剩余 L 个 // 慢指针也需要走 L 步，所以还是会相遇 public class LinkedListCycle2 { public ListNode detectCycle(ListNode head) { // 是否有环  ListNode meetNode = hasCycle(head); if (meetNode == null) { return null; } // 求出环的个数  int cycleLen = lengthOfCycle(meetNode); // 先移动 cycleLen 个  ListNode faster = head; while (cycleLen \u0026gt; 0) { faster = faster.next; cycleLen--; } // 一起移动  ListNode slower = head; while (faster != slower) { faster = faster.next; slower = slower.next; } return slower; } public int lengthOfCycle(ListNode meetNode) { ListNode p = meetNode.next; int c = 1; while (p != meetNode) { p = p.next; c++; } return c; } public ListNode hasCycle(ListNode head) { if (head == null) { return null; } ListNode slower = head; ListNode faster = head.next; while (slower != null \u0026amp;\u0026amp; faster != null) { if (slower == faster) { return slower; } slower = slower.next; if (faster.next == null) { return null; } faster = faster.next.next; } return null; } } "});index.add({'id':316,'href':'/docs/programmer-interview/algorithm/5-read-1-write/','title':"5 个线程读 1 个线程写",'content':"5 个线程读 1 个线程写  微软\n /** * 微软三面面试题: * * - 一个线程写，如果 5 个读线程没有读完，那么等待 * - 5 个线程【同时】读，如果已经读过，那么等待 * * @author zk */ public class ReaderWriter { public static void main(String[] args) { final int READ_COUNT = 5; final ReaderWriter rw = new ReaderWriter(READ_COUNT); Thread writer = new Thread(new Runnable() { @Override public void run() { AtomicInteger atomicInteger = new AtomicInteger(); while (true) { rw.write(atomicInteger.getAndIncrement()); } } }); writer.setName(\u0026#34;Thread-W\u0026#34;); writer.start(); for (int i = 0; i \u0026lt; READ_COUNT; i++) { Thread t = new Thread(new Runnable() { @Override public void run() { while (true) { Object obj = rw.read(); System.out.println(\u0026#34;Thread-\u0026#34; + Thread.currentThread().getName() + \u0026#34; 读 [\u0026#34; + obj + \u0026#34;]\u0026#34;); } } }); t.setName(String.valueOf(i)); t.start(); } } private final Semaphore[] readStateSemaphore; private final int readCount; private final Semaphore readSemaphore = new Semaphore(0); private final AtomicInteger alreadyReadCount = new AtomicInteger(0); // 已经读完的线程数量  private final Semaphore writeSemaphore = new Semaphore(1); private volatile Object sharedObj; public ReaderWriter(int readCount) { this.readCount = readCount; this.readStateSemaphore = new Semaphore[readCount]; for (int i = 0; i \u0026lt; readCount; i++) { readStateSemaphore[i] = new Semaphore(0); } } public Object read() { int tid = Integer.parseInt(Thread.currentThread().getName()); Object res; try { readStateSemaphore[tid].acquire(); readSemaphore.acquire(); res = this.sharedObj; if (alreadyReadCount.incrementAndGet() == readCount) { writeSemaphore.release(); } } catch (InterruptedException e) { e.printStackTrace(); return null; } return res; } public void write(Object obj) { try { writeSemaphore.acquire(); System.out.println(Thread.currentThread().getName() + \u0026#34; 写 [\u0026#34; + obj + \u0026#34;]\u0026#34;); this.sharedObj = obj; this.alreadyReadCount.set(0); readSemaphore.release(readCount); for (Semaphore s: readStateSemaphore) { s.release(); } } catch (InterruptedException e) { e.printStackTrace(); } } } "});index.add({'id':317,'href':'/docs/programmer-interview/front-end/vue/','title':"VUE 面试题",'content':"VUE 面试题 整理 VUE 相关的常见面试题\n介绍一下 VUE 介绍一下 VUEX VUE 2.X 和 3.0 的区别 （1）数据监听方式变化\nVUE 2.X 使用 ES5 的 Object.defineProperty() 的 get() 和 set(newValue) 实现，VUE 3.0 基于 Proxy 监听实现，同时更为强大：\n 可以检测属性的新增和删除 可以检测数组索引的变化和 length 的变化 支持 Map、Set、WeakMap 和 WeakSet   优点：速度加倍，内存占用减半。\n （2）体积更小\n支持 Tree Shaking，内置组件、内置指令按需引入。\n（3）速度更快\n参考：vue3.0和vue2.x的区别、Vue 3.0 和 Vue 2.0的对比以及Vue 2.0精讲以及Vue全家桶精讲\nVUE 的生命周期 VUE 数据双向绑定原理 VUE 采用发布者-订阅者模式的方式来实现双向绑定。\n（1）视图更新数据：\ninput 标签监听 input 事件即可。\n（2）数据更新视图：\nObject.defineProperty() 监听数据变化，通过消息订阅器发布消息，订阅者收到消息执行相应的操纵 DOM 的函数，从而更新视图。\nVUE 的路由机制  hash 和 history 区别  v-if 和 v-show 的区别  v-show：无论值是 true 还是 false，元素都会存在于 HTML 代码中。 v-if：只有值为 true 的时候，元素才会存在于 HTML 代码中。  VUE 组件通信方式  引申：如果有多层的父子组件，用什么通信  VUE 的 v-for 中的 key 的作用 一句话回答：为了高效的更新虚拟 DOM。\nnextTick 原理与应用场景 Vue 在修改数据后，视图不会立刻更新，而是等同一事件循环中的所有数据变化完成之后，再统一进行视图更新。\n//改变数据 vm.message = \u0026#39;changed\u0026#39; //想要立即使用更新后的DOM。这样不行，因为设置message后DOM还没有更新 console.log(vm.$el.textContent) // 并不会得到\u0026#39;changed\u0026#39;  //这样可以，nextTick里面的代码会在DOM更新后执行 Vue.nextTick(function(){ console.log(vm.$el.textContent) //可以得到\u0026#39;changed\u0026#39; }) 应用场景：需要在 DOM 视图更新之后，基于新的 DOM 视图进行操作。\n参考：Vue.nextTick 的原理和用途\nVUE 的虚拟 DOM  VUE 是如何实现 VDOM 的 vue中keep-alive缓存的真实结点还是虚拟结点 vue改变组件的key值, 原来的组件会被销毁么 为什么要用虚拟结点 diff 原理  vue 从 data 改变到页面渲染的过程 参考  诚意满满的前端面试总结（回馈牛客）  "});index.add({'id':318,'href':'/categories/','title':"Categories",'content':""});index.add({'id':319,'href':'/tags/','title':"Tags",'content':""});index.add({'id':320,'href':'/tags/war/','title':"War",'content':""});index.add({'id':321,'href':'/posts/','title':"博客",'content':""});index.add({'id':322,'href':'/categories/%E7%BC%96%E7%A8%8B/','title':"编程",'content':""});index.add({'id':323,'href':'/','title':"首页",'content':"赵坤的个人网站 本博客将致力于整理、分析 Java、前端 等开发者生态圈的开源项目的教程、源码、面试题等，我会参阅大量书籍，一一对这些基础知识点抽丝剥茧，并匹配大量图表，为大家呈现出它们最本质的面目。\n通过阅读和分析开源项目等，可以增长自己的工程实践能力，可以让自己从代码中汲取养分，也可以学习到他人的设计权衡之道，其对于自己成长的重要性不言而喻！\n本网站至少包含但不仅限于如下内容：\n 【前端、UNIX、Git、网络协议等教程】 【RocketMQ 源码分析】 【程序员面试题】：前端、Java、算法 【设计数据密集型应用程序】  "});index.add({'id':324,'href':'/tags/jsp/','title':"JSP",'content':""});index.add({'id':325,'href':'/tags/mq/','title':"MQ",'content':""});index.add({'id':326,'href':'/tags/spring/','title':"Spring",'content':""});index.add({'id':327,'href':'/tags/jax-ws/','title':"JAX-WS",'content':""});index.add({'id':328,'href':'/tags/oracle/','title':"Oracle",'content':""});index.add({'id':329,'href':'/tags/ant/','title':"Ant",'content':""});index.add({'id':330,'href':'/tags/struts/','title':"Struts",'content':""});index.add({'id':331,'href':'/tags/%E7%BC%93%E5%AD%98/','title':"缓存",'content':""});index.add({'id':332,'href':'/tags/nginx/','title':"Nginx",'content':""});index.add({'id':333,'href':'/tags/%E4%BF%A1%E6%81%AF/','title':"信息",'content':""});index.add({'id':334,'href':'/categories/%E7%A7%91%E6%8A%80/','title':"科技",'content':""});index.add({'id':335,'href':'/tags/%E7%B3%BB%E7%BB%9F%E4%B8%80%E8%87%B4%E6%80%A7/','title':"系统一致性",'content':""});index.add({'id':336,'href':'/tags/java/','title':"Java",'content':""});index.add({'id':337,'href':'/docs/','title':"Docs",'content':""});})();