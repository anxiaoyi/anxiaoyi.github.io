<html>
<head>
	
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><title>Redis 常见问题</title>
	<meta name="keywords" content="代码人生,程序员,赵坤" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
    <!--<link rel="stylesheet" href="/css/main.css">-->
	<link href="/css/main.css?v=2" rel="stylesheet" type="text/css" />
    <!--<link rel="stylesheet" href="/css/style.css">-->
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2"/><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="Redis-常见问题"><a href="#Redis-常见问题" class="headerlink" title="Redis 常见问题"></a>Redis 常见问题</h2><h3 id="为什么-Redis-这么快"><a href="#为什么-Redis-这么快" class="headerlink" title="为什么 Redis 这么快"></a>为什么 Redis 这么快</h3><p>总体来说快速的原因如下： </p>
<ol>
<li>绝大部分请求是纯粹的内存操作（非常快速） </li>
<li>采用单线程,避免了不必要的上下文切换和竞争条件 </li>
<li>非阻塞 IO </li>
</ol>
<p>内部实现采用 <code>epoll</code>，采用了 <code>epoll</code> + 自己实现的简单的事件框架。<code>epoll</code> 中的读、写、关闭、连接都转化成了事件，然后利用<code>epoll</code> 的多路复用特性，绝不在 I/O 上浪费一点时间 </p>
<h3 id="redis-如何处理客户端连接"><a href="#redis-如何处理客户端连接" class="headerlink" title="redis 如何处理客户端连接"></a>redis 如何处理客户端连接</h3><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwNjQwNzU2NQ==&amp;mid=400335397&amp;idx=3&amp;sn=35f807a4fd813361a00a7a522bc9f146&amp;3rd=MzA3MDU4NTYzMw==&amp;scene=6#rd" target="_blank" rel="noopener">redis 如何处理客户端连接</a></li>
</ul>
<h3 id="周期性出现-connect-timeout"><a href="#周期性出现-connect-timeout" class="headerlink" title="周期性出现 connect timeout"></a>周期性出现 connect timeout</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis.clients.jedis.exceptions.JedisConnectionException</span><br><span class="line">java.net.SocketException</span><br><span class="line">java.net.SocketTimeoutException:connect time out</span><br></pre></td></tr></table></figure>
<p>一开始怀疑是网络问题，但是并未发现问题，观察各种对比图表，<code>tcp listenOverFlow</code> 和 <code>timeout</code> 经常周期出现。</p>
<p>I/O 多路复用程序<strong>通过队列</strong>向文件事件分派器传送套接字的过程:</p>
<p><img src="20160203203441_736.png" alt=""></p>
<p><code>Redis</code> 的单线程模型（对命令的处理和连接的处理都是在一个线程中），如果存在慢查询的话，会出现上面的这种情况，造成新的 accept 的连接进不了队列:</p>
<p><img src="20160203203441_928.png" alt=""></p>
<p>解决方法:</p>
<ul>
<li>对慢查询进行报警（频率、数量、时间）等等因素</li>
<li>告诉业务端人员执行 <code>monitor, keys *, flushall</code> 这些命令的坑</li>
</ul>
<h3 id="redis-bgrewriteaof-问题"><a href="#redis-bgrewriteaof-问题" class="headerlink" title="redis bgrewriteaof 问题"></a>redis bgrewriteaof 问题</h3><p>Redis 的 AOF 机制有点类似于 Mysql binlog，是 Redis 的提供的一种持久化方式（另一种是 RDB ），它会将所有的写命令按照一定频率(no, always, every seconds)写入到日志文件中，当 Redis 停机重启后恢复数据库。</p>
<p>AOF 重写:</p>
<ul>
<li>随着 AOF 文件越来越大，里面会有大部分是重复命令或者可以合并的命令（100 次 <code>incr</code> = <code>set key 100</code>）</li>
<li>重写的好处：减少 AOF 日志尺寸，减少内存占用，加快数据库恢复时间。</li>
</ul>
<p><img src="20160203203443_877.png" alt=""></p>
<p>单机多实例可能存在 <code>Swap</code> 和 <code>OOM</code> 的隐患: 由于 Redis 的单线程模型，理论上每个 redis 实例只会用到一个 CPU, 也就是说可以在一台多核的服务器上部署多个实例（实际就是这么做的）。但是 <strong>Redis 的 AOF 重写是通过 <code>fork</code> 出一个 Redis 进程来实现的</strong>，所以有经验的 Redis 开发和运维人员会告诉你，在一台服务器上要预留一半的内存（防止出现 AOF 重写集中发生，出现 swap 和 OOM）。</p>
<p><img src="20160203203444_104.png" alt=""></p>
<p>解决方案:</p>
<ul>
<li>让每个 redis 决定是否做 AOF 重写操作（根据 <code>auto-aof-rewrite-percentage</code> 和 <code>auto-aof-rewrite-min-size</code> 两个参数）</li>
<li><code>crontab</code>: 定时任务，可能仍然会出现多个 redis 实例，属于一种折中方案</li>
<li>remote 集中式: 以机器为单位，轮询每个机器的实例，如果满足条件就运行(比如 <code>currentSize</code> 和 <code>baseSize</code> 满足什么关系) <code>bgrewriteaof</code> 命令。</li>
</ul>
<h3 id="Redis-内存占用飙升"><a href="#Redis-内存占用飙升" class="headerlink" title="Redis 内存占用飙升"></a>Redis 内存占用飙升</h3><p>执行命令 <code>INFO</code> 查看当前系统状态:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; INFO</span><br></pre></td></tr></table></figure>
<p>观察 info 信息，有一点引起了怀疑： <code>client_longest_output_list</code> 有些异常，于是理解想到<strong>服务端和客户端交互时，分别为每个客户端设置了输入缓冲区和输出缓冲区</strong>，这部分如果很大的话也会占用 <code>Redis</code> 服务器的内存。</p>
<p><img src="20160203203449_225.png" alt=""></p>
<p>从上面的 <code>client_longest_output_list</code> 看，应该是输出缓冲区占用内存较大，也就是有大量的数据从Redis服务器向某些客户端输出。于是使用 <code>CLIENT LIST</code> 命令（类似于 <code>mysql processlist</code> ）<code>redis-cli -h host -p port client list | grep -v &quot;omem=0&quot;</code>，来查询输出缓冲区不为 0 的客户端连接，于是查询到祸首 <code>monitor</code>，于是豁然开朗. <code>monitor</code> 的模型是这样的，它会将所有在 Redis 服务器执行的命令进行输出，通常来讲 Redis 服务器的 QPS 是很高的，也就是如果执行了 monitor 命令，Redis 服务器在 Monitor 这个客户端的输出缓冲区又会有大量“存货”，也就占用了大量 Redis 内存。</p>
<p><img src="20160203203449_821.png" alt=""></p>
<p>紧急处理和解决方法:</p>
<ul>
<li>进行主从切换（主从内存使用量不一致），也就是 <code>redis-cluster</code> 的 <code>fail-over</code> 操作，继续观察新的 Master 是否有异常，通过观察未出现异常。查找到真正的原因后，也就是 monitor，关闭掉 monitor命令的进程后，内存很快就降下来了。</li>
</ul>
<h3 id="Redis-内存使用优化"><a href="#Redis-内存使用优化" class="headerlink" title="Redis 内存使用优化"></a>Redis 内存使用优化</h3><p>场景:</p>
<table>
<thead>
<tr>
<th>userId(用户id)</th>
<th>weiboCount(微博数)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2000</td>
</tr>
<tr>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td>3</td>
<td>288</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>1000000</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>(1) 使用字符串数据结构:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SET user:1 2000</span><br><span class="line">127.0.0.1:6379&gt; SET user:2 10</span><br><span class="line">127.0.0.1:6379&gt; SET user:3 288</span><br></pre></td></tr></table></figure>
<p>(2) 使用 Hash 结构:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; HMSET allUserWeiboCount user:1 2000 user:2 10 user:3 288</span><br></pre></td></tr></table></figure>
<p>(3) 使用 Hash 结构 + 多个 HashKey: key=userId/100, field=userId%100, fieldValue=weiboCount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; HMSET hashKey:0 1 2000</span><br><span class="line">127.0.0.1:6379&gt; HMSET hashKey:0 2 10</span><br><span class="line">127.0.0.1:6379&gt; HMSET hashKey:0 3 288</span><br><span class="line">...</span><br><span class="line">127.0.0.1:6379&gt; HMSET hashKey:1 0 232</span><br><span class="line">127.0.0.1:6379&gt; HMSET hashKey:1 1 444</span><br><span class="line">127.0.0.1:6379&gt; HMSET hashKey:1 2 22</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>内存占用量对比 (100万用户 userId:1~1000000) :</p>
<p><img src="20160203203449_764.png" alt=""></p>
<p>Redis 其实是一把瑞士军刀:</p>
<p><img src="20160203203449_172.png" alt=""></p>
<h3 id="无穷无尽的-Replication-循环"><a href="#无穷无尽的-Replication-循环" class="headerlink" title="无穷无尽的 Replication 循环"></a>无穷无尽的 Replication 循环</h3><h4 id="1-Slave-和-Master-如何同步数据"><a href="#1-Slave-和-Master-如何同步数据" class="headerlink" title="(1) Slave 和 Master 如何同步数据:"></a>(1) Slave 和 Master 如何同步数据:</h4><ol>
<li>Slave: “我想要变成你”</li>
<li>Master: “你必须要有耐心”</li>
<li>Master forks 了他自己，然后<ol>
<li>fork 出来的进程开始 Dump RDB 文件</li>
<li>主进程继续处理常规 Redis 连接</li>
<li>任何对数据做出的改变都被拷贝到了 Replication 缓冲中</li>
</ol>
</li>
<li>Dump 完成了，Master: “过来拿吧”</li>
<li>Slave 通过网络连接把文件取回来，然后放到自己的磁盘上</li>
<li>Slave 放到本地之后，然后加载这个 RDB 文件</li>
<li>Slave: “我已经完成了我的 circle，我准备好了”</li>
<li>Master Replication 缓冲区的任何数据的改变将会自动同步到 Slave</li>
</ol>
<p>由上述步骤可知，Slave 和 Master 之间的数据同步分为两个阶段: <strong>全量和增量</strong>，当数据量很大的时候，会产生一种称之为 <strong><a href="https://redislabs.com/blog/testing-fork-time-on-awsxen-infrastructure" target="_blank" rel="noopener">latency due to fork</a></strong> 的现象: <strong>The bigger your dataset is, the longer it will take to fork, dump, copy and load it into the slave.</strong></p>
<h3 id="Replication-缓存限制"><a href="#Replication-缓存限制" class="headerlink" title="Replication 缓存限制"></a>Replication 缓存限制</h3><p>对于 Redis 服务器的输出（也就是命令的返回值）来说，其大小通常是不可控制的。有可能一个简单的命令，能够产生体积庞大的返回数据。另外也有可能因为执行了太多命令，导致产生返回数据的速率超过了往客户端发送的速率，这是也会导致<strong>服务器堆积大量消息，从而导致输出缓冲区越来越大，占用过多内存，甚至导致系统崩溃</strong>。</p>
<p>所幸，Redis 设置了一些保护机制来避免这种情况的出现，不同类型的客户端有不同的限制参数。限制方式有如下两种：</p>
<ol>
<li>大小限制，当某一个客户端的缓冲区超过某一个大小值时，直接关闭这个客户端的连接；</li>
<li>持续性限制，当某一个客户端的缓冲区持续一段时间占用过大空间时，会直接关闭客户端连接。</li>
</ol>
<p>In a full master-slave synchronization, changes performed to the data during the initial phase of the synchronization are <strong>held in the replication buffer by the master server</strong>.</p>
<p>所有的 client 请求 redis 数据的时候，redis 要返回给 client 的数据都会先被存储在 <code>output-buffer</code> 中，等所有信息都被传送完毕之后，再清除 <code>output-buffer</code> 中的数据:</p>
<p>Redis 限制:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; config get client-output-buffer-limit</span><br><span class="line">1) &quot;client-output-buffer-limit&quot;</span><br><span class="line">// 硬限制: 256MB = 268435456 = 256 * 1024 * 1024</span><br><span class="line">// 当 output-buffer 的大小大于 256MB 之后就会断开连接</span><br><span class="line">// 软限制: 67108864 = 64MB = 64 * 1024 * 1024</span><br><span class="line">// 当 output-buffer 的大小大于 64MB 并且超过了 60 秒的时候就会断开连接</span><br><span class="line">2) &quot;normal 0 0 0 slave 268435456 67108864 60 pubsub 33554432 8388608 60&quot;</span><br></pre></td></tr></table></figure>
<p>增加输出 Slave 的缓存大小:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// hard limit: 512MB</span><br><span class="line">// soft limit: 512MB</span><br><span class="line">// 临时生效</span><br><span class="line">127.0.0.1:6379&gt; config set client-output-buffer-limit &quot;slave 536870912 536870912 0&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Replication-超时"><a href="#Replication-超时" class="headerlink" title="Replication 超时"></a>Replication 超时</h3><p>默认超时时间是 60 秒:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; CONFIG GET repl-timeout</span><br><span class="line">1) &quot;repl-timeout&quot;</span><br><span class="line">2) &quot;60&quot;</span><br></pre></td></tr></table></figure>
<p>超时时间 = ( <code>BGSAVE</code> + COPY + LOAD ) * (110% ~ 120%)</p>
<p>(1) 执行 <code>BGSAVE</code> 命令，然后查看日志:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tail -f /var/log/redis/redis-server.log</span><br></pre></td></tr></table></figure>
<p>如果是从源代码安装的话，那么日志文件位于:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tail -f /var/log/redis_6379.log</span><br></pre></td></tr></table></figure>
<p>计量这两行经过了多少秒</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1012:M 05 Jul 11:27:29.659 * Background saving started by pid 10675</span><br><span class="line">...</span><br><span class="line">1012:M 05 Jul 11:27:29.764 * Background saving terminated with success</span><br></pre></td></tr></table></figure>
<p>(2) 统计拷贝 RDB 文件到 Slave 磁盘需要多久</p>
<p>(3) 统计加载 RDB 文件需要多久</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1012:M 05 Jul 08:27:31.898 * DB loaded from disk: 0.006 seconds</span><br></pre></td></tr></table></figure>
<p>最后多个 10% ~ 20% 的时间，来确保超时时间不出任何问题</p>
<h3 id="客户端缓冲区"><a href="#客户端缓冲区" class="headerlink" title="客户端缓冲区"></a>客户端缓冲区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Latency Comparison Numbers</span><br><span class="line">--------------------------</span><br><span class="line">L1 cache reference                           0.5 ns</span><br><span class="line">Branch mispredict                            5   ns</span><br><span class="line">L2 cache reference                           7   ns                      14x L1 cache</span><br><span class="line">Mutex lock/unlock                           25   ns</span><br><span class="line">Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache</span><br><span class="line">Compress 1K bytes with Zippy             3,000   ns        3 us</span><br><span class="line">Send 1K bytes over 1 Gbps network       10,000   ns       10 us</span><br><span class="line">Read 4K randomly from SSD*             150,000   ns      150 us          ~1GB/sec SSD</span><br><span class="line">Read 1 MB sequentially from memory     250,000   ns      250 us</span><br><span class="line">Round trip within same datacenter      500,000   ns      500 us</span><br><span class="line">Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory</span><br><span class="line">Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip</span><br><span class="line">Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD</span><br><span class="line">Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">-----</span><br><span class="line">1 ns = 10^-9 seconds</span><br><span class="line">1 us = 10^-6 seconds = 1,000 ns</span><br><span class="line">1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns</span><br><span class="line"></span><br><span class="line">Credit</span><br><span class="line">------</span><br><span class="line">By Jeff Dean:               http://research.google.com/people/jeff/</span><br><span class="line">Originally by Peter Norvig: http://norvig.com/21-days.html#answers</span><br><span class="line"></span><br><span class="line">Contributions</span><br><span class="line">-------------</span><br><span class="line">Some updates from:       https://gist.github.com/2843375</span><br><span class="line">&apos;Humanized&apos; comparison:  https://gist.github.com/2843375</span><br><span class="line">Visual comparison chart: http://i.imgur.com/k0t1e.png</span><br><span class="line">Animated presentation:   http://prezi.com/pdkvgys-r0y6/latency-numbers-for-programmers-web-development/latency.txt</span><br></pre></td></tr></table></figure>
<p>每一个客户端连接都被分配了自己的缓冲区，当处理完一个请求之后，Redis 就会将响应数据到客户端缓冲区中。查看客户端缓冲区 (0: 无限制) 的命令是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; config get client-output-buffer-limit</span><br><span class="line">1) &quot;client-output-buffer-limit&quot;</span><br><span class="line">// |&lt;- CLI BUF -&gt;|</span><br><span class="line">2) &quot;normal 0 0 0 slave 268435456 67108864 60 pubsub 33554432 8388608 60&quot;</span><br><span class="line">//                   |&lt;|- REPLICATION BUF -&gt;|</span><br></pre></td></tr></table></figure>
<p>客户端缓冲区的内存是从 Redis 可用的最大内存中分配的，<code>KEYS, SMEMBERS, HGETALL, LRANGE, ZRANGE</code> 这些个简单的命令可能会产生体积庞大的数据，很容易就会消耗掉巨大的内存，我们应该避免使用以上命令，取而代之的是应该首选 <code>[SCAN](https://redis.io/commands/scan)</code> 命令。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://www.open-open.com/lib/view/open1454502890526.html" target="_blank" rel="noopener">Redis上踩过的一些坑-美团</a></li>
<li><a href="https://redislabs.com/blog/top-redis-headaches-for-devops-replication-buffer/" target="_blank" rel="noopener">Top Redis Headaches for Devops – Replication Buffer</a></li>
<li><a href="https://redislabs.com/blog/top-redis-headaches-for-devops-replication-timeouts/" target="_blank" rel="noopener">Top Redis Headaches for Devops – Replication Timeouts</a></li>
<li><a href="https://redislabs.com/blog/top-redis-headaches-for-devops-client-buffers/" target="_blank" rel="noopener">Top Redis Headaches for Devops – Client Buffers</a></li>
<li><a href="https://redislabs.com/blog/the-endless-redis-replication-loop-what-why-and-how-to-solve-it/" target="_blank" rel="noopener">The Endless Redis Replication Loop: What, Why and How to Solve It</a></li>
</ul><!-- hexo-inject:begin --><!-- hexo-inject:end -->






</body>
</html>
